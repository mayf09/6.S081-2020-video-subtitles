1
00:00:02,110 --> 00:00:03,880
Alright, I want we get started,

2
00:00:03,970 --> 00:00:07,330
you know people want to turn on the camera again

3
00:00:07,330 --> 00:00:11,830
or well it be great used to create sort of class atmosphere,

4
00:00:12,430 --> 00:00:14,140
it's the best we can.

5
00:00:14,620 --> 00:00:16,750
Okay, so this is,

6
00:00:17,440 --> 00:00:19,090
we're going to talk today about this paper,

7
00:00:19,120 --> 00:00:22,750
you know the benefits and cost of writing a UNIX kernel in a high-level language,

8
00:00:24,070 --> 00:00:30,280
you know, this is basically paper that was partly written because of 6.S081 or 6.828,

9
00:00:30,840 --> 00:00:34,170
and there's the paper that you know we've written,

10
00:00:34,260 --> 00:00:35,670
you know Robert and I,

11
00:00:35,670 --> 00:00:39,300
and that the main person was a main lead was Cody Cutler,

12
00:00:39,300 --> 00:00:42,960
who was a TA this class many many times.

13
00:00:43,600 --> 00:00:45,340
It's always a little bit,

14
00:00:45,340 --> 00:00:46,480
you know I don't really enjoy

15
00:00:46,480 --> 00:00:49,180
actually particularly talking about papers that we worked on ourselves,

16
00:00:49,180 --> 00:00:53,890
but you know this paper came about basically 6.S081 or 6.828,

17
00:00:53,890 --> 00:00:56,740
and so I'm gonna use some slides this time

18
00:00:56,740 --> 00:00:59,440
instead of actually writing on a whiteboard.

19
00:01:00,850 --> 00:01:04,720
And so really the source of this paper is this question

20
00:01:04,720 --> 00:01:07,960
in what language should you write a kernel.

21
00:01:08,520 --> 00:01:12,540
And this is a question that many, many of you asked

22
00:01:12,570 --> 00:01:19,350
where you know students in the past 6.828 graduate asked many many many times,

23
00:01:19,380 --> 00:01:23,490
apparently you know because you know you have bugs in the operating system or your kernel

24
00:01:23,490 --> 00:01:25,980
and you're like well, if I had written this other language,

25
00:01:25,980 --> 00:01:28,410
then you know maybe I would not have those bugs.

26
00:01:29,140 --> 00:01:32,560
So this is a question that often comes about

27
00:01:32,560 --> 00:01:36,850
and it turns out you know in the operating system community at large,

28
00:01:36,850 --> 00:01:39,430
you know this is a hotly debated question,

29
00:01:39,520 --> 00:01:44,260
but not that many facts, you could actually make a sort of any informed discussion.

30
00:01:45,200 --> 00:01:47,390
And what we'll see at this at the end of this lecture,

31
00:01:47,390 --> 00:01:49,580
during this lecture or you as you read the paper,

32
00:01:49,700 --> 00:01:55,010
I know we don't really have a crisp answer to this question,

33
00:01:55,160 --> 00:01:59,150
but we have sort of this paper contributes you know a bunch of data,

34
00:01:59,150 --> 00:02:02,870
that you know allows you to have a little bit more of an in-depth discussion

35
00:02:03,140 --> 00:02:07,250
about what is a good programming language for the kernel.

36
00:02:08,130 --> 00:02:11,220
So that was really the original of this paper,

37
00:02:11,220 --> 00:02:21,300
and and so we're, and the source you know of this paper is basically you guys.

38
00:02:21,860 --> 00:02:25,430
So, try to answer the question,

39
00:02:25,610 --> 00:02:28,880
we wrote a new kernel,

40
00:02:29,270 --> 00:02:34,280
and we did it in the language with automatic memory management

41
00:02:34,280 --> 00:02:35,570
that means with garbage collector,

42
00:02:35,570 --> 00:02:37,760
so you don't actually have to call free,

43
00:02:37,760 --> 00:02:40,040
you know and avoid a class bugs,

44
00:02:40,040 --> 00:02:43,310
so that's one of the properties at a high level language typically has,

45
00:02:43,550 --> 00:02:46,220
and so we wanted to have picked a language that has that,

46
00:02:46,460 --> 00:02:50,720
and we followed basically traditional monolithic Unix organization,

47
00:02:50,720 --> 00:02:52,280
we could do a fair comparison.

48
00:02:52,910 --> 00:02:58,640
And in fact some way you could think about what we built is something like xv6,

49
00:02:58,640 --> 00:03:04,070
much much more, more features and more high performance.

50
00:03:05,140 --> 00:03:05,980
I mean, as you know,

51
00:03:05,980 --> 00:03:12,430
xv6 has all kinds of chronic algorithms or linear search, type algorithms,

52
00:03:12,430 --> 00:03:14,920
and of course if you want to achieve high performance,

53
00:03:14,920 --> 00:03:15,790
you can't have those.

54
00:03:17,360 --> 00:03:20,900
And so that was the original, this paper in the original,

55
00:03:20,900 --> 00:03:23,930
why we built Biscuit trying to answer that question

56
00:03:23,930 --> 00:03:25,100
or at least shed some light.

57
00:03:25,650 --> 00:03:28,440
First, I'm going to talk a little bit about sort of more general background,

58
00:03:28,440 --> 00:03:31,770
I got a lot of questions over email,

59
00:03:31,770 --> 00:03:34,530
we're trying to get a little bit more context,

60
00:03:34,830 --> 00:03:38,400
and then I will dive into Biscuit in more more detail,

61
00:03:38,610 --> 00:03:42,330
feel free to jump in with questions in any particular point of time,

62
00:03:42,540 --> 00:03:47,470
here as you know this paper was motivated by questions you asked,

63
00:03:47,470 --> 00:03:50,380
and so you know please please ask keep asking questions.

64
00:03:52,980 --> 00:03:56,160
So, as you can just do,

65
00:03:56,370 --> 00:04:00,480
sort of the setting of this paper is a lot of kernels are written in C,

66
00:04:00,480 --> 00:04:03,270
and you know xv6 is written in C, you're programming in C,

67
00:04:03,420 --> 00:04:09,180
but most popular kernels that you see on your desktop or your phone are written in C,

68
00:04:09,180 --> 00:04:14,130
Windows Linux Linux to all the various forms of BSDs.

69
00:04:14,850 --> 00:04:21,420
And, and the reason the written or written in C is that,

70
00:04:21,990 --> 00:04:25,680
C provides a lot of control as you've seen in the labs,

71
00:04:25,770 --> 00:04:28,710
yeah, complete control for memory allocation and freeing,

72
00:04:28,980 --> 00:04:31,290
there's almost no implicit code,

73
00:04:31,290 --> 00:04:34,140
you know you can almost imagine when you're reading the C code,

74
00:04:34,140 --> 00:04:36,540
what the corresponding RISC-V instructions are,

75
00:04:37,000 --> 00:04:38,740
you have direct access to memory,

76
00:04:38,740 --> 00:04:44,140
you can read or write you know the PTE bits you know or the register of a devices,

77
00:04:44,410 --> 00:04:48,640
and you know C itself [] very few dependencies,

78
00:04:48,640 --> 00:04:51,280
you know there's no large runtime that you actually have to have,

79
00:04:51,400 --> 00:04:52,750
to be able to run a C program,

80
00:04:52,750 --> 00:04:55,870
you can almost run almost immediately on bare hareware.

81
00:04:56,260 --> 00:04:58,540
And you've seen that once xv6 boots,

82
00:04:58,540 --> 00:05:00,310
you know it's basically a few lines of assembly

83
00:05:00,310 --> 00:05:01,900
and then basically you're running C code.

84
00:05:02,780 --> 00:05:06,860
But there are all the good virtues of C

85
00:05:06,860 --> 00:05:08,660
and one reason that we like C a lot.

86
00:05:09,230 --> 00:05:11,810
But C also has some downsides,

87
00:05:12,080 --> 00:05:16,190
you know it has been proven over the last few decades,

88
00:05:16,340 --> 00:05:19,340
that writing security C code is difficult,

89
00:05:19,340 --> 00:05:26,030
you know there are types of bugs that can, you can often be exploited

90
00:05:26,030 --> 00:05:27,470
whether it is buffer overruns,

91
00:05:27,470 --> 00:05:30,950
which are probably the most well-known one you're writing behind the past,

92
00:05:31,450 --> 00:05:35,980
an array bound or you're writing you know below your stack,

93
00:05:36,340 --> 00:05:42,640
use-after-free bugs, you know where you know the free of memory, but it's still in use,

94
00:05:43,140 --> 00:05:45,060
and so when somebody scribbles on it

95
00:05:45,060 --> 00:05:47,460
and you know scribble something bad on it,

96
00:05:47,580 --> 00:05:52,440
and generally you know when threads are sharing memory

97
00:05:52,440 --> 00:05:55,770
is typically difficult to decide you know what actually memory can be freed.

98
00:05:56,940 --> 00:05:59,670
Some of these bugs, you know don't really show up that,

99
00:05:59,940 --> 00:06:04,200
you know some of the bug manifest itself explicitly xv6, some less,

100
00:06:04,200 --> 00:06:09,360
so you know, you know xv6 very little dynamic memory allocation,

101
00:06:09,360 --> 00:06:11,820
you're almost after being sort of pre allocated [right up front],

102
00:06:12,260 --> 00:06:16,580
a buffer overruns usually have free bugs do show up.

103
00:06:17,440 --> 00:06:22,540
So, in fact you know if you look at CVEs,

104
00:06:22,630 --> 00:06:25,360
you know these are you know there's a website

105
00:06:25,360 --> 00:06:27,790
where there's an organization that keeps control,

106
00:06:27,790 --> 00:06:32,110
that checks and keeps a record of all of the security exploits,

107
00:06:32,350 --> 00:06:36,730
and investigate that you'll find that in 2017 when we were doing this paper,

108
00:06:36,730 --> 00:06:41,380
that there were 40 Linux kernel bugs,

109
00:06:41,380 --> 00:06:47,950
that can actually lead to an attacker running take complete control over the machine.

110
00:06:48,570 --> 00:06:50,220
Clearly, you know those are serious bugs

111
00:06:50,220 --> 00:06:52,740
and those bugs came out of buffer overruns

112
00:06:52,890 --> 00:06:55,860
and other types of memory safety bugs.

113
00:06:56,720 --> 00:06:59,990
So you know it's sort of too bad,

114
00:07:00,020 --> 00:07:01,850
you know that if you write code in C,

115
00:07:01,850 --> 00:07:03,050
you know that actually is hard,

116
00:07:03,050 --> 00:07:08,060
even for people that do that professionally and to actually get this right.

117
00:07:08,730 --> 00:07:11,580
And of course you know, I'm sure you've seen this in the lab,

118
00:07:11,580 --> 00:07:16,590
you know probably certainly I remember from some of the the Piazza questions,

119
00:07:16,590 --> 00:07:19,200
you know the number if you run into use-after-free bugs,

120
00:07:19,200 --> 00:07:21,690
you know in particular in the copy-on-write lab,

121
00:07:21,930 --> 00:07:23,760
they showed up a bunch of times.

122
00:07:25,530 --> 00:07:30,660
So yeah so, one reason that a high-level language would be attractive is that,

123
00:07:30,690 --> 00:07:33,240
a high-level language that provides memory safety,

124
00:07:33,360 --> 00:07:37,170
and so all these bugs that CVE exploits

125
00:07:37,170 --> 00:07:38,670
that I mentioned on the previous slide

126
00:07:38,790 --> 00:07:41,490
would not be would not be possible,

127
00:07:41,520 --> 00:07:43,740
you know could use you know they if they happen,

128
00:07:43,830 --> 00:07:45,660
they either would result in a panic,

129
00:07:45,690 --> 00:07:48,410
you know, because you know the runtime which would say,

130
00:07:48,410 --> 00:07:50,450
like, oh you're writing passed away, you can't do that,

131
00:07:50,660 --> 00:07:55,100
or you know you just couldn't manifest itself at all,

132
00:07:55,130 --> 00:07:58,010
because the language wouldn't allow you to write that kind of code.

133
00:08:00,800 --> 00:08:03,560
So there are of course other benefits to a high-level language,

134
00:08:03,650 --> 00:08:09,710
you know which often is also mentioned you know by you know students in this class,

135
00:08:09,740 --> 00:08:11,030
when they're doing labs,

136
00:08:11,030 --> 00:08:12,350
in addition to type safety,

137
00:08:12,350 --> 00:08:14,990
you know there's automatic memory management with garbage collector,

138
00:08:15,320 --> 00:08:16,850
so freeing is easy,

139
00:08:16,850 --> 00:08:18,080
you don't have to think about it,

140
00:08:18,080 --> 00:08:19,850
the garbage collector does all the work for you,

141
00:08:19,970 --> 00:08:22,130
it's good for concurrency,

142
00:08:22,130 --> 00:08:23,690
you know it has good abstractions,

143
00:08:23,720 --> 00:08:29,090
you know it's like Go its interfaces or some other classes or some other form of,

144
00:08:29,270 --> 00:08:33,710
you know forces you or encourage you to actually write modular code.

145
00:08:36,620 --> 00:08:38,720
The downsides and you might be wondering like,

146
00:08:38,720 --> 00:08:41,360
oh if there's so many upsides to high-level languages,

147
00:08:41,360 --> 00:08:42,980
you know why not,

148
00:08:42,980 --> 00:08:47,540
you know why is xv6 not written in you know Java Go Python or whatever,

149
00:08:47,900 --> 00:08:50,810
and the reason is you know the or Linux,

150
00:08:50,810 --> 00:08:53,000
you know the reason is there's poor performance,

151
00:08:53,000 --> 00:08:56,600
you know there's a cost to actually a high-level language,

152
00:08:56,600 --> 00:08:59,780
sometimes this is referred to as the high-level language tax.

153
00:09:00,240 --> 00:09:05,460
And you know these are basically you know if you do an array bounds, array index,

154
00:09:05,460 --> 00:09:07,230
you know you have to check the bounds,

155
00:09:07,230 --> 00:09:09,060
you know you have to check nil-pointers,

156
00:09:10,060 --> 00:09:14,020
and they have more expensive cast,

157
00:09:14,260 --> 00:09:17,260
and you know garbage collection itself is also not free,

158
00:09:17,290 --> 00:09:20,260
you know there's gonna be some cycles spent tracking down

159
00:09:20,260 --> 00:09:23,740
which objects are free, which are allocated, then you know that's a cost.

160
00:09:26,640 --> 00:09:28,620
So that's you know from the performance side

161
00:09:28,650 --> 00:09:30,840
and a lot of the paper focuses on this.

162
00:09:30,930 --> 00:09:33,870
And then in principle you know often I mean

163
00:09:33,900 --> 00:09:37,740
it's perceived as a sort of incompatibility with Linux kernel programming,

164
00:09:38,360 --> 00:09:40,730
you know no direct memory access,

165
00:09:40,730 --> 00:09:44,930
you know, because the principal could know a violate type safety,

166
00:09:45,380 --> 00:09:47,090
no hand-written assembly

167
00:09:47,090 --> 00:09:49,430
and you need some hand-written assembly in the kernel,

168
00:09:49,430 --> 00:09:51,920
whereas the context switch between two threads

169
00:09:51,920 --> 00:09:54,650
or to get off the ground when the machine boots,

170
00:09:55,320 --> 00:10:00,790
in you know often,

171
00:10:00,880 --> 00:10:06,460
you know the language may have a particular plan for concurrency or parallelism

172
00:10:06,460 --> 00:10:11,770
that might not line up with the plan that the kernel needs work concurrency parallelism,

173
00:10:11,770 --> 00:10:15,730
we've seen for example and a scheduling lecture,

174
00:10:15,730 --> 00:10:19,570
you know one thread passes a lock to another thread,

175
00:10:19,570 --> 00:10:23,350
where you know there's a couple you know there's patterns of concurrency management

176
00:10:23,350 --> 00:10:25,270
that are sort of unusual and usual programs,

177
00:10:25,270 --> 00:10:27,690
but they do show up in, in kernels.

178
00:10:28,580 --> 00:10:35,680
So, so the goal basically of this you know paper was

179
00:10:35,680 --> 00:10:38,560
to sort of measure the high-level language trade-offs,

180
00:10:38,710 --> 00:10:41,440
explore the total effects of using high-level language instead of C,

181
00:10:41,470 --> 00:10:44,920
you know both in terms of a safety program ability,

182
00:10:44,950 --> 00:10:46,870
but also for performance cost.

183
00:10:47,200 --> 00:10:49,870
And of course if you'd like to do this, this kind of experiment,

184
00:10:49,870 --> 00:10:52,390
you know you need to do that sort of a production grade kernel,

185
00:10:52,390 --> 00:10:53,740
you can't do that on xv6,

186
00:10:53,740 --> 00:10:56,440
because it's so slow that basically you probably won't learn anything,

187
00:10:56,470 --> 00:10:59,320
you know it's a program that was written slow in C,

188
00:10:59,320 --> 00:11:01,780
you're written slow in Go,

189
00:11:01,780 --> 00:11:05,230
you know doesn't really tell you much about you know the C versus Go question,

190
00:11:05,230 --> 00:11:06,940
it just says like well xv6 slow.

191
00:11:07,560 --> 00:11:12,570
And so you want to do that in a more high performance you know oriented kernel

192
00:11:12,690 --> 00:11:15,060
or the kernel was designed for high performance.

193
00:11:17,600 --> 00:11:19,700
And so you're you know one of the things that are surprising,

194
00:11:19,700 --> 00:11:24,260
because like many of you ask this [] ask this

195
00:11:24,260 --> 00:11:29,300
and I would imagine well this question must be answered in the literature,

196
00:11:29,480 --> 00:11:31,430
and you know it turns out that actually isn't,

197
00:11:31,490 --> 00:11:36,590
it is there's quite a number of studies that look into the question

198
00:11:36,590 --> 00:11:40,040
of high-level language trade-offs in the context of user programs,

199
00:11:40,510 --> 00:11:46,510
and, but you know as you know the kernel is quite a bit different from user programs,

200
00:11:46,990 --> 00:11:49,990
for example memory management, careful memory management is really important,

201
00:11:50,440 --> 00:11:52,720
this different types of concurrency may be slightly different,

202
00:11:52,870 --> 00:11:55,570
so we want to do it in the context of a kernel.

203
00:11:56,310 --> 00:12:00,300
And we don't actually really find any sort of papers really answer this question,

204
00:12:00,880 --> 00:12:02,620
the closer you could come to is,

205
00:12:02,620 --> 00:12:07,540
there you know there are many kernels written in high-level language,

206
00:12:07,540 --> 00:12:09,640
you know there's a long history of doing that,

207
00:12:09,640 --> 00:12:13,840
they're dating back even to sort of the early [] machines,

208
00:12:14,080 --> 00:12:19,450
and you know but many of the sort of recent versions of these kernels

209
00:12:19,810 --> 00:12:26,650
are not really written with the idea of evaluating this high-level language tax question,

210
00:12:27,000 --> 00:12:32,790
but really to explore new OS designs and new OS architectures,

211
00:12:33,090 --> 00:12:36,900
and so none of them really measured directly,

212
00:12:36,900 --> 00:12:39,450
you know sort of head to head comparison,

213
00:12:39,660 --> 00:12:41,970
and keep the structure the same,

214
00:12:42,210 --> 00:12:45,210
so you can really focus on this issue of the language

215
00:12:45,210 --> 00:12:47,490
as opposed to some other issues.

216
00:12:49,220 --> 00:12:53,960
In fact we used to read you know some of these papers actually in the past.

217
00:12:59,350 --> 00:13:02,320
So, you know there's one reason may be that,

218
00:13:02,320 --> 00:13:05,290
there's not a lot of work that actually answers,

219
00:13:05,290 --> 00:13:07,840
a ton of papers not answer this question is,

220
00:13:07,840 --> 00:13:09,340
it's actually tricky to do it,

221
00:13:09,460 --> 00:13:12,580
basically if you really do it right,

222
00:13:12,610 --> 00:13:15,100
you know you want to compare with a production grade C kernel,

223
00:13:15,160 --> 00:13:19,510
that means something like Linux or something that Windows whatever,

224
00:13:19,750 --> 00:13:23,350
but then you have to build a production grade kernel

225
00:13:23,350 --> 00:13:28,570
and you know clearly for a small team that is very hard to do,

226
00:13:28,870 --> 00:13:31,570
you know there's lots of lots of Linux kernel developers,

227
00:13:31,570 --> 00:13:36,580
make many many changes you know week by week day by day

228
00:13:36,580 --> 00:13:39,100
and so it's going to be hard to

229
00:13:39,100 --> 00:13:40,930
you know to do the same thing

230
00:13:40,930 --> 00:13:43,450
and build an equivalent you know type of thing,

231
00:13:43,450 --> 00:13:48,460
so you have to settle for something slightly less than.

232
00:13:51,660 --> 00:13:55,050
So the best we could do or the best we could come up to do is

233
00:13:55,170 --> 00:13:57,840
build a high-level build a kernel on the high-level language,

234
00:13:57,870 --> 00:14:00,360
you know keep most of the important aspects of same as Linux,

235
00:14:00,840 --> 00:14:02,880
optimized performance,

236
00:14:02,880 --> 00:14:06,300
roughly optimized performance until it's roughly similar to Linux,

237
00:14:06,300 --> 00:14:09,210
you know even though maybe it's not identically exactly identical features,

238
00:14:09,210 --> 00:14:10,950
but it gets into the same ballpark

239
00:14:11,310 --> 00:14:13,710
and then measured high-level language trade-offs.

240
00:14:14,520 --> 00:14:18,300
And of course you know the risk you know this approach is that

241
00:14:18,300 --> 00:14:21,990
you know the kernel that we built you know actually is slightly different than Linux,

242
00:14:21,990 --> 00:14:23,880
you know it's not gonna be exactly like Linux

243
00:14:23,880 --> 00:14:28,440
and so you've gotta be very careful when drawing any conclusions.

244
00:14:28,900 --> 00:14:31,660
And you know and this is one reason

245
00:14:31,660 --> 00:14:34,570
why you can still get a really crystal clear answer,

246
00:14:34,570 --> 00:14:36,640
that is the question that basically this paper proposes,

247
00:14:36,820 --> 00:14:39,910
but we can hopefully get a little bit more deeper insight

248
00:14:39,910 --> 00:14:42,070
than basically saying almost nothing about it.

249
00:14:44,560 --> 00:14:47,860
Is this makes sense, so far, any questions?

250
00:14:48,690 --> 00:14:51,030
That's sort of the context of this paper,

251
00:14:51,030 --> 00:14:54,120
and you know why we gone to do it.

252
00:14:55,990 --> 00:14:58,700
Okay so, if there's no questions,

253
00:14:58,700 --> 00:15:02,510
I'd like to talk a little bit more about the methodology.

254
00:15:03,290 --> 00:15:07,220
So, so basically you know the sort of setup is

255
00:15:07,220 --> 00:15:12,170
here on the left side, you know we have our is gonna be Biscuit,

256
00:15:12,860 --> 00:15:17,600
you know, we're gonna, in our particular case,

257
00:15:17,600 --> 00:15:21,590
we wrote for this paper the kernel in Go,

258
00:15:21,590 --> 00:15:26,900
it provides roughly a similar subset of the system calls Linux provides,

259
00:15:26,900 --> 00:15:28,370
but you know the way to,

260
00:15:28,370 --> 00:15:30,050
but they have same arguments,

261
00:15:30,050 --> 00:15:32,150
you know the same calling interfaces.

262
00:15:32,850 --> 00:15:36,360
And we run basically the same applications on top of that interface,

263
00:15:36,630 --> 00:15:40,680
so you know one of the applications and Nginx which is a web server,

264
00:15:41,390 --> 00:15:43,640
and so the idea is that

265
00:15:43,640 --> 00:15:49,240
you know we're on the same application both on Biscuit and Linux,

266
00:15:49,270 --> 00:15:54,730
you know the application will generate the same system call trace with exactly the same arguments

267
00:15:55,030 --> 00:16:00,100
and both Biscuit and Linux you know perform all the necessary operations

268
00:16:00,100 --> 00:16:02,350
that are invoked by those system calls,

269
00:16:02,590 --> 00:16:05,200
and then we can sort of look at you know

270
00:16:05,200 --> 00:16:09,070
the difference is basically between you know the high-level language kernel and Linux,

271
00:16:09,070 --> 00:16:12,940
and sort of talk about like you know what are the trade-offs.

272
00:16:13,670 --> 00:16:16,760
And so after the core of the methodology

273
00:16:16,760 --> 00:16:20,750
and again because Linux and Biscuit are not going to be exactly identical,

274
00:16:20,990 --> 00:16:23,870
you know there's gonna be some differences,

275
00:16:23,870 --> 00:16:27,770
but you know we spend a lot of time in this

276
00:16:27,770 --> 00:16:30,140
trying to you know make comparison as far as possible,

277
00:16:32,970 --> 00:16:35,670
wherever possible we could think of making it.

278
00:16:37,090 --> 00:16:39,460
So a lot of you ask this question

279
00:16:39,460 --> 00:16:41,200
which high-level language use,

280
00:16:41,230 --> 00:16:43,000
you know for this kind of work

281
00:16:43,030 --> 00:16:47,290
and you know we pick Go and for a couple of reasons,

282
00:16:47,320 --> 00:16:50,260
it is a statically compiled language,

283
00:16:50,260 --> 00:16:53,680
so unlike Python, there's no interpreter,

284
00:16:53,800 --> 00:16:57,280
and the reason that we like static compiled,

285
00:16:57,280 --> 00:16:59,710
because we've basically compiles actually high performance code,

286
00:16:59,770 --> 00:17:01,810
in fact the particular Go compiler is pretty good.

287
00:17:02,740 --> 00:17:06,430
So basically you know sort of high performance language,

288
00:17:06,460 --> 00:17:10,390
furthermore, Go design is actually intended for system programming

289
00:17:10,510 --> 00:17:12,730
you know kernels or for system programming,

290
00:17:12,730 --> 00:17:14,230
so that could be a good match.

291
00:17:14,780 --> 00:17:19,400
As an example, you know aspect of why it's a good system programming,

292
00:17:19,400 --> 00:17:22,460
it's actually easy to call assembly or other foreign code,

293
00:17:23,010 --> 00:17:26,250
it has good support for concurrency, quite flexible.

294
00:17:26,550 --> 00:17:29,190
And then another reason that we wanted to use it,

295
00:17:29,190 --> 00:17:30,360
because it has a garbage collector,

296
00:17:31,260 --> 00:17:33,660
one of the things that you think about a high-level language,

297
00:17:33,660 --> 00:17:35,460
and one of the virtues of a high-level languages

298
00:17:35,460 --> 00:17:37,110
that you don't have to do memory management,

299
00:17:37,380 --> 00:17:40,860
and garbage collectors typically in a central role in the,

300
00:17:42,910 --> 00:17:46,420
yeah, provides a central role in a sort of memory management story.

301
00:17:48,830 --> 00:17:51,410
By the time we started this paper,

302
00:17:51,410 --> 00:17:52,820
or we started this project,

303
00:17:53,030 --> 00:17:55,790
Rust was not very popular

304
00:17:55,790 --> 00:17:58,370
or Rust was not very stable and mature at that point,

305
00:17:58,370 --> 00:17:59,990
and the actually could write a real kernel,

306
00:17:59,990 --> 00:18:03,110
in a retrospect now,

307
00:18:03,110 --> 00:18:04,220
you do it again,

308
00:18:04,220 --> 00:18:06,200
you know you may write it in Rust,

309
00:18:06,230 --> 00:18:09,020
because it also designs for system programming,

310
00:18:09,080 --> 00:18:13,760
it has a small runtime, produces good code,

311
00:18:13,940 --> 00:18:17,390
although one thing that

312
00:18:17,390 --> 00:18:20,660
actually might still make it very [instinct] to go for Go is that,

313
00:18:20,720 --> 00:18:23,210
Rust takes the starting assumption

314
00:18:23,210 --> 00:18:28,040
that that if you want high performance systems programs,

315
00:18:28,040 --> 00:18:30,170
then you can't do that with a garbage collector.

316
00:18:30,680 --> 00:18:35,590
And in fact a Rust type system is set up in a very clever way

317
00:18:35,590 --> 00:18:36,880
and a very interesting way,

318
00:18:37,060 --> 00:18:39,490
so that actually the garbage collector is not necessary.

319
00:18:40,080 --> 00:18:43,440
And in some ways, we were interested in answering this question,

320
00:18:43,440 --> 00:18:46,110
what is the cost of garbage collection in a high-level language,

321
00:18:46,110 --> 00:18:47,610
you know on kernel programming

322
00:18:47,790 --> 00:18:52,440
and it is really impossible to use or what does that cost,

323
00:18:52,710 --> 00:18:54,990
in some ways, you know Rust sidestep that question

324
00:18:54,990 --> 00:18:57,450
and just like you know use a language without garbage collection,

325
00:18:57,450 --> 00:18:59,340
you haven't to think about this particular cost.

326
00:19:01,860 --> 00:19:02,760
Any questions about this,

327
00:19:02,760 --> 00:19:07,070
in terms of the programming language we decided to use.

328
00:19:11,050 --> 00:19:13,330
Lots of email questions related to this topic.

329
00:19:14,850 --> 00:19:16,620
This is a theoretical question,

330
00:19:16,620 --> 00:19:19,380
that maybe doesn't have an immediate answer,

331
00:19:19,440 --> 00:19:24,930
but if the Linux kernel were to be written on Rust not Go,

332
00:19:25,260 --> 00:19:28,200
and like optimized in the same capacity,

333
00:19:28,230 --> 00:19:31,230
would it be able to achieve higher performance?

334
00:19:32,490 --> 00:19:35,880
Than, than the Go concurrency?

335
00:19:36,450 --> 00:19:39,210
Than C, like a Linux C kernel.

336
00:19:39,630 --> 00:19:41,520
I doubt it will be,

337
00:19:41,520 --> 00:19:44,340
okay, hard to know, just speculation, correct,

338
00:19:44,340 --> 00:19:45,750
because we haven't done this experiment,

339
00:19:46,050 --> 00:19:50,490
in my senses, you know would be not higher performance than C,

340
00:19:50,580 --> 00:19:53,580
but you know probably roughly the same ballpark.

341
00:19:54,580 --> 00:19:56,920
Because C so low-level,

342
00:19:56,920 --> 00:19:58,660
you can assume whatever you do in Rust,

343
00:19:58,660 --> 00:19:59,770
you could also have done in C.

344
00:20:04,810 --> 00:20:05,500
Does that make sense?

345
00:20:07,060 --> 00:20:08,020
Yes, thank you.

346
00:20:11,010 --> 00:20:12,390
Okay.

347
00:20:13,270 --> 00:20:14,980
Okay, so let's move on them,

348
00:20:15,610 --> 00:20:17,800
unless there are any other further questions about this,

349
00:20:17,860 --> 00:20:19,630
and again feel free to interrupt

350
00:20:19,630 --> 00:20:22,180
and you know there's a bit of a discussion based lecture,

351
00:20:22,900 --> 00:20:27,820
and so it was intended to stimulate intellectual interests,

352
00:20:27,820 --> 00:20:31,180
so you jump in if you have anything to think about this topic.

353
00:20:34,900 --> 00:20:38,140
So actually before you know maybe the question I want to ask,

354
00:20:38,170 --> 00:20:40,420
maybe I'll come back to that at the end of the lecture,

355
00:20:40,420 --> 00:20:41,680
closer to the end of the lecture.

356
00:20:42,280 --> 00:20:47,680
Partly you know the whole reason we ought to use high-level languages

357
00:20:47,680 --> 00:20:49,180
to avoid sort of class of bugs

358
00:20:49,300 --> 00:20:51,250
and one question you should ask yourself,

359
00:20:51,310 --> 00:20:56,230
where bugs you know in the labs that you had that would have been avoided,

360
00:20:56,260 --> 00:20:57,730
if you had a high level language.

361
00:20:58,460 --> 00:21:00,830
You know, so you know think back,

362
00:21:00,830 --> 00:21:03,410
you know I'm sure you can come up with some bugs,

363
00:21:03,410 --> 00:21:06,080
that you cost you a lot of time and a lot of pain,

364
00:21:06,230 --> 00:21:09,140
and you could ask yourself those kind of bugs,

365
00:21:09,140 --> 00:21:14,180
you know if we if the xv6 where we run into labs

366
00:21:14,180 --> 00:21:16,700
would be done in another high-level programming language,

367
00:21:16,730 --> 00:21:18,920
would have life would be a lot easier,

368
00:21:18,950 --> 00:21:21,350
would you have a lot more spare time to do other things.

369
00:21:22,470 --> 00:21:24,570
So let's keep that question in your head

370
00:21:24,570 --> 00:21:27,930
and you know hopefully return to that at the end of the lecture.

371
00:21:28,820 --> 00:21:30,920
But if you have opinions right away, that's fine too.

372
00:21:32,350 --> 00:21:35,140
Okay, so let me talk a little bit about Biscuit,

373
00:21:35,290 --> 00:21:39,640
and you know sort of works

374
00:21:39,640 --> 00:21:41,320
in sort of surprises where the things,

375
00:21:41,320 --> 00:21:43,720
that we ran into while building Biscuit things,

376
00:21:43,720 --> 00:21:46,450
that we anticipated in some things that we actually did not anticipate.

377
00:21:48,360 --> 00:21:49,890
So the user programs,

378
00:21:49,890 --> 00:21:51,690
there's a classic kernel,

379
00:21:51,750 --> 00:21:55,170
a monolithic kernel in the same way to Linux or xv6's,

380
00:21:55,320 --> 00:21:57,750
and so there's user space and kernel space,

381
00:21:57,960 --> 00:22:00,150
user space programs are

382
00:22:00,180 --> 00:22:03,210
you know whatever your compiler GCC

383
00:22:03,210 --> 00:22:06,660
or in our speaking of paper it's mostly a web server,

384
00:22:06,660 --> 00:22:08,850
and some other benchmarks.

385
00:22:09,250 --> 00:22:12,820
And the user programs are actually all written in C,

386
00:22:12,820 --> 00:22:14,650
although it could be principal in any language,

387
00:22:14,650 --> 00:22:16,600
you know since they're just a benchmark,

388
00:22:17,200 --> 00:22:18,550
we took C versions,

389
00:22:18,940 --> 00:22:21,310
and most of the programs are multi-thread,

390
00:22:21,310 --> 00:22:26,140
so unlike in xv6 where basically there's one thread per user program,

391
00:22:26,200 --> 00:22:30,850
in Biscuit, actually support multiple user level threads.

392
00:22:31,780 --> 00:22:34,540
And basically for every user level thread,

393
00:22:34,750 --> 00:22:39,040
there's corresponding kernel thread in kernel,

394
00:22:39,220 --> 00:22:43,240
and this kernel threads are actually implemented by Go itself

395
00:22:43,240 --> 00:22:45,070
and Go calls these Go routines,

396
00:22:47,970 --> 00:22:50,730
you can think about Go routines just as ordinary threads

397
00:22:50,760 --> 00:22:56,280
in the same way that xv6 has and the kernel has threads,

398
00:22:57,540 --> 00:22:59,070
the Go routines are similar,

399
00:22:59,190 --> 00:23:01,140
the main difference it correctly is that,

400
00:23:01,140 --> 00:23:04,380
in xv6 you know the threads are implemented by the kernel itself,

401
00:23:04,530 --> 00:23:07,350
and in this case you know Go runtime basically provides,

402
00:23:07,350 --> 00:23:09,090
the Go runtime schedules them,

403
00:23:09,090 --> 00:23:13,770
Go runtime has support for things like sleep wakeup or condition variables,

404
00:23:13,770 --> 00:23:15,180
there's lots of different sleep wakeup,

405
00:23:15,180 --> 00:23:18,420
but there's some condition variable synchronization mechanism,

406
00:23:18,720 --> 00:23:20,730
and there's a whole bunch of other you know things primitive,

407
00:23:20,730 --> 00:23:23,790
and Go runtime just provided by the Go language itself,

408
00:23:23,790 --> 00:23:26,280
and you have not been implemented by Biscuit itself,

409
00:23:26,280 --> 00:23:27,840
we just get them from the Go runtime.

410
00:23:29,440 --> 00:23:33,470
The Go runtime itself runs directly on the bare hardware.

411
00:23:36,400 --> 00:23:40,390
And, I'll talk a little bit about that more in the lecture,

412
00:23:40,510 --> 00:23:43,180
but like so you think about this is the machine boots,

413
00:23:43,180 --> 00:23:45,310
you know the first thing it actually boots to Go runtime,

414
00:23:46,000 --> 00:23:47,980
does it cause a lot of complications,

415
00:23:47,980 --> 00:23:51,850
because Go runtime that normally runs in user space as user level program

416
00:23:51,850 --> 00:23:54,130
and assumes that the kernel there's a kernel there,

417
00:23:54,130 --> 00:23:55,390
we can ask some services,

418
00:23:55,390 --> 00:23:59,190
for example, needs to allocate memory to, for its heap.

419
00:23:59,710 --> 00:24:02,590
And so there's a little bit of, all talk a little bit about,

420
00:24:02,590 --> 00:24:05,530
that there's a little bit of shim code,

421
00:24:05,530 --> 00:24:09,340
the Biscuit has to basically trick you know Go runtime

422
00:24:09,340 --> 00:24:11,920
into believing that it runs on top of the operating system,

423
00:24:11,920 --> 00:24:13,330
even though it's running on bare hardware,

424
00:24:14,050 --> 00:24:15,340
and basically get the boot.

425
00:24:16,360 --> 00:24:20,830
And then the kernel itself, you know it's very similar, you think xv6,

426
00:24:20,890 --> 00:24:22,240
think there's a model,

427
00:24:22,240 --> 00:24:25,690
except it's a little bit more elaborate and more high performance,

428
00:24:25,690 --> 00:24:27,190
it has some virtual memory system,

429
00:24:27,190 --> 00:24:31,030
you know that for example implements mmap lab you're doing this week,

430
00:24:31,060 --> 00:24:32,290
it has a file system,

431
00:24:32,290 --> 00:24:34,480
like there's more high performance file system,

432
00:24:34,870 --> 00:24:36,250
it has a couple of drivers,

433
00:24:36,250 --> 00:24:37,450
you know it has a disk drive

434
00:24:37,450 --> 00:24:40,630
and it has a network driver, it has a network stack,

435
00:24:41,180 --> 00:24:42,710
so it will be more complete,

436
00:24:42,710 --> 00:24:46,670
and when you can see that is, it has like 58 system calls,

437
00:24:46,670 --> 00:24:49,460
you know like I can't remember how much xv6 has,

438
00:24:49,460 --> 00:24:52,250
but [startling] the order of 18 19 or something like that.

439
00:24:52,860 --> 00:24:55,800
And the total number of launch code is 28000,

440
00:24:55,800 --> 00:25:00,030
you know xv6 is like not in, I think below 10000,

441
00:25:00,030 --> 00:25:02,280
so you know there's more features.

442
00:25:04,580 --> 00:25:06,650
Any questions about sort of this high-level overview?

443
00:25:08,470 --> 00:25:11,590
Oh, sorry I wanted to ask about the the interface,

444
00:25:11,620 --> 00:25:14,890
so the interface is just like xv6, right,

445
00:25:14,890 --> 00:25:21,040
so the processes they have to put something in some register

446
00:25:21,040 --> 00:25:25,060
and then they call the ecall or whatever it is.

447
00:25:25,060 --> 00:25:26,980
Yeah, yeah I'll talk a little bit more about this,

448
00:25:26,980 --> 00:25:29,020
but it's exactly the same, there's no difference.

449
00:25:29,530 --> 00:25:30,970
Okay I see, thank you.

450
00:25:32,860 --> 00:25:36,070
So some of the features you know already mentioned them a little bit,

451
00:25:36,070 --> 00:25:39,400
maybe worth talking about its multicore,

452
00:25:39,760 --> 00:25:42,130
Go with good support for concurrency

453
00:25:42,130 --> 00:25:44,140
and so you know Biscuit's multicore,

454
00:25:44,260 --> 00:25:49,180
in the same way that xv6 sort of has limited support for multi-core,

455
00:25:49,270 --> 00:25:50,560
in this, we have

456
00:25:50,560 --> 00:25:55,030
a little bit more fine-grained synchronization or coordination than actually in xv6.

457
00:25:55,500 --> 00:25:57,990
It has threads you know user level threads,

458
00:25:58,050 --> 00:26:01,920
[backed] up by kernel threads,

459
00:26:01,920 --> 00:26:03,570
which xv6 doesn't have,

460
00:26:03,900 --> 00:26:06,150
there's journal file system much higher performance,

461
00:26:06,150 --> 00:26:09,240
like think you know you'll called the ext3 paper,

462
00:26:09,450 --> 00:26:13,230
sort of like you know the ext3 generally file system.

463
00:26:14,000 --> 00:26:17,930
It has, you know quite reasonable, sophisticated memory system,

464
00:26:17,960 --> 00:26:19,400
you know using VMAs,

465
00:26:19,490 --> 00:26:22,760
and you know it could support mmap and all that stuff.

466
00:26:23,270 --> 00:26:25,160
It has a complete TCP/IP stack,

467
00:26:25,160 --> 00:26:29,330
you know good enough to actually talk to other network servers across the Internet

468
00:26:29,480 --> 00:26:33,230
and it has two drivers with high performance drivers,

469
00:26:33,230 --> 00:26:34,730
so like a ten gigabit NIC,

470
00:26:35,000 --> 00:26:36,380
in the next lab,

471
00:26:36,380 --> 00:26:40,130
you can actually implement a little driver for a very very simple NIC,

472
00:26:40,130 --> 00:26:43,340
this is a much more high performance and sophisticated driver.

473
00:26:43,770 --> 00:26:45,810
And in a pretty sophisticated this driver,

474
00:26:45,870 --> 00:26:49,500
you know more sophisticated than the VIRTIO_disk driver,

475
00:26:49,680 --> 00:26:56,140
that you've sort of seen or you might have looked at in the labs.

476
00:27:06,680 --> 00:27:11,360
So in terms of a user programs as I mentioned before,

477
00:27:11,360 --> 00:27:13,700
every user program runs its own page table,

478
00:27:14,000 --> 00:27:16,970
user kernel memory isolated by hardware,

479
00:27:16,970 --> 00:27:19,580
so you use a kernel bit basically,

480
00:27:20,780 --> 00:27:25,250
and every user thread has a corresponding kernel thread,

481
00:27:25,250 --> 00:27:27,710
so for example when a user thread makes a system call,

482
00:27:27,830 --> 00:27:31,460
it will continue running on the corresponding kernel thread,

483
00:27:31,640 --> 00:27:33,080
and if the system call blocks,

484
00:27:33,080 --> 00:27:35,120
then another user thread in the same address space

485
00:27:35,120 --> 00:27:37,910
and the user address space might actually be scheduled by the kernel.

486
00:27:38,990 --> 00:27:40,520
And as mentioned early,

487
00:27:40,550 --> 00:27:43,850
kernel threads are provided by the Go runtime

488
00:27:43,880 --> 00:27:45,350
and so they just Go routines.

489
00:27:46,070 --> 00:27:48,170
So you're write ever user level,

490
00:27:48,170 --> 00:27:51,170
if you ever written a user level application in Go

491
00:27:51,170 --> 00:27:55,490
and using Go and used to Go call to create a thread,

492
00:27:55,640 --> 00:27:57,890
you know that those those Go routines are the ones

493
00:27:57,890 --> 00:27:59,630
that were actually being used by the Biscuit kernel.

494
00:28:02,410 --> 00:28:03,790
So talking about system calls,

495
00:28:03,790 --> 00:28:07,360
you know this question is just asked,

496
00:28:07,660 --> 00:28:12,190
so it works exactly as roughly you know as in xv6,

497
00:28:12,190 --> 00:28:15,370
you know the user thread put arguments in registers

498
00:28:15,370 --> 00:28:17,470
using a little library,

499
00:28:17,470 --> 00:28:21,400
you know that provides system call interface,

500
00:28:21,400 --> 00:28:24,040
then the user threads executed SYSENTER call,

501
00:28:24,040 --> 00:28:27,490
you know this Biscuit runs on an x86 processor

502
00:28:27,490 --> 00:28:29,050
not on the RISC-V processor,

503
00:28:29,050 --> 00:28:32,350
so the assembly instructions for actually Intel system kernel

504
00:28:32,350 --> 00:28:35,470
are slightly different than on the RISC-V on the RISC-V,

505
00:28:36,280 --> 00:28:40,270
but you know roughly similar similar to RISC-V,

506
00:28:40,270 --> 00:28:43,060
and then control passes to the kernel thread,

507
00:28:43,650 --> 00:28:45,930
that was running that user thread,

508
00:28:46,350 --> 00:28:48,720
and then the kernel thread executes a system call,

509
00:28:48,720 --> 00:28:50,580
and then returns use SYSEXIT.

510
00:28:51,180 --> 00:28:52,530
So roughly similar frame,

511
00:28:52,530 --> 00:28:55,170
you know it's a trapframe that's being built and all that kind of stuff.

512
00:28:57,910 --> 00:28:58,630
Okay.

513
00:29:01,040 --> 00:29:02,390
Any questions so far,

514
00:29:02,950 --> 00:29:05,200
before I dive into sort of a more sort of,

515
00:29:05,350 --> 00:29:08,270
things that were unexpected or expected,

516
00:29:08,270 --> 00:29:10,430
but we're a little bit more challenging than were different,

517
00:29:10,430 --> 00:29:13,250
that I think it would go in the xv6.

518
00:29:13,920 --> 00:29:14,790
I have a question,

519
00:29:14,790 --> 00:29:17,630
I guess, I think,

520
00:29:18,270 --> 00:29:24,630
Go wants you to use channels more than mutex locks,

521
00:29:24,630 --> 00:29:28,230
I guess, so would you like, would there be like,

522
00:29:29,040 --> 00:29:31,680
what the design of somethings in xv6 be like,

523
00:29:31,680 --> 00:29:35,130
could use as channels instead of holding a lock for something.

524
00:29:35,550 --> 00:29:38,070
Yeah,, there's a great a great question,

525
00:29:38,070 --> 00:29:40,590
so we I'll come back to it a little bit again,

526
00:29:40,680 --> 00:29:41,580
further down

527
00:29:41,580 --> 00:29:43,380
and we have some slides

528
00:29:43,380 --> 00:29:46,920
about what features of Go did we use in Biscuit,

529
00:29:46,920 --> 00:29:51,030
but you know the the [] we didn't have any of using channels that much,

530
00:29:51,060 --> 00:29:53,760
we mostly use locks and condition variables.

531
00:29:54,250 --> 00:29:56,950
So in some sense closer to like a way xv6 looks,

532
00:29:56,950 --> 00:30:00,640
than actually a one you would do then you would do with channels,

533
00:30:00,730 --> 00:30:03,700
we did experiment actually with designs of the file system

534
00:30:03,700 --> 00:30:06,220
that were much more channel heavy,

535
00:30:06,460 --> 00:30:09,130
and it didn't work out great,

536
00:30:09,220 --> 00:30:10,510
and we got bad performance,

537
00:30:10,720 --> 00:30:15,550
so yeah we switched back to sort of more a sort of simple style of synchronization

538
00:30:15,550 --> 00:30:17,770
xv6 does or Linux uses.

539
00:30:21,740 --> 00:30:26,480
Okay, so you know a couple sort of a little puzzles or implementation challenges

540
00:30:26,480 --> 00:30:27,830
as we went through,

541
00:30:28,070 --> 00:30:31,820
one, we gotta get to runtime to work on the bare-metal

542
00:30:31,940 --> 00:30:34,460
and you know that required you know wanted to

543
00:30:34,460 --> 00:30:38,060
make of course like zero modifications to the runtime where as little as possible,

544
00:30:38,060 --> 00:30:40,820
so that you know Go come out with a new version of the runtime,

545
00:30:40,820 --> 00:30:41,630
we could just use it.

546
00:30:42,180 --> 00:30:45,330
In fact, you know through the years,

547
00:30:45,330 --> 00:30:48,540
you know that we worked on this, where the code worked on this,

548
00:30:48,540 --> 00:30:51,600
we upgraded the runtime many you know number of times.

549
00:30:52,210 --> 00:30:53,950
And that was turned out to be a good thing,

550
00:30:53,950 --> 00:30:54,520
and it turned out,

551
00:30:54,520 --> 00:30:57,640
they were not to be too difficult to actually to get to work on the bare-metal.

552
00:30:58,170 --> 00:31:02,220
You know Go in general is designed pretty carefully

553
00:31:02,220 --> 00:31:04,890
to sort of be mostly OS agnostic,

554
00:31:04,890 --> 00:31:07,050
because they want to be able to run into many operating system,

555
00:31:07,050 --> 00:31:09,210
so it doesn't rely on a ton of OS features.

556
00:31:09,970 --> 00:31:13,570
And we're basically emulated the features that actually needed,

557
00:31:13,750 --> 00:31:15,520
and mostly those are the features,

558
00:31:15,520 --> 00:31:18,490
that actually just get off the Go runtime to get started

559
00:31:18,850 --> 00:31:20,650
and once it started, it runs just happily.

560
00:31:24,470 --> 00:31:28,670
We have sort of range that Go routine run different applications,

561
00:31:28,700 --> 00:31:34,220
normally in Go program correct, you know one single application,

562
00:31:34,490 --> 00:31:36,560
and here now we're using Go routine thread

563
00:31:36,560 --> 00:31:40,220
around different user, different user applications.

564
00:31:40,670 --> 00:31:44,900
And but the user applications have to run with different page tables.

565
00:31:45,490 --> 00:31:49,240
And the little you know [] here is that

566
00:31:49,240 --> 00:31:53,230
you know the we don't control or Biscuit doesn't control the scheduler,

567
00:31:53,260 --> 00:31:55,630
because we're using the Go runtime unmodified,

568
00:31:55,630 --> 00:31:57,370
so we're using the Go runtime scheduler,

569
00:31:57,520 --> 00:32:00,010
and so in the scheduler, we can't switch page tables.

570
00:32:00,460 --> 00:32:05,710
So what xv6 in basically what Biscuit does is very similar to xv6,

571
00:32:05,710 --> 00:32:07,450
it actually switches page tables,

572
00:32:07,450 --> 00:32:11,050
when it changes from kernel to user space or the other way around.

573
00:32:12,030 --> 00:32:16,860
So when entry and exit of the kernel, switch page tables.

574
00:32:17,360 --> 00:32:19,310
And that means like in xv6

575
00:32:19,310 --> 00:32:21,590
and then when you need to copy data

576
00:32:21,590 --> 00:32:24,050
from user space to kernel space or the other way around,

577
00:32:24,140 --> 00:32:27,500
you have to do that sort of using those copyin and copyout functions,

578
00:32:27,500 --> 00:32:28,760
that we also have in xv6,

579
00:32:28,760 --> 00:32:30,890
basically you do the page table walking software.

580
00:32:33,550 --> 00:32:38,150
Another issue of challenge where little challenge was device interrupts,

581
00:32:38,600 --> 00:32:41,360
and Go runtime normally runs in user mode,

582
00:32:41,420 --> 00:32:44,810
you know it doesn't really get interrupts from the hardware,

583
00:32:45,200 --> 00:32:47,450
but we're using it on the bare-metal

584
00:32:47,450 --> 00:32:48,980
and so we're going to get interrupts,

585
00:32:49,040 --> 00:32:51,830
time clock interrupts, interrupt from the network driver,

586
00:32:51,830 --> 00:32:55,850
interrupt from the disk driver etc, you know from the UART.

587
00:32:56,290 --> 00:32:58,150
And so we need to deal with that

588
00:32:58,150 --> 00:33:01,900
and and there's also no notion in Go

589
00:33:01,900 --> 00:33:05,500
you know for switching off interrupts while holding a lock,

590
00:33:05,890 --> 00:33:08,500
because just show up user applications,

591
00:33:08,800 --> 00:33:14,290
and so we have a little bit careful how to actually write a device interrupt

592
00:33:14,290 --> 00:33:16,330
and basically the way we did it is,

593
00:33:16,330 --> 00:33:18,520
we do almost nothing in the device interrupt,

594
00:33:18,700 --> 00:33:20,140
we don't take any locks out,

595
00:33:20,170 --> 00:33:22,450
basically we don't allocate any memory,

596
00:33:22,750 --> 00:33:23,980
the only thing we do is

597
00:33:23,980 --> 00:33:27,190
basically sending a flag somewhere that wasn't interrupted

598
00:33:27,370 --> 00:33:31,900
and then wake up a really functional Go routine to actually deal with the interrupt.

599
00:33:35,000 --> 00:33:38,240
And that Go routine, of course you can use all the Go features that it wants.

600
00:33:39,160 --> 00:33:41,620
Because it does run in the context of an interrupt handler,

601
00:33:41,620 --> 00:33:44,080
just it runs in the context of normal normal Go routine.

602
00:33:45,410 --> 00:33:48,830
Then one thing that surprises, it was a bit of surprise,

603
00:33:48,860 --> 00:33:51,380
you know the first three things were completely anticipated,

604
00:33:51,380 --> 00:33:54,350
that we have to deal with when a building Biscuit,

605
00:33:54,350 --> 00:33:57,440
the hardest one that actually had suprised us,

606
00:33:57,770 --> 00:33:59,930
and we learned a lot about,

607
00:33:59,930 --> 00:34:02,120
it was this puzzle of heap exhaustion.

608
00:34:02,760 --> 00:34:05,610
So I'm going to talk mostly for a little while about heap exhaustion

609
00:34:05,610 --> 00:34:07,770
and you know what it is, you know how it comes about

610
00:34:07,770 --> 00:34:09,030
and how we solved it,

611
00:34:09,030 --> 00:34:10,740
but maybe before diving into that,

612
00:34:11,040 --> 00:34:12,750
any any questions so far?

613
00:34:18,810 --> 00:34:20,460
So, [] clear.

614
00:34:24,540 --> 00:34:26,700
Okay, so let's talk a little bit about heap exhaustion,

615
00:34:26,700 --> 00:34:28,650
I'm not going to go with full depth was in the paper,

616
00:34:28,650 --> 00:34:31,650
but at least gives you a flavor of what the problem is.

617
00:34:40,940 --> 00:34:42,560
So in the heap exhaustion,

618
00:34:42,590 --> 00:34:46,430
you know let's say the blue box here is the kernel again,

619
00:34:49,450 --> 00:34:51,310
and you know the kernel has a heap,

620
00:34:51,340 --> 00:34:54,610
from which delegates dynamically memory,

621
00:34:54,610 --> 00:34:56,350
in xv6, we don't have such a heap,

622
00:34:56,350 --> 00:34:58,450
because we don't have a memory allocator in the kernel,

623
00:34:58,450 --> 00:34:59,620
everything statically allocated,

624
00:34:59,650 --> 00:35:01,960
but any other kernel we'll have a heap,

625
00:35:01,960 --> 00:35:06,250
so you can call malloc, you know and free in the kernel.

626
00:35:08,120 --> 00:35:10,640
And you know the things that actually get allocated on the heap,

627
00:35:10,640 --> 00:35:17,600
for example you know socket objects or file descriptor objects or process objects,

628
00:35:17,900 --> 00:35:19,820
struct proc, you know struct fd,

629
00:35:19,820 --> 00:35:23,600
all the structures that we basically statically allocated in xv6,

630
00:35:23,720 --> 00:35:25,880
normal kernels, they dynamically allocate them,

631
00:35:26,360 --> 00:35:28,040
so when you open the new file descriptor,

632
00:35:28,040 --> 00:35:30,830
there will be a file descriptor object allocated in the heap.

633
00:35:32,230 --> 00:35:33,700
And so, then the problem is,

634
00:35:33,700 --> 00:35:35,560
if you're running many applications,

635
00:35:35,560 --> 00:35:38,680
you know they might open many file descriptors, may have many sockets,

636
00:35:38,770 --> 00:35:41,380
and they sort of start filling the heap basically slowly,

637
00:35:41,530 --> 00:35:43,870
and so the issue is that,

638
00:35:43,870 --> 00:35:45,490
at some points, like the heap full,

639
00:35:45,880 --> 00:35:49,420
there's no space anymore for allocating a new object

640
00:35:49,420 --> 00:35:53,080
or when an application asks for example opens a new file descriptor

641
00:35:53,080 --> 00:35:56,290
and there's like you know new process, like there's new fork,

642
00:35:56,440 --> 00:35:58,690
and the kernel wants to allocate a struct proc

643
00:35:58,690 --> 00:36:00,790
and heap used like there's no space anymore.

644
00:36:01,660 --> 00:36:03,580
And what did you do that,

645
00:36:03,760 --> 00:36:06,280
you know what is you know,

646
00:36:06,340 --> 00:36:08,470
how do you deal with that particular case,

647
00:36:08,770 --> 00:36:09,730
and this is typically,

648
00:36:09,730 --> 00:36:13,210
you know this is maybe in common cases does not show up that often,

649
00:36:13,300 --> 00:36:15,820
but like if you're pushing machine hard,

650
00:36:15,820 --> 00:36:19,420
you have a couple heavy consumer processes running user level processes,

651
00:36:19,450 --> 00:36:20,800
you might end in this situation,

652
00:36:20,800 --> 00:36:24,250
where basically in all the available memory is in use

653
00:36:24,310 --> 00:36:26,320
and your heap is just full.

654
00:36:27,120 --> 00:36:29,880
And no processes calling free yet,

655
00:36:29,880 --> 00:36:31,170
you know because they're all running

656
00:36:31,170 --> 00:36:34,920
and trying to allocate more resources for their for their particular jobs.

657
00:36:39,790 --> 00:36:42,070
So all kind of kernel face this problem,

658
00:36:42,070 --> 00:36:45,070
when it like C kernel or Biscuit or anything

659
00:36:45,070 --> 00:36:47,470
and any kernel must solve this particular problem.

660
00:36:48,290 --> 00:36:50,630
The reason they they sort of showed up for us

661
00:36:50,630 --> 00:36:54,410
as a serious issue in Biscuit

662
00:36:54,410 --> 00:36:59,000
was because in many kernels the,

663
00:37:01,270 --> 00:37:03,610
you can return an error on malloc,

664
00:37:03,670 --> 00:37:06,040
in fact the xv6 does that, right, once in a while,

665
00:37:06,040 --> 00:37:07,930
but you know in Go runtime,

666
00:37:07,930 --> 00:37:10,870
when you call new to allocate a Go object,

667
00:37:11,080 --> 00:37:13,990
there's no error condition, new succeeds,

668
00:37:14,400 --> 00:37:16,200
and so there's no way to fail it.

669
00:37:16,740 --> 00:37:22,050
So let's talk a little bit about possible ways to solve this problem,

670
00:37:22,770 --> 00:37:26,790
the, you know we've seen it actually xv6 once in a while,

671
00:37:26,790 --> 00:37:29,850
like if you remember the bcache,

672
00:37:29,850 --> 00:37:34,920
if xv6 can't find a new block you know to,

673
00:37:34,920 --> 00:37:37,950
a free block to use for storage a disk block in,

674
00:37:38,040 --> 00:37:39,630
actually sometimes just panics.

675
00:37:40,060 --> 00:37:44,890
You know this clearly is a completely undesirable solution

676
00:37:44,890 --> 00:37:46,180
and it's not a real solution,

677
00:37:46,180 --> 00:37:48,280
so why we call it a strawman solution.

678
00:37:49,010 --> 00:37:52,220
The other strawman solution is to,

679
00:37:52,250 --> 00:37:55,730
when you call, let's say you allocate a new piece of memory,

680
00:37:55,730 --> 00:37:59,300
you know you go to call alloc, new to actually allocate it,

681
00:37:59,600 --> 00:38:02,120
you could actually you know wait for memory allocator,

682
00:38:02,820 --> 00:38:04,410
it going to be one proposal to do,

683
00:38:04,470 --> 00:38:06,150
turns out not to be a good proposal,

684
00:38:06,570 --> 00:38:10,500
and the reason it's not a good proposal is that you may deadlock,

685
00:38:10,530 --> 00:38:12,700
you know, assume the following scenario,

686
00:38:12,700 --> 00:38:13,570
you're holding some,

687
00:38:13,570 --> 00:38:15,550
let's say the kernel has one big kernel lock,

688
00:38:15,820 --> 00:38:20,470
and you call malloc, you waiting the memory allocator,

689
00:38:20,770 --> 00:38:22,840
then basically no other process can run.

690
00:38:23,530 --> 00:38:26,470
And you would have a deadlock have in your next process,

691
00:38:26,470 --> 00:38:29,980
that would actually try to run for example to free some memory,

692
00:38:29,980 --> 00:38:31,720
you know couldn't run, actually deadlock.

693
00:38:32,490 --> 00:38:33,180
Of course, this is,

694
00:38:33,180 --> 00:38:36,090
if you have a big kernel lock, there's an obvious problem,

695
00:38:36,090 --> 00:38:40,590
you know, but even if you have a very small, a fine-grained locking,

696
00:38:40,650 --> 00:38:42,210
it is easy to run in a situation,

697
00:38:42,210 --> 00:38:48,030
where basically the person or the process that's waiting in the allocator is holding some lock,

698
00:38:48,030 --> 00:38:50,220
that somebody else needs to actually free the memory,

699
00:38:50,840 --> 00:38:53,420
and that can get you basically this deadlock situation.

700
00:38:54,710 --> 00:39:00,290
And so we're going strawman free is to basically fail,

701
00:39:00,470 --> 00:39:03,440
or when you there's no memory anymore,

702
00:39:03,440 --> 00:39:06,590
alloc returns like no pointer, you check with no pointer,

703
00:39:06,590 --> 00:39:09,200
it's no point you fail use a [].

704
00:39:09,840 --> 00:39:14,070
But [] is actually not that straightforward,

705
00:39:14,070 --> 00:39:17,790
you know the process might actually have allocated memory already,

706
00:39:17,820 --> 00:39:19,110
you need to get rid of that,

707
00:39:19,230 --> 00:39:21,900
you may have done some partial disk operations,

708
00:39:21,900 --> 00:39:23,550
like for example if you do a multi steps,

709
00:39:23,610 --> 00:39:26,340
you know partial operation maybe have done some of it, but not all of it,

710
00:39:26,340 --> 00:39:27,420
you have to bail out of that.

711
00:39:27,880 --> 00:39:31,570
And so, it turns out to actually get very very hard to get right.

712
00:39:32,340 --> 00:39:37,170
In sort of interesting, you know when digging into this

713
00:39:37,260 --> 00:39:40,020
and trying to think about how to solve this problem,

714
00:39:40,140 --> 00:39:43,020
Linux uses both of these solutions.

715
00:39:43,500 --> 00:39:47,550
And you know both actually have trouble or problems,

716
00:39:47,730 --> 00:39:52,530
and indeed kernel developers actually have difficulty to actually get this all straight,

717
00:39:52,560 --> 00:39:54,330
if you're very interested in this

718
00:39:54,330 --> 00:39:56,370
and you want to see some interesting discussion about this,

719
00:39:56,880 --> 00:39:58,710
google for "too small to fail"

720
00:39:58,890 --> 00:40:02,730
and there's a little article that talks about some of these complications,

721
00:40:02,730 --> 00:40:06,990
you know free memory or waiting in the allocator,

722
00:40:06,990 --> 00:40:09,660
and the problem that can cause.

723
00:40:10,650 --> 00:40:12,930
Now it turns out, for us,

724
00:40:12,930 --> 00:40:15,210
you know so strawman 2 be the solution,

725
00:40:15,210 --> 00:40:16,140
that you could imagine doing,

726
00:40:16,140 --> 00:40:16,800
but then for us,

727
00:40:16,800 --> 00:40:19,620
just as mentioned earlier, this was not possible,

728
00:40:19,620 --> 00:40:23,070
because new just cannot return, cannot fail,

729
00:40:23,100 --> 00:40:24,480
it just always succeeds,

730
00:40:24,540 --> 00:40:27,300
so we've got a range in some way that this cannot happen.

731
00:40:28,590 --> 00:40:31,710
Plus neither of these two solutions actually particular ideal,

732
00:40:31,710 --> 00:40:35,160
so we wanted to come up with something that was potentially better.

733
00:40:36,780 --> 00:40:40,260
Any questions so far, about the setup around heap exhaustion,

734
00:40:40,260 --> 00:40:42,720
before I talk about like how the way Biscuit does it.

735
00:40:48,420 --> 00:40:49,500
This problem makes sense?

736
00:40:58,960 --> 00:41:02,170
I'll interpret the signings as yes, then keep going,

737
00:41:02,170 --> 00:41:03,910
but figure to interrupt anytime.

738
00:41:06,800 --> 00:41:08,810
Okay, so what is the Biscuit solution,

739
00:41:08,840 --> 00:41:13,790
yeah, as a high level of the Biscuit solution is like almost straight forward,

740
00:41:14,420 --> 00:41:15,890
what basically does,

741
00:41:15,890 --> 00:41:19,670
like when you execute a system call like say you read or fork,

742
00:41:20,420 --> 00:41:23,900
before jumping actually into the fork system call,

743
00:41:23,930 --> 00:41:25,910
right at the beginning of the fork system call,

744
00:41:25,910 --> 00:41:28,970
if you know like in a system call scheduler in xv6,

745
00:41:28,970 --> 00:41:31,520
then first thing it does actually calls reserve,

746
00:41:32,630 --> 00:41:34,880
and it basically reserves enough memory,

747
00:41:35,120 --> 00:41:37,700
to be able to execute the system call.

748
00:41:38,380 --> 00:41:42,430
So there's a free memory enough

749
00:41:42,430 --> 00:41:45,730
that actually whatever on memory that actually the system call needs,

750
00:41:45,910 --> 00:41:49,570
the reservation will be big enough that actually and will succeed.

751
00:41:50,430 --> 00:41:53,010
So, so once the system call goes off

752
00:41:53,010 --> 00:41:55,560
and actually successful in reserving memory,

753
00:41:55,560 --> 00:41:57,390
it will actually run all the way through

754
00:41:57,420 --> 00:41:59,100
and we were never with the problems,

755
00:41:59,100 --> 00:42:01,950
that there won't be enough memory or heap exhaustion.

756
00:42:03,340 --> 00:42:05,200
And if there's not enough memory,

757
00:42:05,200 --> 00:42:06,790
at the point you want to do the reservation,

758
00:42:06,820 --> 00:42:08,290
then basically just wait here,

759
00:42:09,940 --> 00:42:11,920
but at the beginning of the system call,

760
00:42:11,920 --> 00:42:13,660
the system call doesn't hold any locks,

761
00:42:13,660 --> 00:42:15,040
doesn't hold any resources yet,

762
00:42:15,040 --> 00:42:16,480
so it actually is perfectly fine,

763
00:42:16,480 --> 00:42:21,190
you know wait there, no risk of deadlock.

764
00:42:22,160 --> 00:42:23,660
And while it's waiting,

765
00:42:23,690 --> 00:42:26,540
you know it can of course be doing, it can call,

766
00:42:26,540 --> 00:42:29,690
kernel can actually evict cache,

767
00:42:29,690 --> 00:42:33,620
you know try to reduce the, basically make free up heap space,

768
00:42:33,800 --> 00:42:36,920
maybe as you seen,

769
00:42:36,920 --> 00:42:41,000
maybe kill a process that to force you know memory actually be freed.

770
00:42:41,590 --> 00:42:43,990
And then once you know memory is available

771
00:42:43,990 --> 00:42:47,020
and the kernel decides well you know I can meet a reservation,

772
00:42:47,020 --> 00:42:50,020
then it will let the system call basically goes off and runs,

773
00:42:50,170 --> 00:42:52,570
and basically executes whatever it needs to be done

774
00:42:52,900 --> 00:42:55,060
and then at the very end, when the system call goes down,

775
00:42:55,060 --> 00:42:56,170
it's like, okay, I'm done,

776
00:42:56,350 --> 00:42:59,410
and all the memory that was reserve basically comes back to the pool,

777
00:42:59,620 --> 00:43:02,380
available for subsequent system calls.

778
00:43:03,770 --> 00:43:07,310
And there's a couple of nice properties about this particular solutions,

779
00:43:07,310 --> 00:43:09,980
there's no checks necessary in the kernel itself,

780
00:43:10,010 --> 00:43:14,390
like you never have to check whether memory memory allocation can fail,

781
00:43:14,570 --> 00:43:16,190
which is particularly in our case good,

782
00:43:16,190 --> 00:43:18,110
because you know Go, it can't fail.

783
00:43:18,620 --> 00:43:20,960
There's no error handling code and necessary at all,

784
00:43:21,080 --> 00:43:22,400
and there's no risk for deadlock,

785
00:43:22,400 --> 00:43:24,860
because you're avoiding in the very beginning without,

786
00:43:24,860 --> 00:43:26,300
when you actually hold no locks.

787
00:43:27,010 --> 00:43:29,830
Of course, you know there's all wonderful well,

788
00:43:29,980 --> 00:43:32,290
the only thing is like how there's a challenge,

789
00:43:32,290 --> 00:43:33,640
of course, how you do the reservation,

790
00:43:33,880 --> 00:43:34,930
how do you compute,

791
00:43:34,960 --> 00:43:42,710
you know, how much memory a system call might need to to execute it.

792
00:43:43,590 --> 00:43:45,300
And so now we have a puzzle.

793
00:43:47,780 --> 00:43:52,460
And you know it's important that the amount you reserve,

794
00:43:52,460 --> 00:43:53,660
one, one you could do is

795
00:43:53,660 --> 00:43:55,940
you can reserve half memory or something like that,

796
00:43:55,940 --> 00:43:58,640
some ridiculous amount of memory for every system call,

797
00:43:58,670 --> 00:44:01,850
but that means you limit the number of system calls you can execute concurrently,

798
00:44:01,850 --> 00:44:03,950
so you want to sort of do a pretty good job

799
00:44:03,980 --> 00:44:07,220
and actually computing bound of the amount of memory

800
00:44:07,220 --> 00:44:09,970
that the system call might need.

801
00:44:11,050 --> 00:44:17,290
So, the way, we ended up doing this,

802
00:44:17,290 --> 00:44:22,240
you know turned out like sort of high-level language help us here,

803
00:44:22,390 --> 00:44:25,450
turned out like Go is actually pretty easy to static analyze,

804
00:44:25,450 --> 00:44:30,490
in fact Go runtime and Go infrastructure ecosystems

805
00:44:30,490 --> 00:44:33,580
comes with a whole bunch of packages to analyze Go code.

806
00:44:34,100 --> 00:44:37,400
And we use those packages basically to compute

807
00:44:37,700 --> 00:44:42,920
the amount of memory that the system call needs,

808
00:44:42,950 --> 00:44:44,060
so you can think about the,

809
00:44:44,060 --> 00:44:47,240
let's say you have a read system call, right,

810
00:44:47,240 --> 00:44:50,810
you know you know we could look at the call graph of the system call,

811
00:44:50,810 --> 00:44:54,440
you know calls the function f, calls the function g, calls the function h, blah blah blah,

812
00:44:54,440 --> 00:44:55,790
might continue a whole bunch,

813
00:44:55,910 --> 00:44:58,130
and you know at the end of system call,

814
00:44:58,130 --> 00:44:59,060
it [binds] to [stack] again,

815
00:44:59,060 --> 00:45:01,880
and then goes back to return to user space.

816
00:45:02,520 --> 00:45:04,200
And basically what we can do is

817
00:45:04,200 --> 00:45:08,370
like you know allocate you know or figure out what the maximum depth,

818
00:45:08,940 --> 00:45:13,770
you know of this this call graph is,

819
00:45:14,330 --> 00:45:16,100
at any particular time,

820
00:45:16,130 --> 00:45:17,930
and then basically for that maximum depth,

821
00:45:17,930 --> 00:45:21,770
you know compute how much you know live memory each of these functions need,

822
00:45:21,770 --> 00:45:25,640
so if this function calls new, you know that will allocate memory,

823
00:45:25,670 --> 00:45:27,830
you know we know what kind of objects there are,

824
00:45:27,860 --> 00:45:28,910
there's a high level language,

825
00:45:28,910 --> 00:45:30,770
so we can compute the size of that object is,

826
00:45:30,770 --> 00:45:33,590
we can just add them up and it gives us some number s,

827
00:45:33,620 --> 00:45:36,650
that says like the total amount of memory or the maximum amount of memory,

828
00:45:36,650 --> 00:45:42,290
that can be live at any particular point of time for that call graph.

829
00:45:43,290 --> 00:45:45,420
And the reason is you know it's slightly tricky,

830
00:45:45,420 --> 00:45:46,590
it's not as simple as this,

831
00:45:46,590 --> 00:45:50,280
because for example a function h might allocate some memory,

832
00:45:50,800 --> 00:45:53,470
and then pass back to g,

833
00:45:53,530 --> 00:45:55,090
and so you know h finishes,

834
00:45:55,510 --> 00:46:00,820
and but you know g actually you know gets the memory that h is allocated,

835
00:46:01,120 --> 00:46:03,430
and this is called escaping,

836
00:46:03,430 --> 00:46:08,390
the memory escapes from you know from h to g

837
00:46:08,390 --> 00:46:12,770
it turns out, there are standard algorithms were doing sort of this escape analysis

838
00:46:12,770 --> 00:46:15,560
to see determine which variables escape to the callers,

839
00:46:16,010 --> 00:46:17,570
and in that case,

840
00:46:17,570 --> 00:46:20,690
you know basically whatever memory was allocated by h and that's still alive,

841
00:46:20,690 --> 00:46:22,880
we have to add to whatever g is,

842
00:46:23,760 --> 00:46:25,890
so you know we have to be added into s.

843
00:46:27,040 --> 00:46:28,510
A quick question about this,

844
00:46:29,440 --> 00:46:32,470
so let's assume more in some function,

845
00:46:32,800 --> 00:46:36,490
depending on different workloads that the function is expected to have,

846
00:46:36,790 --> 00:46:40,150
there might be different memories memory amounts allocated,

847
00:46:40,360 --> 00:46:44,590
so what is there like a worst-case, what memory allocation process.

848
00:46:44,620 --> 00:46:47,170
Yeah, that's basically it's sort of conservative scheme correct,

849
00:46:47,170 --> 00:46:51,430
and we we computed, the tool computes,

850
00:46:51,430 --> 00:46:56,200
basically a worst possible depth of function calls.

851
00:46:56,830 --> 00:47:00,040
And you know for that the worst-case,

852
00:47:00,040 --> 00:47:03,940
analysis how much memory that reaches system call might need,

853
00:47:03,970 --> 00:47:06,760
in practice, it might need, there system call might need a lot less,

854
00:47:06,940 --> 00:47:10,210
but you know for you know to be conservative,

855
00:47:10,210 --> 00:47:13,240
we have to allocate the work we plan for the worst case.

856
00:47:14,000 --> 00:47:18,050
And so we've come to a couple of important points here,

857
00:47:18,050 --> 00:47:21,830
because you know some system calls for example executive for loop,

858
00:47:21,830 --> 00:47:25,020
that's depended on argument to the system call, right,

859
00:47:25,020 --> 00:47:27,600
so you can't actually statically figure out what the bound is,

860
00:47:27,990 --> 00:47:31,260
and so a number of cases we annotated the code

861
00:47:31,260 --> 00:47:33,900
to say well this is the maximum bounds of this loop,

862
00:47:34,200 --> 00:47:36,210
and you can assume it's no more than that,

863
00:47:36,210 --> 00:47:38,700
and use that to actually compute this number s.

864
00:47:39,990 --> 00:47:43,620
Similarly you know, for example if you have a recursive function,

865
00:47:43,980 --> 00:47:46,290
you know who knows how deep the recursion is, right,

866
00:47:46,470 --> 00:47:48,210
and that might also be a dependent

867
00:47:48,210 --> 00:47:50,730
on a dynamic variable or an argument to a system call.

868
00:47:51,220 --> 00:47:55,150
In fact, you know we you know we treat Biscuit in some places

869
00:47:55,150 --> 00:47:57,490
basically avoid recursive function pass,

870
00:47:57,820 --> 00:48:01,990
so actually it was possible to do this, you know do this kind of analysis.

871
00:48:02,770 --> 00:48:04,510
So this kind of analysis not for free,

872
00:48:04,510 --> 00:48:05,620
it's not completely automatic,

873
00:48:05,800 --> 00:48:08,980
it takes a couple days of work you know for this case,

874
00:48:08,980 --> 00:48:16,240
you know Cody to go through look at all these loops and and annotate.

875
00:48:16,840 --> 00:48:19,540
You know there are a couple others Go specific issues,

876
00:48:19,540 --> 00:48:20,560
that you have to deal with

877
00:48:20,560 --> 00:48:22,960
like slice, you know they might double in size,

878
00:48:22,960 --> 00:48:25,150
if you add an element to slice

879
00:48:25,570 --> 00:48:29,500
and so we imitate the slices which maximum capacity,

880
00:48:29,800 --> 00:48:32,830
but I use all sort of doable for a couple days work,

881
00:48:32,860 --> 00:48:35,140
and you know using this tool,

882
00:48:35,140 --> 00:48:37,660
then you can get a number out there is reasonable good,

883
00:48:38,080 --> 00:48:43,390
in terms of computing on maximum amount of memory

884
00:48:43,390 --> 00:48:44,830
that a particular system call needs.

885
00:48:46,150 --> 00:48:47,680
And so this is basically how

886
00:48:47,710 --> 00:48:50,890
you know we basically Biscuit solves this particular problem.

887
00:48:54,100 --> 00:48:57,910
Oh, sorry, what else are people using this tool for,

888
00:48:58,000 --> 00:49:00,250
like they're not, they're not building a kernel,

889
00:49:00,250 --> 00:49:01,570
what are they using it for?

890
00:49:01,600 --> 00:49:03,280
Over static analysis package.

891
00:49:03,910 --> 00:49:04,300
Yeah.

892
00:49:04,330 --> 00:49:08,260
Go compiler internally uses it for all kinds of optimizations,

893
00:49:08,260 --> 00:49:12,490
you know to and do static analysis,

894
00:49:12,760 --> 00:49:17,380
Go go to figure out the best way to for the best way to compile it.

895
00:49:18,270 --> 00:49:20,490
I see, I see, okay, thank you.

896
00:49:20,880 --> 00:49:23,640
So this is one of the cool things about this package,

897
00:49:23,640 --> 00:49:25,020
you know the compiler happens to use,

898
00:49:25,020 --> 00:49:26,250
you know what we could do.

899
00:49:27,500 --> 00:49:31,790
We'll see later on, we also use it for a couple other features.

900
00:49:32,710 --> 00:49:34,570
It's very convenient to have.

901
00:49:37,640 --> 00:49:39,020
Okay.

902
00:49:40,580 --> 00:49:42,110
Okay, turns to the implementation,

903
00:49:42,200 --> 00:49:46,640
you know basically basically very similar to other kernels

904
00:49:46,640 --> 00:49:50,600
or like you know xv6, except more high performance.

905
00:49:51,070 --> 00:49:54,070
You know what we adopted

906
00:49:54,070 --> 00:49:58,090
many of the optimizations or cleverness that the Linux kernel has,

907
00:49:58,090 --> 00:50:00,400
at least the system calls that we were trying to implement,

908
00:50:00,920 --> 00:50:05,840
you know use large pages for kernel text to avoid TLB cost,

909
00:50:06,350 --> 00:50:13,460
we have per-CPU NIC transmit queues to avoid synchronization between port,

910
00:50:13,790 --> 00:50:14,900
we have RCU,

911
00:50:14,900 --> 00:50:17,990
I will talk a little bit more about the directory cache,

912
00:50:17,990 --> 00:50:22,550
which is basically lock free or read lock free directory cache,

913
00:50:22,550 --> 00:50:25,280
at the end of the semester, we'll talk about RCU in more detail,

914
00:50:25,280 --> 00:50:27,260
but you know, this could have some too,

915
00:50:29,720 --> 00:50:33,230
you know sort of the usual type of optimization,

916
00:50:33,230 --> 00:50:36,200
that actually you need to get high performance.

917
00:50:36,720 --> 00:50:40,020
And the main lesson I think we learned is that

918
00:50:40,320 --> 00:50:43,410
Go was not standing in the way of implementing these optimizations,

919
00:50:44,160 --> 00:50:48,600
so these optimizations that were implemented in C and Linux,

920
00:50:48,600 --> 00:50:51,390
you know we've basically implemented same optimization,

921
00:50:51,390 --> 00:50:52,680
but ever implemented in Go,

922
00:50:52,890 --> 00:50:56,410
and so the language itself is not a hurdle or a problem,

923
00:50:56,560 --> 00:51:00,040
in fact completely conducive to actually implementing these optimizations.

924
00:51:01,460 --> 00:51:03,590
There's a lot of work to implement these optimizations,

925
00:51:03,590 --> 00:51:05,720
but irrespective of the language.

926
00:51:09,810 --> 00:51:12,900
OK, so that brings me sort of to evaluation,

927
00:51:12,900 --> 00:51:15,990
which is really what the, the motivation of the whole paper was,

928
00:51:15,990 --> 00:51:21,420
which is like trying to handle on the benefits and the costs of high-level language,

929
00:51:21,420 --> 00:51:24,670
so basically, the evaluation sort of split in two parts,

930
00:51:24,670 --> 00:51:27,070
first talking about the benefits and then talking about the costs.

931
00:51:29,370 --> 00:51:31,320
So, so three questions,

932
00:51:31,320 --> 00:51:35,280
you know first of all, you know there's a question like didn't cheat,

933
00:51:35,280 --> 00:51:38,400
you know maybe we avoided all the expensive high-level language features,

934
00:51:38,940 --> 00:51:40,890
that Go offers.

935
00:51:40,950 --> 00:51:45,180
Does the, second question of course does the high-level language simplify the Biscuit code,

936
00:51:45,180 --> 00:51:47,640
and would to prevent some of these exploits

937
00:51:47,640 --> 00:51:50,400
that you know I mentioned early on in the lecture.

938
00:51:51,300 --> 00:51:54,750
So first, respect to use high-level language features,

939
00:51:54,990 --> 00:51:56,040
we just wanted to see

940
00:51:56,040 --> 00:51:59,370
whether we were similar in terms of other big Go projects,

941
00:51:59,370 --> 00:52:00,540
in terms of language features,

942
00:52:00,540 --> 00:52:01,410
so that we could say,

943
00:52:01,410 --> 00:52:04,080
like all the kernel seems to be doing roughly

944
00:52:04,080 --> 00:52:07,410
the same advantage of the same features in similar ways.

945
00:52:07,860 --> 00:52:10,980
So we use actually the same static analysis tool or package

946
00:52:10,980 --> 00:52:17,430
to basically analyze whole bunch of two big pieces of Go software that on github,

947
00:52:17,490 --> 00:52:19,110
you know there are millions lines of code,

948
00:52:19,110 --> 00:52:21,690
one is you know Go runtime itself and all its packages

949
00:52:21,960 --> 00:52:23,790
and the system called Moby.

950
00:52:24,550 --> 00:52:29,110
And then we just basically product for number of high-level language features,

951
00:52:29,110 --> 00:52:31,270
how many times they were used for thousand lines,

952
00:52:31,360 --> 00:52:32,740
so this graph shows that,

953
00:52:32,740 --> 00:52:36,360
as usual, on the x-axis are the language features.

954
00:52:36,390 --> 00:52:39,540
Basically allocations correspond to calling new

955
00:52:39,570 --> 00:52:41,400
and so this corresponds to the memory,

956
00:52:41,400 --> 00:52:44,550
that it will be dynamically allocated by the garbage collector.

957
00:52:44,910 --> 00:52:49,080
You know maps are like hash tables, slices or dynamic arrays.

958
00:52:49,080 --> 00:52:51,030
You know, here's the channels synchronization,

959
00:52:51,030 --> 00:52:53,280
as you can see, we use it very littlely,

960
00:52:53,280 --> 00:52:55,410
but so does the Go runtime and the Moby.

961
00:52:56,160 --> 00:52:58,950
Clearly the feature that we like most,

962
00:52:59,010 --> 00:53:04,260
it was multi function return, being able to return multiple values.

963
00:53:04,800 --> 00:53:06,750
You know we use closures,

964
00:53:06,810 --> 00:53:08,550
we didn't use finalizer,

965
00:53:08,580 --> 00:53:10,710
we use defer a little bit,

966
00:53:10,710 --> 00:53:13,410
you know there's a bunch of Go routines that we do create,

967
00:53:13,410 --> 00:53:14,550
we use interfaces.

968
00:53:15,040 --> 00:53:18,760
You know type assertions to convert from one type to another,

969
00:53:19,180 --> 00:53:21,760
in the type asserts matter.

970
00:53:21,970 --> 00:53:24,310
And importing many packages,

971
00:53:24,340 --> 00:53:27,220
so kernel selves build out of any packages

972
00:53:27,220 --> 00:53:29,230
are not like one big single program.

973
00:53:29,880 --> 00:53:30,600
So if you look at this,

974
00:53:30,600 --> 00:53:34,830
you know some features you know Biscuit could use less than Golang and Moby

975
00:53:34,830 --> 00:53:38,760
and sometimes you know this could lose some features more or roughly in the [],

976
00:53:39,970 --> 00:53:43,480
not not in any sort of distinctly different way.

977
00:53:44,090 --> 00:53:46,190
So the main conclusion from this is

978
00:53:46,190 --> 00:53:49,730
you know basically uses the high-level features actually Go offers,

979
00:53:49,730 --> 00:53:55,190
and doesn't sidestep them to basically get good performance.

980
00:53:56,950 --> 00:53:58,270
Okay.

981
00:53:58,850 --> 00:54:00,500
I have a question,

982
00:54:01,430 --> 00:54:05,450
how did you, how are you able to count all this,

983
00:54:05,450 --> 00:54:08,150
did you use the static analysis tool?

984
00:54:08,960 --> 00:54:12,140
Yeah basically use static package, static analysis package

985
00:54:12,140 --> 00:54:14,810
and then wrote a little program that uses static analysis packages

986
00:54:14,810 --> 00:54:16,730
to go over every statement in these programs

987
00:54:16,730 --> 00:54:18,680
and look at what kind of type of statement is.

988
00:54:19,800 --> 00:54:22,290
And then, you get the argument

989
00:54:22,290 --> 00:54:23,970
to see how the arguments are being used

990
00:54:23,970 --> 00:54:26,100
and that gives you a sense about how,

991
00:54:27,080 --> 00:54:29,120
allows you to count these features.

992
00:54:31,250 --> 00:54:31,760
Okay.

993
00:54:37,600 --> 00:54:41,290
Okay, so the next thing is a little subjective,

994
00:54:41,380 --> 00:54:45,940
did the high-level language simplified Biscuit code?

995
00:54:46,730 --> 00:54:48,140
Yeah, I think it generally did,

996
00:54:48,170 --> 00:54:51,950
and so argued with one or two examples explicitly,

997
00:54:51,950 --> 00:54:56,180
but you're now having GC allocation is actually very nice,

998
00:54:56,180 --> 00:54:57,380
and maybe I can make the point,

999
00:54:57,380 --> 00:54:58,550
if you think of xv6

1000
00:54:58,550 --> 00:54:59,630
or you do an exec,

1001
00:55:00,170 --> 00:55:01,160
on point of exec,

1002
00:55:01,160 --> 00:55:04,820
there's a lot of data structures that need to be freed or return to the kernel

1003
00:55:05,930 --> 00:55:09,290
and so that later process can use,

1004
00:55:09,290 --> 00:55:10,820
using garbage collector is really easy,

1005
00:55:10,820 --> 00:55:12,650
you know the garbage collector takes care of all of it,

1006
00:55:12,680 --> 00:55:14,300
you know you don't really have to do much.

1007
00:55:14,700 --> 00:55:16,800
So if you allocate you know free an address page,

1008
00:55:16,800 --> 00:55:19,170
you know the VMA correspond in address space

1009
00:55:19,170 --> 00:55:21,390
will be automatically freed by the garbage collector too.

1010
00:55:22,420 --> 00:55:24,400
Yeah, so you know just that as simple,

1011
00:55:24,700 --> 00:55:26,260
as we mentioned earlier,

1012
00:55:26,260 --> 00:55:29,680
the multi return values were really nice in terms of programming style,

1013
00:55:29,890 --> 00:55:31,000
closures were nice,

1014
00:55:31,000 --> 00:55:31,870
maps were great,

1015
00:55:31,870 --> 00:55:35,230
you know you don't have to many [],

1016
00:55:35,410 --> 00:55:36,940
places xv6 for example,

1017
00:55:37,300 --> 00:55:39,970
you know look up something in a linear fashion,

1018
00:55:39,970 --> 00:55:43,930
but if you have hash tables or maps as a first class object or abstraction,

1019
00:55:43,930 --> 00:55:45,250
the programmer you know would never do that,

1020
00:55:47,450 --> 00:55:51,110
you use map and the runtime will take care of doing everything efficiently.

1021
00:55:51,960 --> 00:55:56,400
So, in in fact I think qualitatively, it feels you get simpler code.

1022
00:55:57,450 --> 00:55:59,070
But as clearly qualitatively,

1023
00:55:59,070 --> 00:56:01,170
you know just give a little bit more of a concrete example,

1024
00:56:01,170 --> 00:56:06,630
where really, where sort of high-level language, particular garbage collector shines is

1025
00:56:06,630 --> 00:56:08,700
when there's a lot of concurrency between,

1026
00:56:09,120 --> 00:56:10,200
when there's concurrency threads

1027
00:56:10,200 --> 00:56:12,660
and threads actually share a particular shared data item.

1028
00:56:13,330 --> 00:56:17,230
And so for example, here's a simple case,

1029
00:56:17,230 --> 00:56:19,600
you know or you can boil down this question to,

1030
00:56:19,600 --> 00:56:25,150
let's say allocate somehow dynamically object like a buffer,

1031
00:56:25,210 --> 00:56:28,300
you know fork a thread you know and that process that buffer,

1032
00:56:28,300 --> 00:56:30,670
and there's another thread that also process buffer

1033
00:56:30,670 --> 00:56:31,780
and do something of this buffer.

1034
00:56:32,330 --> 00:56:33,590
When both threads are done,

1035
00:56:33,590 --> 00:56:34,790
you know the buffer needs to be free,

1036
00:56:34,880 --> 00:56:39,530
so they can be used for later later kernel operations.

1037
00:56:40,200 --> 00:56:43,200
And the question is like who should do this, who's in charge,

1038
00:56:43,770 --> 00:56:49,980
and there's a little bit difficult to coordinate in C,

1039
00:56:49,980 --> 00:56:52,230
because you have to have some way of deciding that

1040
00:56:52,230 --> 00:56:54,000
actually the buffer is actually not being used,

1041
00:56:54,090 --> 00:56:55,290
if you use a garbage collector,

1042
00:56:55,350 --> 00:56:56,460
there's nothing to decide,

1043
00:56:56,490 --> 00:56:59,760
basically both threads run when the done of the buffer,

1044
00:56:59,970 --> 00:57:02,310
no thread is pointing to that buffer anymore,

1045
00:57:02,310 --> 00:57:06,090
the garbage collector you know will trace you know starting from the threads stacks

1046
00:57:06,330 --> 00:57:09,870
and will never you know and will not count buffer any of the thread stacks

1047
00:57:09,870 --> 00:57:13,230
and therefore the garbage collector free the memory at some point later.

1048
00:57:13,720 --> 00:57:15,940
And so in a garbage collector language,

1049
00:57:15,940 --> 00:57:17,800
you don't have to think about this problem at all.

1050
00:57:19,990 --> 00:57:25,360
So you know one way you could try to solve this problem, in the kernel like C,

1051
00:57:25,360 --> 00:57:28,270
so you maybe put reference counts on the objects,

1052
00:57:28,270 --> 00:57:31,600
the reference count, of course have to be protected by locks,

1053
00:57:31,600 --> 00:57:33,550
perhaps were some atomic operations

1054
00:57:33,850 --> 00:57:35,830
and then when the reference count reaches zero,

1055
00:57:35,830 --> 00:57:38,470
then you can dereference it.

1056
00:57:40,640 --> 00:57:41,210
And it turns out,

1057
00:57:41,210 --> 00:57:43,970
like you know locks in reference counts are actually slightly expensive,

1058
00:57:44,000 --> 00:57:46,820
if you wanna high performance,

1059
00:57:46,820 --> 00:57:50,060
you know concurrency and scale up to the number of cores

1060
00:57:50,060 --> 00:57:51,980
and then actually can be a bottleneck,

1061
00:57:52,010 --> 00:57:53,270
then we'll see that later

1062
00:57:53,270 --> 00:57:55,070
in a couple weeks we read a paper,

1063
00:57:55,070 --> 00:57:57,170
that actually talks about this very explicitly.

1064
00:57:57,890 --> 00:57:59,510
And so people tend to,

1065
00:57:59,540 --> 00:58:02,510
want to do high performance, get good parallelism,

1066
00:58:02,510 --> 00:58:03,470
people tend to avoid them.

1067
00:58:04,020 --> 00:58:06,690
And in fact, in particular scenario,

1068
00:58:06,690 --> 00:58:09,120
we try to avoid them is like in read block,

1069
00:58:09,180 --> 00:58:12,120
you would like to make at least reading sort of lock free,

1070
00:58:12,150 --> 00:58:13,530
so you don't have to pay the cost.

1071
00:58:14,080 --> 00:58:17,020
And so, for example, here's a code fragment, that we do that,

1072
00:58:17,020 --> 00:58:18,370
here we have a get function,

1073
00:58:18,820 --> 00:58:21,490
basically you reads the head of a queue

1074
00:58:21,790 --> 00:58:24,670
and returns the whatever is at the head of the queue,

1075
00:58:25,310 --> 00:58:27,920
does it basically in a lock free manner,

1076
00:58:27,920 --> 00:58:31,400
use atomic_load to actually read the head,

1077
00:58:31,400 --> 00:58:32,990
but it doesn't actually take a lock out,

1078
00:58:33,510 --> 00:58:35,730
then, the writer does locks out.

1079
00:58:35,730 --> 00:58:40,310
So this is like lock free, the writers not lock free.

1080
00:58:41,280 --> 00:58:43,980
And this is a very common style in the Linux kernel,

1081
00:58:43,980 --> 00:58:46,290
and so the writer actually they takes the lock,

1082
00:58:46,320 --> 00:58:48,120
you know whatever looks at the head,

1083
00:58:48,120 --> 00:58:52,500
maybe is the pop function and pops of the head from the queue,

1084
00:58:52,950 --> 00:58:55,290
and then you know in principle you could reuse it,

1085
00:58:55,850 --> 00:58:58,420
and then unlocks, when you free the head.

1086
00:58:58,720 --> 00:59:02,680
Now again in,

1087
00:59:02,740 --> 00:59:04,240
you see, there's a little bit difficult,

1088
00:59:04,270 --> 00:59:05,860
when do you actually free the head,

1089
00:59:05,980 --> 00:59:07,510
because it could be the case,

1090
00:59:07,510 --> 00:59:09,190
that some other concurrent thread that [],

1091
00:59:09,190 --> 00:59:11,680
you know just before you did this atomic_store,

1092
00:59:11,980 --> 00:59:13,690
you know this guy actually came through

1093
00:59:13,690 --> 00:59:16,210
and basically got a pointer to that particular object,

1094
00:59:16,210 --> 00:59:18,520
so once you're done with this atomic_store,

1095
00:59:18,610 --> 00:59:20,290
you can't actually free the pointer,

1096
00:59:20,290 --> 00:59:23,500
because there could be another thread actually has a pointer to it,

1097
00:59:23,530 --> 00:59:24,880
and if you free it right here,

1098
00:59:24,880 --> 00:59:26,860
you could actually have a use-after-free bug.

1099
00:59:27,900 --> 00:59:32,490
And so, and so you know we'll see in a couple lectures,

1100
00:59:33,050 --> 00:59:36,380
there is currently a very clever solution for this

1101
00:59:36,770 --> 00:59:39,020
which is called read copy update or RCU,

1102
00:59:39,320 --> 00:59:40,790
basically what it does is

1103
00:59:40,790 --> 00:59:44,420
defers free of memory until really knows it's safe.

1104
00:59:45,050 --> 00:59:48,500
And it has a very clever scheme to actually decide how when it's safe,

1105
00:59:48,650 --> 00:59:51,470
but that scheme does come with all kinds of, comes with restrictions

1106
00:59:51,470 --> 00:59:54,140
and programmers actually have to obey some set of rules,

1107
00:59:54,500 --> 00:59:59,180
that you must follow for sort of RCU critical sections as they're called,

1108
00:59:59,700 --> 01:00:02,910
for example, you can't call just you can't call,

1109
01:00:02,910 --> 01:00:06,240
you can't go to sleep in an RCU critical section or schedule.

1110
01:00:06,930 --> 01:00:11,430
And so it turns out you know alrough the Linux kernel uses extremely successful,

1111
01:00:11,430 --> 01:00:12,780
you know a bit error prone

1112
01:00:12,780 --> 01:00:16,050
and requires careful program to get it right.

1113
01:00:16,860 --> 01:00:20,190
And in the case of the garbage collector language like you know Go,

1114
01:00:20,190 --> 01:00:22,170
this is a non issue,

1115
01:00:22,230 --> 01:00:26,370
because the garbage collector will actually determine when actually something is not in use anymore

1116
01:00:26,370 --> 01:00:27,450
and then only then free it.

1117
01:00:28,210 --> 01:00:31,150
And so there's nothing really you know there's no restrictions on the programmer,

1118
01:00:31,210 --> 01:00:33,910
just taken care of by the garbage collector.

1119
01:00:36,020 --> 01:00:37,280
So that's sort of an example of

1120
01:00:37,280 --> 01:00:42,800
you know where sort of more may be qualitatively or more explicit,

1121
01:00:42,800 --> 01:00:45,110
you can see through the advantage of a garbage collected language.

1122
01:00:45,920 --> 01:00:48,800
Okay, terms of the CVEs you know I sort of mentioned this already,

1123
01:00:49,100 --> 01:00:53,120
we went through all the CVEs and inspected them manually,

1124
01:00:53,420 --> 01:00:56,480
and then try to decide whether to actually Go can fix the problem.

1125
01:00:56,980 --> 01:00:59,020
There for 11 them we couldn't figure out,

1126
01:00:59,110 --> 01:01:03,130
you know we looked at the fix, the patch that addresses is,

1127
01:01:03,130 --> 01:01:06,610
we couldn't really figure out what the outcome in Go would like

1128
01:01:06,610 --> 01:01:08,680
or how would manifest or how we change,

1129
01:01:09,400 --> 01:01:11,080
we could see how the implement it to fix,

1130
01:01:11,080 --> 01:01:13,870
but couldn't decide whether it actually Go would avoided the problem or not.

1131
01:01:14,650 --> 01:01:17,560
A number of logic bugs in the CVEs

1132
01:01:17,560 --> 01:01:20,860
and so presumably Go you would make the same logic bugs in C

1133
01:01:20,860 --> 01:01:23,350
and you know the outcome would be the same.

1134
01:01:23,880 --> 01:01:27,120
But then there were about 40 memory safety bugs,

1135
01:01:27,150 --> 01:01:29,580
use-after-free or double-free or out-of-bounds

1136
01:01:29,970 --> 01:01:32,910
and in eight of these disappear,

1137
01:01:32,910 --> 01:01:34,770
because the garbage collector takes care of them

1138
01:01:34,770 --> 01:01:37,080
as described in the last couple slides.

1139
01:01:37,500 --> 01:01:40,800
And in 32 cases, Go would have generated panic,

1140
01:01:40,800 --> 01:01:43,440
because for example would Go outside of an array bound,

1141
01:01:44,090 --> 01:01:47,810
and of course panic is not good, you know the kernel crashes,

1142
01:01:47,990 --> 01:01:50,090
but it's probably better than a security exploit.

1143
01:01:50,880 --> 01:01:54,900
And so you know so 40 cases, you know basically the high-level language helped us.

1144
01:01:59,920 --> 01:02:03,070
Okay, so that's the qualitatively of the benefits,

1145
01:02:03,100 --> 01:02:07,690
so now I want to talk a little bit about the performance cost,

1146
01:02:07,690 --> 01:02:10,240
the high-level language tax.

1147
01:02:10,820 --> 01:02:11,750
Before doing that,

1148
01:02:11,750 --> 01:02:13,760
let me ask if there's any more questions?

1149
01:02:20,990 --> 01:02:23,480
Okay, I'm going to go through them,

1150
01:02:23,480 --> 01:02:25,880
I'm not sure we'll make it through all six,

1151
01:02:25,910 --> 01:02:28,520
because we reserve a couple minutes at least at the end,

1152
01:02:28,520 --> 01:02:32,600
it's going to come back to the starting point of the lecture, today's question.

1153
01:02:35,900 --> 01:02:38,150
So to set up in terms of experiments,

1154
01:02:38,180 --> 01:02:42,260
you know the Biscuit runs on raw hardware,

1155
01:02:42,770 --> 01:02:47,630
so these experiments are on little physical machines, not on top of QEMU,

1156
01:02:47,630 --> 01:02:51,950
is a 4 core, 2.8Ghz Intel processor,

1157
01:02:51,950 --> 01:02:54,560
16 GB RAM, but Hyperthreads disabled.

1158
01:02:54,590 --> 01:02:55,820
We use three applications,

1159
01:02:55,850 --> 01:02:57,530
a webserver, a key/value store

1160
01:02:57,620 --> 01:02:58,850
and a mail-server benchmark.

1161
01:02:59,220 --> 01:03:02,970
You know of these applications stress the kernel intensively

1162
01:03:03,030 --> 01:03:05,820
and so they run execute system calls

1163
01:03:05,820 --> 01:03:08,280
and the kernel must do a lot of work.

1164
01:03:09,050 --> 01:03:09,800
And you can see that,

1165
01:03:09,800 --> 01:03:12,590
because most of the time in these applications is spent in the kernel.

1166
01:03:15,060 --> 01:03:16,260
So first question is like,

1167
01:03:16,260 --> 01:03:19,110
is Linux [even] or is Biscuit [even] in the []

1168
01:03:19,110 --> 01:03:24,330
of production quality the kernel or industrial quality kernel,

1169
01:03:24,660 --> 01:03:25,650
and so what we did,

1170
01:03:25,650 --> 01:03:27,870
we compare the absolute through Biscuit and Linux,

1171
01:03:27,900 --> 01:03:31,800
for Linux, we used 4.9 Linux,

1172
01:03:31,800 --> 01:03:32,850
a little bit out of date now,

1173
01:03:32,850 --> 01:03:35,580
because paper is of course are a couple years old again.

1174
01:03:36,140 --> 01:03:39,230
But of course when Linux we have to disable all kinds of features

1175
01:03:39,230 --> 01:03:42,770
that Biscuit [uses] or doesn't provide I mean,

1176
01:03:42,770 --> 01:03:45,260
so like page-table isolation, retpoline,

1177
01:03:45,260 --> 01:03:47,690
you know all kinds of you know a long list of features

1178
01:03:47,690 --> 01:03:50,750
that actually Biscuit doesn't provide nor xv6 provides,

1179
01:03:51,170 --> 01:03:52,460
and we disable them on Linux

1180
01:03:52,460 --> 01:03:54,230
to make the comparison as fair as possible.

1181
01:03:54,880 --> 01:03:57,340
And of course you know some features are hard to disable,

1182
01:03:57,340 --> 01:03:59,800
you know we were not able to disabled,

1183
01:03:59,920 --> 01:04:02,110
but you know we tried to get as close as possible.

1184
01:04:02,820 --> 01:04:05,130
And then we measured basically the throughput.

1185
01:04:05,740 --> 01:04:10,990
And as you can see, the Biscuit is almost always slower,

1186
01:04:11,110 --> 01:04:12,820
which always slower than Linux,

1187
01:04:13,330 --> 01:04:17,380
you know CMailbench, you know it's about to get whatever 10%,

1188
01:04:17,380 --> 01:04:19,330
on NGINX a little bit more,

1189
01:04:19,450 --> 01:04:21,280
Redis is a little bit of 10 15 percent.

1190
01:04:21,820 --> 01:04:24,700
But you should use these numbers very grain as result, right,

1191
01:04:24,700 --> 01:04:27,250
because, you know they're not identical,

1192
01:04:27,460 --> 01:04:30,610
and it's not apples to apples comparison,

1193
01:04:30,700 --> 01:04:32,800
but they like to sort of first order

1194
01:04:33,070 --> 01:04:35,560
you know they're roughly the same ballpark at least,

1195
01:04:35,560 --> 01:04:38,500
you know they're not like 2x 3x 4x or 10x off,

1196
01:04:38,680 --> 01:04:43,450
and so you know maybe it's worthwhile to actually be able to do actually,

1197
01:04:43,860 --> 01:04:45,600
you know to draw some conclusions out of that.

1198
01:04:51,820 --> 01:04:53,050
So then we sort of looked at,

1199
01:04:53,050 --> 01:04:55,540
like you know we look basically profile the code

1200
01:04:55,540 --> 01:04:59,800
and try to bucket you know the cycles that were spent by the code,

1201
01:04:59,800 --> 01:05:03,970
particularly we're looking at which cycles were actually in the garbage collector,

1202
01:05:03,970 --> 01:05:07,570
which cycles were actually in the prologue function calls,

1203
01:05:07,570 --> 01:05:11,110
and prologues in Go does a bunch of work,

1204
01:05:11,110 --> 01:05:13,390
you know to ensure that the stack is large enough,

1205
01:05:13,390 --> 01:05:14,710
so you know run of the stack,

1206
01:05:15,130 --> 01:05:20,560
write barrier cycles, this is actually when in garbage collector mode,

1207
01:05:20,560 --> 01:05:24,550
you know the garbage collector turns on write barriers

1208
01:05:25,080 --> 01:05:28,860
to basically track pointers between different spaces.

1209
01:05:29,310 --> 01:05:31,470
And the safety cycles,

1210
01:05:31,650 --> 01:05:38,380
which are safety cycles are the cycles spent on array bound checks

1211
01:05:38,380 --> 01:05:40,330
and things like that, no pointer checks.

1212
01:05:42,500 --> 01:05:46,280
And so if you look at these applications you know here the numbers,

1213
01:05:46,400 --> 01:05:50,660
so 3% of the execution time was actually spent in sort of GC cycles,

1214
01:05:50,660 --> 01:05:54,200
and I'll talk a little bit about why that's low,

1215
01:05:54,200 --> 01:05:56,780
but you know in this case,

1216
01:05:56,780 --> 01:05:59,660
that the garbage collector running while running these applications,

1217
01:05:59,660 --> 01:06:01,790
so it's not the case that we measured the applications,

1218
01:06:01,790 --> 01:06:03,170
we give so much memory,

1219
01:06:03,170 --> 01:06:08,040
that you know just run without actually running the garbage collector,

1220
01:06:08,460 --> 01:06:11,910
surprisingly actually the prologue cycles turned out to be the highest,

1221
01:06:12,030 --> 01:06:14,640
and this is basically you know the way the scheme,

1222
01:06:14,640 --> 01:06:18,330
that we're using that time for checking whether the kernel stack

1223
01:06:18,330 --> 01:06:19,710
or the stack of a thread needed to

1224
01:06:19,710 --> 01:06:21,390
or Go routine needed to be grown or not,

1225
01:06:21,970 --> 01:06:25,660
and this is something that actually the Go design that point in a [],

1226
01:06:25,660 --> 01:06:27,340
that it's probably easier to get lower,

1227
01:06:27,790 --> 01:06:29,770
very little time actually write barrier is,

1228
01:06:29,770 --> 01:06:33,700
you know 2-3% you know in the safety cycles.

1229
01:06:34,570 --> 01:06:37,990
And so in some sense, you know there's good news,

1230
01:06:37,990 --> 01:06:42,280
you are not at, you know tax is not gigantic,

1231
01:06:42,280 --> 01:06:43,960
of course this number could be much higher,

1232
01:06:43,990 --> 01:06:49,420
because this is completely dependent on how many, how big you know the heap is

1233
01:06:49,420 --> 01:06:52,210
or live the live number of, live objects is,

1234
01:06:52,210 --> 01:06:55,060
because the garbage collector will have to trace all the live objects

1235
01:06:55,060 --> 01:06:56,860
to actually determine which objects are not alive.

1236
01:06:57,520 --> 01:07:00,460
And so if there's a lot of live objects,

1237
01:07:00,460 --> 01:07:02,680
you know the garbage collector will have to trace more objects,

1238
01:07:02,740 --> 01:07:06,700
and so this completely sort of linear with the number of live objects.

1239
01:07:07,290 --> 01:07:08,790
So we did some other experiments.

1240
01:07:09,380 --> 01:07:10,970
Let me zoom out a little bit,

1241
01:07:11,180 --> 01:07:13,970
where we basically allocate a ton of live data,

1242
01:07:14,300 --> 01:07:15,620
2 million vnodes,

1243
01:07:15,620 --> 01:07:17,450
think about this as 2 million inodes

1244
01:07:17,840 --> 01:07:20,600
and a free the amount of heap RAM

1245
01:07:20,630 --> 01:07:23,930
or change the amount of heap RAM the garbage collector has,

1246
01:07:23,930 --> 01:07:25,130
you know for free memory,

1247
01:07:25,340 --> 01:07:28,580
and then [] and then measure the cost.

1248
01:07:29,140 --> 01:07:30,670
So this is the table here,

1249
01:07:30,880 --> 01:07:33,280
we have like 640 megabytes is live data,

1250
01:07:33,610 --> 01:07:36,370
and there's running with different memory sizes,

1251
01:07:36,640 --> 01:07:40,090
and one sizes case, there are 320 megabytes of data,

1252
01:07:40,090 --> 01:07:41,860
so the ratio of live to free is 2,

1253
01:07:42,190 --> 01:07:43,810
you see that in that case,

1254
01:07:44,080 --> 01:07:48,580
Go does do a great imitation of [] overhead the garbage collector,

1255
01:07:48,580 --> 01:07:50,620
because the garbage collector needs to run a lot,

1256
01:07:50,650 --> 01:07:52,150
because it doesn't have much heap RAM.

1257
01:07:52,900 --> 01:07:56,590
But you know if you're basically if free memories about twice,

1258
01:07:56,590 --> 01:07:57,940
you know you could buy enough memory,

1259
01:07:57,940 --> 01:08:00,160
that free memory twice you know that the live memory,

1260
01:08:00,280 --> 01:08:05,230
then the garbage collection overhead is not actually that great, in the 9% range.

1261
01:08:05,890 --> 01:08:11,020
So basically to keep the GC overhead like a rough [] [], around below 10%,

1262
01:08:11,140 --> 01:08:15,670
you need about three times the heap size in terms of physical memory.

1263
01:08:19,920 --> 01:08:20,970
Any questions about this?

1264
01:08:23,500 --> 01:08:27,520
I had a question about the write barriers,

1265
01:08:27,520 --> 01:08:29,920
what are those, do you,

1266
01:08:29,980 --> 01:08:33,790
is it like you said some permissions?

1267
01:08:34,270 --> 01:08:38,740
You know, so if you remember the lecture for a little while ago,

1268
01:08:38,740 --> 01:08:41,560
the kind of Appel&Li paper paper,

1269
01:08:41,560 --> 01:08:44,200
where we talked about the to and from spaces.

1270
01:08:44,700 --> 01:08:47,370
And garbage collector runs,

1271
01:08:47,370 --> 01:08:50,940
then you have to check whether the pointers in the from space, right,

1272
01:08:50,940 --> 01:08:52,770
because it's in the [from] space, you have to copy it.

1273
01:08:53,420 --> 01:08:56,870
And basically that the write barrier are very similar,

1274
01:08:57,830 --> 01:08:59,270
and it's the same sort of type idea,

1275
01:08:59,270 --> 01:09:01,580
where you need to check every pointer

1276
01:09:01,580 --> 01:09:04,550
to see you actually actually point in space

1277
01:09:04,550 --> 01:09:06,200
that actually you need in garbage collector.

1278
01:09:07,300 --> 01:09:07,540
Okay.

1279
01:09:07,540 --> 01:09:08,470
That's write barrier.

1280
01:09:11,730 --> 01:09:15,210
Sorry, so like the free memory,

1281
01:09:15,210 --> 01:09:17,490
what is what is it exactly like how does it work,

1282
01:09:17,490 --> 01:09:19,320
that the live is more than free.

1283
01:09:19,740 --> 01:09:23,340
Oh yeah, yeah, okay so you buy some amount of memory,

1284
01:09:23,780 --> 01:09:26,900
and live memory is actually memory that was used by these vnodes,

1285
01:09:27,200 --> 01:09:29,750
and then there was another 320 megabyte was just free.

1286
01:09:30,540 --> 01:09:33,840
And so when this application allocated more vnodes,

1287
01:09:33,840 --> 01:09:35,610
the first came out of the free memory,

1288
01:09:35,610 --> 01:09:36,750
until the free memory is full,

1289
01:09:36,750 --> 01:09:39,210
[] then concurrently the garbage collector is running.

1290
01:09:40,350 --> 01:09:44,130
And, and so we're running like three configuration,

1291
01:09:44,130 --> 01:09:49,260
in one configuration, basically the amount of free memory twice as the live memory,

1292
01:09:49,920 --> 01:09:52,620
and so that means that the garbage collector has a lot of heap RAM

1293
01:09:53,010 --> 01:09:56,070
to do sort of concurrently while running with the application.

1294
01:09:56,730 --> 01:09:58,080
And if there's a lot of heap RAM,

1295
01:09:58,230 --> 01:09:59,970
in this case where free memory,

1296
01:09:59,970 --> 01:10:02,220
then you know the garbage collection overheads are not that high,

1297
01:10:03,310 --> 01:10:06,130
over there around 10% instead of 34%.

1298
01:10:07,520 --> 01:10:09,080
Okay, I see, I see, thank you.

1299
01:10:09,260 --> 01:10:11,150
Think about it like there's a little bit of slack

1300
01:10:11,180 --> 01:10:13,280
you know for the garbage collector to do its work.

1301
01:10:14,460 --> 01:10:18,240
Right, I I thought that it's like total 320, that was confused.

1302
01:10:18,270 --> 01:10:20,970
No, no, the total is 320 plus 640

1303
01:10:21,180 --> 01:10:23,940
and my last line is 640 plus 1280.

1304
01:10:24,650 --> 01:10:26,300
Okay, thank you.

1305
01:10:29,930 --> 01:10:31,610
I'm gonna skip this,

1306
01:10:31,700 --> 01:10:36,290
actually let me talk a little bit of pauses,

1307
01:10:36,290 --> 01:10:37,280
you know this is,

1308
01:10:37,860 --> 01:10:40,200
the Go garbage collector is a concurrent garbage collector

1309
01:10:40,200 --> 01:10:45,510
and short pauses stop the world for a very short period of time,

1310
01:10:45,510 --> 01:10:46,860
basically to enable write barriers

1311
01:10:46,860 --> 01:10:49,350
and then basically the application keep on running,

1312
01:10:49,650 --> 01:10:51,570
while the garbage collector doesn't work,

1313
01:10:51,690 --> 01:10:55,860
and it's incremental as like the one that we discussed a couple weeks ago,

1314
01:10:55,860 --> 01:10:59,940
where basically every call to new does a little bit of garbage collection work.

1315
01:11:00,800 --> 01:11:03,020
And so every time you do a little bit of garbage collection working,

1316
01:11:03,020 --> 01:11:05,390
there's some some delay that's been cost, right.

1317
01:11:06,110 --> 01:11:09,560
And so we measured you know the,

1318
01:11:09,560 --> 01:11:13,580
it took one application and looked at the maximum pause time,

1319
01:11:13,580 --> 01:11:15,920
so the maximum time an application can be stopped

1320
01:11:15,920 --> 01:11:18,050
and of course the garbage collector needs to do some work.

1321
01:11:19,140 --> 01:11:24,840
And it turned out to be the max single pause 150 microseconds,

1322
01:11:25,200 --> 01:11:28,800
that's in the case of the webserver that was using the TCP stack,

1323
01:11:28,800 --> 01:11:33,120
and basically, a large part of the TCP connection table needed to be marked,

1324
01:11:33,150 --> 01:11:36,930
before you know continuing, that took 115 microseconds.

1325
01:11:37,660 --> 01:11:44,140
The maximum total pause time for a single Nginx http request is

1326
01:11:44,140 --> 01:11:46,420
the sum of the number of single pauses,

1327
01:11:46,420 --> 01:11:51,670
and the maximum pause time in total for a single request was 582 microseconds,

1328
01:11:51,910 --> 01:11:54,430
so [] when the request comes into the machine,

1329
01:11:54,490 --> 01:12:00,850
during you know there was total delay of 582 microseconds, execute that request.

1330
01:12:03,100 --> 01:12:06,100
And it just happened very, very seldom,

1331
01:12:06,190 --> 01:12:11,620
you know only you know point 3% requested times actually had a delay of more than 100 microseconds.

1332
01:12:12,380 --> 01:12:14,840
And so you know that's not good,

1333
01:12:14,840 --> 01:12:17,690
if you're trying to achieve like an SLA

1334
01:12:17,690 --> 01:12:24,850
or where basically the longest period of time of request takes you know it's small,

1335
01:12:25,360 --> 01:12:29,980
but you know the, you look at you know google papers about like tail at scale,

1336
01:12:29,980 --> 01:12:33,280
like how long the longest request takes,

1337
01:12:33,280 --> 01:12:34,210
you know they're talking about

1338
01:12:34,210 --> 01:12:37,480
the order of tens of milliseconds, milliseconds or 10 milliseconds,

1339
01:12:37,690 --> 01:12:43,870
and so probably the programs that these particular programs that actually have a pause

1340
01:12:43,870 --> 01:12:47,050
with maximum [] to pause for 582 microseconds,

1341
01:12:47,050 --> 01:12:48,250
sort of within the budget.

1342
01:12:48,860 --> 01:12:51,290
You know it's not ideal, but it's not crazy,

1343
01:12:51,380 --> 01:12:53,810
and so basically says that actually the,

1344
01:12:55,110 --> 01:12:56,610
really, what this basically says that,

1345
01:12:56,610 --> 01:12:59,550
the Go designers you know did actually terribly good job

1346
01:12:59,550 --> 01:13:01,170
of actually implementing their garbage collector,

1347
01:13:02,070 --> 01:13:03,420
or impressively good job.

1348
01:13:04,160 --> 01:13:07,250
And this is one of those things that we've noticed while doing this project,

1349
01:13:07,250 --> 01:13:09,200
every time we upgraded the Go runtime,

1350
01:13:09,320 --> 01:13:11,870
the next runtime, it came with a better garbage collector,

1351
01:13:11,870 --> 01:13:13,490
and actually these numbers got better and better.

1352
01:13:17,910 --> 01:13:22,050
Okay, one more through technical detail that I want to go over,

1353
01:13:22,050 --> 01:13:26,730
so far you know like the first comparison between Linux and Biscuit,

1354
01:13:26,730 --> 01:13:27,930
you know it's not really fair,

1355
01:13:27,930 --> 01:13:32,490
because Biscuit and Linux implement slightly different features,

1356
01:13:32,760 --> 01:13:34,230
so we did one more experiment,

1357
01:13:34,230 --> 01:13:38,550
where we basically tried to code up two kernel paths completely identical,

1358
01:13:38,730 --> 01:13:43,650
evolving in Linux and in like in C and Go,

1359
01:13:43,770 --> 01:13:48,660
and so we looked at the code path and sort of verify it,

1360
01:13:48,660 --> 01:13:51,180
basically you know it implements exactly the same thing,

1361
01:13:51,180 --> 01:13:52,710
and we look at the assembly structures,

1362
01:13:52,710 --> 01:13:55,410
you know to really see what what the differences are,

1363
01:13:55,500 --> 01:13:56,760
there gonna be some differences,

1364
01:13:56,760 --> 01:13:59,070
because Go is going to pay the safety checks,

1365
01:13:59,340 --> 01:14:02,640
but just in terms of basic operation,

1366
01:14:02,640 --> 01:14:05,790
that at least two code paths are identical in terms of functionality.

1367
01:14:07,380 --> 01:14:09,960
And we did that for two code paths,

1368
01:14:09,960 --> 01:14:11,070
you know it's difficult to do,

1369
01:14:11,070 --> 01:14:13,650
because it's painstaking job,

1370
01:14:13,650 --> 01:14:14,460
we did for two,

1371
01:14:14,700 --> 01:14:15,840
or Cody did actually for two.

1372
01:14:16,410 --> 01:14:17,490
And then we compare them,

1373
01:14:18,040 --> 01:14:20,350
and so here's results from one of them,

1374
01:14:20,350 --> 01:14:22,840
this is pipe ping-pong you know sort of test,

1375
01:14:22,840 --> 01:14:24,970
you know ping-pong you know byte across a pipe,

1376
01:14:25,180 --> 01:14:27,430
and we just looked at the code path through the kernel

1377
01:14:27,430 --> 01:14:30,880
to actually get a byte from one end to the pipe to the other end of the pipe.

1378
01:14:31,790 --> 01:14:39,100
You know, sort of amount code in Go is like this 1.2k lines code

1379
01:14:39,100 --> 01:14:42,070
and C it's 1.8k lines of code,

1380
01:14:42,400 --> 01:14:44,140
and there's no allocation, no GC,

1381
01:14:44,140 --> 01:14:46,660
so those things are just a differ,

1382
01:14:46,720 --> 01:14:48,610
we also looked at runtime,

1383
01:14:48,610 --> 01:14:51,400
like where's the most time spent in both code paths,

1384
01:14:51,400 --> 01:14:54,360
you know the same top-10 instructions showed up,

1385
01:14:54,360 --> 01:14:57,930
so we have some confidence that the code paths really are closer,

1386
01:14:58,080 --> 01:15:00,850
closer, you can get to make them similar.

1387
01:15:01,390 --> 01:15:04,570
And then we looked at basically the amount of operations you can do per second,

1388
01:15:04,630 --> 01:15:06,490
and as you see here,

1389
01:15:06,580 --> 01:15:11,650
basically you know Go a little slower than the C implementation

1390
01:15:12,010 --> 01:15:15,610
and you know the ratio is about 1.15% slower.

1391
01:15:16,260 --> 01:15:21,180
And, that's you know you look at the prologue/safety-checks,

1392
01:15:21,180 --> 01:15:24,150
you know these are all the instructions that C code does not have to execute,

1393
01:15:24,270 --> 01:15:29,310
it turned out to be 16% more assembly instructions,

1394
01:15:29,370 --> 01:15:32,280
and so that's sort of roughly sort of makes sense.

1395
01:15:32,830 --> 01:15:36,070
So you know the main conclusion is you know Go is slower,

1396
01:15:36,280 --> 01:15:39,610
but pretty competitive, you know not not ridiculously slower.

1397
01:15:40,570 --> 01:15:42,790
And that seems in line with the early results of

1398
01:15:42,790 --> 01:15:45,610
where we did these Linux to Biscuit comparison directly.

1399
01:15:47,790 --> 01:15:50,920
Okay, so let me zoom a little bit further,

1400
01:15:50,950 --> 01:15:52,720
let me skip this,

1401
01:15:52,750 --> 01:15:54,370
because I want to talk a little bit about,

1402
01:15:54,460 --> 01:15:57,220
this sort of the question that we asked in the beginning,

1403
01:15:57,220 --> 01:15:59,740
where should one use high-level language for a new kernel.

1404
01:16:00,830 --> 01:16:03,950
And, maybe you know like instead of answering,

1405
01:16:03,950 --> 01:16:05,750
I have some thoughts about this here in this slide,

1406
01:16:05,750 --> 01:16:07,370
you know there were some conclusion that we draw,

1407
01:16:07,370 --> 01:16:10,400
and you know it's not a crisp conclusion, some considerations,

1408
01:16:10,760 --> 01:16:13,490
so maybe to take a step back and ask yourself the question,

1409
01:16:13,490 --> 01:16:15,940
like what would you have preferred,

1410
01:16:15,940 --> 01:16:19,510
you know, would you have preferred to write you know xv6 and the labs in C

1411
01:16:19,510 --> 01:16:23,020
or would you prefer to use a high-level language for example like Go.

1412
01:16:23,660 --> 01:16:25,940
And particularly answer this question,

1413
01:16:25,940 --> 01:16:28,190
what what kind of bugs would you have avoided,

1414
01:16:28,220 --> 01:16:31,520
and maybe you have some time during this lecture

1415
01:16:31,520 --> 01:16:33,320
to think about like what bugs you had,

1416
01:16:33,770 --> 01:16:36,860
and I would love to hear you know what your experience,

1417
01:16:38,160 --> 01:16:41,610
how do you think switching to a high level language

1418
01:16:41,610 --> 01:16:43,050
would have changed your experience.

1419
01:16:45,090 --> 01:16:47,460
Or if you have any thoughts on this question at all.

1420
01:16:51,460 --> 01:16:55,010
Let me, pause you for a little bit,

1421
01:16:55,010 --> 01:16:57,560
so you can think about this and maybe chime in.

1422
01:16:59,120 --> 01:17:02,180
I have had a couple of times when I did the thing,

1423
01:17:02,180 --> 01:17:04,250
where I create an object in a function

1424
01:17:04,460 --> 01:17:06,980
and then return a pointer to it

1425
01:17:06,980 --> 01:17:08,540
and then I do stuff with a pointer

1426
01:17:08,840 --> 01:17:11,120
and then I realized that the object is gone.

1427
01:17:11,450 --> 01:17:15,670
Yeah, so this is a classic example of sort of use-after-free case, correct.

1428
01:17:17,810 --> 01:17:22,010
Yeah, the second time I realize it faster than the first time.

1429
01:17:22,440 --> 01:17:23,490
Yeah, it's definitely true,

1430
01:17:23,490 --> 01:17:25,320
when you see a couple of times in those bugs,

1431
01:17:25,320 --> 01:17:26,340
here you get better at them.

1432
01:17:26,990 --> 01:17:29,180
Any other thoughts on this,

1433
01:17:29,180 --> 01:17:31,340
you know what the experience that people have had.

1434
01:17:33,110 --> 01:17:37,090
Think about your worst bugs, bugs that took most time,

1435
01:17:38,820 --> 01:17:40,830
would high-level language would have help?

1436
01:17:43,320 --> 01:17:47,640
I think it definitely like like some of the bugs were absolutely terrible to deal with,

1437
01:17:47,700 --> 01:17:51,000
but at the same time like in in this context,

1438
01:17:51,000 --> 01:17:55,170
I definitely appreciated having to work with such a low-level language C,

1439
01:17:55,260 --> 01:17:58,320
because it helped me to really gain a very,

1440
01:17:58,410 --> 01:18:02,670
a deep understanding of what's actually going on inside the operating system,

1441
01:18:02,670 --> 01:18:04,260
like how it's working with memory,

1442
01:18:04,260 --> 01:18:07,260
like it it it's definitely refreshing to to

1443
01:18:07,260 --> 01:18:09,630
like not have all of that abstracted away

1444
01:18:09,630 --> 01:18:12,540
and to actually see exactly what's going on.

1445
01:18:15,560 --> 01:18:16,430
Yeah, it makes a lot of sense,

1446
01:18:16,430 --> 01:18:19,170
any other, any other people have opinions on this?

1447
01:18:20,070 --> 01:18:24,300
I think also made a lot of bugs

1448
01:18:24,300 --> 01:18:32,660
in which I was writing after the end of string or something like that,

1449
01:18:32,720 --> 01:18:36,650
but then I wasn't getting any useful feedback about it

1450
01:18:36,650 --> 01:18:40,610
and then very strange things happened, that I couldn't explain,

1451
01:18:41,150 --> 01:18:42,890
so yeah, I.

1452
01:18:42,920 --> 01:18:44,900
That show up in lab one,

1453
01:18:46,130 --> 01:18:47,900
where there's a bunch of strange operations,

1454
01:18:47,900 --> 01:18:50,210
like when you parsing directories and things like that.

1455
01:18:51,410 --> 01:18:53,000
It showed up in multiple apps.

1456
01:18:53,390 --> 01:18:56,180
Okay, I'm not surprised.

1457
01:18:56,570 --> 01:18:57,830
Okay, that's a great example,

1458
01:18:57,830 --> 01:19:01,280
like you know that it's very nice to actually have real string objects.

1459
01:19:03,870 --> 01:19:05,040
Something on my end is that,

1460
01:19:05,040 --> 01:19:09,390
I I found myself lacking whenever I needed something like a map,

1461
01:19:09,690 --> 01:19:14,640
and I just I I [] every time I needed to do for loop over something and then find.

1462
01:19:15,160 --> 01:19:15,700
Yeah.

1463
01:19:15,700 --> 01:19:16,960
However, I will say,

1464
01:19:16,960 --> 01:19:20,020
like coming from a high-level programming background,

1465
01:19:20,500 --> 01:19:23,650
this was my first real exposure to something like C,

1466
01:19:23,710 --> 01:19:25,210
so going of of Noah's point,

1467
01:19:25,210 --> 01:19:27,010
it kind of helped me to understand

1468
01:19:27,070 --> 01:19:28,390
really what it means that,

1469
01:19:29,140 --> 01:19:31,840
this code that I'm writing is actually running on the CPU

1470
01:19:31,840 --> 01:19:34,000
and everything is from the perspective of the CPU.

1471
01:19:34,360 --> 01:19:42,460
Yep, any other thoughts?

1472
01:19:46,580 --> 01:19:48,770
Oh, I actually remember it was specifically,

1473
01:19:48,770 --> 01:19:54,590
the difference between safe string copy or just string copy,

1474
01:19:54,830 --> 01:19:58,850
one of them was putting, was using the null terminator,

1475
01:19:58,850 --> 01:20:00,200
and that was.

1476
01:20:00,200 --> 01:20:04,270
Yeah, a common C bug.

1477
01:20:04,390 --> 01:20:07,720
Well, so you know first thing you know, thanks for the input,

1478
01:20:07,870 --> 01:20:12,100
of course we're not going to change xv6 to Go

1479
01:20:12,100 --> 01:20:13,300
or any high-level language exactly,

1480
01:20:13,300 --> 01:20:16,990
for the reasons that you know have a number of you like Noah, Amir mentioned,

1481
01:20:18,140 --> 01:20:20,180
Go still hides too much,

1482
01:20:20,210 --> 01:20:21,710
and you know in this particular class,

1483
01:20:21,710 --> 01:20:24,350
whole purpose is really trying to understand everything

1484
01:20:24,350 --> 01:20:27,470
between the CPU and the system call interface,

1485
01:20:27,560 --> 01:20:30,890
and for example Go of course hides threads,

1486
01:20:31,040 --> 01:20:33,410
and you know we don't want to hide that,

1487
01:20:33,410 --> 01:20:35,660
we want to explain to you actually how threads are implemented,

1488
01:20:35,900 --> 01:20:39,710
and so, we would not want to hide this from you.

1489
01:20:40,190 --> 01:20:41,660
So certainly future years,

1490
01:20:41,660 --> 01:20:45,440
you know of xv6 class will keep on using C,

1491
01:20:45,770 --> 01:20:47,750
but if you implement a new kernel

1492
01:20:47,750 --> 01:20:53,700
and goal is not you know educating students about kernels,

1493
01:20:53,700 --> 01:20:57,900
but goals are I'd like to say safe, you know high performance kernel,

1494
01:20:58,050 --> 01:21:00,990
you know there's sort of you know somethings you conclude from this study,

1495
01:21:00,990 --> 01:21:03,000
they've done and what we've done, right.

1496
01:21:03,500 --> 01:21:06,830
Yeah, memory or performance is really paramount,

1497
01:21:06,860 --> 01:21:10,730
you know you can't sacrifice 15%, then you should probably use C,

1498
01:21:10,820 --> 01:21:14,390
if you want to minimize memory use, you probably use C too.

1499
01:21:15,180 --> 01:21:17,130
If safety is important or security is important

1500
01:21:17,130 --> 01:21:18,900
and probably the high-level language is the way to go.

1501
01:21:19,600 --> 01:21:24,550
And probably in many cases performances merely important as opposed to absolute paramount,

1502
01:21:24,820 --> 01:21:25,600
and in many case,

1503
01:21:25,600 --> 01:21:29,140
I think you know using high-level languages perfectly reasonable thing to do for kernel.

1504
01:21:30,220 --> 01:21:31,720
Probably one thing I've learned,

1505
01:21:31,720 --> 01:21:35,500
probably you know Cody Robert and I learned from this whole project is

1506
01:21:35,500 --> 01:21:38,050
like whatever programming language is a programming language,

1507
01:21:38,050 --> 01:21:39,880
and you can use it to build kernels,

1508
01:21:39,880 --> 01:21:41,380
you can build user applications,

1509
01:21:41,500 --> 01:21:44,530
there's not really standing anything in really in a way.

1510
01:21:50,800 --> 01:21:53,170
Okay, you know I think it's time to wrap up,

1511
01:21:53,170 --> 01:21:56,320
you know if you have any more questions,

1512
01:21:56,320 --> 01:21:59,620
you know fell free to hang around and ask them,

1513
01:21:59,620 --> 01:22:01,150
if you have to go somewhere else,

1514
01:22:01,510 --> 01:22:04,540
good luck with finishing the mmap lab,

1515
01:22:04,540 --> 01:22:05,440
and for those of you,

1516
01:22:05,440 --> 01:22:08,110
when we have to are leaving campus for Thanksgiving,

1517
01:22:08,320 --> 01:22:11,920
safe travels and hope to see you after Thanksgiving

1518
01:22:11,920 --> 01:22:13,630
in Monday's lecture after Thanksgiving.

1519
01:22:17,650 --> 01:22:18,130
Thank you.

1520
01:22:20,250 --> 01:22:22,800
I was curious, how did you implement it,

1521
01:22:22,830 --> 01:22:25,380
it said you are doing that just on the hardware,

1522
01:22:25,440 --> 01:22:29,100
so like when you start out, how do you start out.

1523
01:22:30,010 --> 01:22:32,290
You know there's basically little shim code,

1524
01:22:32,380 --> 01:22:34,840
that sets up enough of the hardware,

1525
01:22:34,840 --> 01:22:37,390
so that when Biscuit you know asks for,

1526
01:22:37,390 --> 01:22:41,320
when the Go runtime ask for memory for the heap,

1527
01:22:41,320 --> 01:22:42,940
that we can actually respond.

1528
01:22:44,240 --> 01:22:50,270
That was one of the main things that actually Go runtime actually relies on.

1529
01:22:51,460 --> 01:22:53,200
Right, I guess I was like you said that,

1530
01:22:53,200 --> 01:22:57,490
you didn't use a virtual machine for that, so.

1531
01:22:57,490 --> 01:23:02,830
We did, of course we developed most of development on QEMU

1532
01:23:02,830 --> 01:23:06,280
and of course again we actually have to get it running on the hardware,

1533
01:23:06,370 --> 01:23:08,140
that also costs there's a bunch of problems,

1534
01:23:08,140 --> 01:23:09,580
because the boot loaders are different,

1535
01:23:09,580 --> 01:23:11,380
there's a bunch of boot code that you actually need to write,

1536
01:23:11,380 --> 01:23:13,210
you don't have to write if you run it on QEMU,

1537
01:23:13,600 --> 01:23:15,220
and that kind of stuff,

1538
01:23:15,220 --> 01:23:17,680
but most of the development is all done on QEMU,

1539
01:23:17,680 --> 01:23:21,460
in fact if you want to actually show you running Biscuit on QEMU,

1540
01:23:21,640 --> 01:23:23,260
and it looks very simple to xv6,

1541
01:23:23,260 --> 01:23:25,510
you know the only thing it does like show prompt,

1542
01:23:25,510 --> 01:23:28,120
there's no window system, nothing like that.

1543
01:23:29,800 --> 01:23:30,760
Okay, I see,

1544
01:23:31,000 --> 01:23:34,720
so like what happens if you if you make a mistake in the boot code.

1545
01:23:35,660 --> 01:23:36,500
It doesn't boot,

1546
01:23:36,530 --> 01:23:37,910
you know basically nothing happens,

1547
01:23:37,910 --> 01:23:39,620
you know it's completely nothing.

1548
01:23:40,370 --> 01:23:41,720
How do you know?

1549
01:23:42,460 --> 01:23:44,920
You, you, you will know,

1550
01:23:44,920 --> 01:23:49,060
because you know okay what will happen is because you don't see print statement,

1551
01:23:49,060 --> 01:23:51,160
like xv6 for the first thing we print is

1552
01:23:51,160 --> 01:23:54,820
like you know xv6 hello or something or xv6 booting,

1553
01:23:55,180 --> 01:23:56,800
you won't see anything like that,

1554
01:23:57,250 --> 01:23:58,930
and so you'll see nothing,

1555
01:23:59,080 --> 01:24:04,000
and then you'll have to track down and guess what the problem might be.

1556
01:24:05,640 --> 01:24:08,130
Okay, so you do it by looking?

1557
01:24:08,660 --> 01:24:12,410
Okay, little bit, you can write a [synchronously] to the UART,

1558
01:24:12,440 --> 01:24:13,730
you know you could like,

1559
01:24:13,940 --> 01:24:17,750
you know characters you see put them in random places in the code,

1560
01:24:17,750 --> 01:24:18,830
and hope that you see something.

1561
01:24:21,440 --> 01:24:23,090
This is interesting, thank you.

1562
01:24:23,420 --> 01:24:23,660
You know.

1563
01:24:23,660 --> 01:24:25,610
I wanted to ask,

1564
01:24:26,000 --> 01:24:30,050
when you, so I know like you implemented the Go,

1565
01:24:30,050 --> 01:24:32,960
some of the calls that Go runtime would make,

1566
01:24:32,990 --> 01:24:34,730
that you cannot make,

1567
01:24:34,730 --> 01:24:36,680
because you're implementing the kernel itself,

1568
01:24:37,130 --> 01:24:38,780
is there any like,

1569
01:24:38,780 --> 01:24:41,480
did you just implement just all of that in assembly

1570
01:24:41,510 --> 01:24:45,200
or did you say okay like some of this we can still do in Go,

1571
01:24:45,200 --> 01:24:47,480
like we can bring Go a bit closer,

1572
01:24:47,480 --> 01:24:49,520
and then do assembly only what's necessary,

1573
01:24:49,640 --> 01:24:53,720
what did you say like once the Go runtime ends like that's assembly.

1574
01:24:54,230 --> 01:24:58,070
You the that's where the 1500 lines of assembly came from,

1575
01:24:58,650 --> 01:25:00,990
in the, in Biscuit,

1576
01:25:01,170 --> 01:25:02,760
you know that is basically the code

1577
01:25:02,760 --> 01:25:06,420
to sort of get everything ready to be actually able to run the Go runtime.

1578
01:25:08,450 --> 01:25:10,040
Now some of that we would have implemented C,

1579
01:25:10,040 --> 01:25:11,030
but we didn't want to do that,

1580
01:25:11,030 --> 01:25:12,260
because we didn't want to use any C,

1581
01:25:12,260 --> 01:25:13,160
so we're use assembly.

1582
01:25:13,900 --> 01:25:15,940
And many of it actually required assembly,

1583
01:25:15,940 --> 01:25:17,110
because it's in the booting part.

1584
01:25:18,250 --> 01:25:20,980
Right, but I guess some of the part, that's not the boot,

1585
01:25:20,980 --> 01:25:26,410
so I I you know I know that some just you cannot avoid some boot code and assembly,

1586
01:25:26,410 --> 01:25:30,400
but could you, could you have transformed some of the assembly to Go,

1587
01:25:30,400 --> 01:25:32,170
or did you go to the [].

1588
01:25:32,170 --> 01:25:34,240
We did a bunch of Go,

1589
01:25:34,240 --> 01:25:38,050
that basically runs very early on,

1590
01:25:38,480 --> 01:25:41,060
you know some of the Go goes quite careful,

1591
01:25:41,120 --> 01:25:42,860
it doesn't do any memory allocation,

1592
01:25:46,820 --> 01:25:49,490
and we tried to write as much as possible Go,

1593
01:25:49,520 --> 01:25:52,010
I I can like I have to look at the code exactly

1594
01:25:52,010 --> 01:25:54,320
you know to be able to answer your question,

1595
01:25:54,320 --> 01:25:57,050
specifically you can look at the git repo,

1596
01:25:57,050 --> 01:26:00,790
but, yeah, we tried to write everything in Go.

1597
01:26:03,580 --> 01:26:06,520
And then one like kind of unrelated small question I had,

1598
01:26:06,880 --> 01:26:09,970
what does Go do with its Go routines,

1599
01:26:09,970 --> 01:26:13,180
that makes it possible to run like hundred thousands of them,

1600
01:26:14,040 --> 01:26:17,520
because you cannot just spin up a hundred thousand pthreads, right.

1601
01:26:18,210 --> 01:26:19,680
Yeah, it depends,

1602
01:26:19,710 --> 01:26:23,340
a lot longer

1603
01:26:23,340 --> 01:26:26,760
and so the main issue is that you need to allocate stack,

1604
01:26:27,320 --> 01:26:31,280
and the Go runtime actually allocates stack incrementally

1605
01:26:31,490 --> 01:26:36,350
and so grows them dynamically as you run your Go Go routine,

1606
01:26:36,500 --> 01:26:38,870
this where's prologue code is for,

1607
01:26:38,960 --> 01:26:40,160
when you make a function call,

1608
01:26:40,160 --> 01:26:41,570
you see if there's enough space

1609
01:26:41,570 --> 01:26:44,000
to actually make the function call,

1610
01:26:44,000 --> 01:26:46,520
and if not, it will grow dynamically for you.

1611
01:26:47,390 --> 01:26:50,390
And often in pthread implementations,

1612
01:26:50,570 --> 01:26:55,370
allocating threads a little bit more heavyweight,

1613
01:26:55,370 --> 01:27:00,500
because actually for example Linux basically corresponds

1614
01:27:00,500 --> 01:27:02,810
corresponding kernel threads is actually allocated too,

1615
01:27:03,680 --> 01:27:05,930
and they tend to be more heavyweight than.

1616
01:27:08,790 --> 01:27:09,390
I see,

1617
01:27:10,090 --> 01:27:14,830
is the is the scheduling of all the Go routines done completely in user space,

1618
01:27:14,860 --> 01:27:17,860
or does it help itself with some of kernel, so.

1619
01:27:18,400 --> 01:27:20,950
It is mostly done in user space.

1620
01:27:25,930 --> 01:27:28,900
So the Go runtime allocates a bunch of kernel threads,

1621
01:27:29,520 --> 01:27:32,790
you know they call them I think mthreads,

1622
01:27:32,790 --> 01:27:36,000
and on top of that it implements the Go routines.

1623
01:27:37,370 --> 01:27:40,460
So it's so it has, like a couple kernel threads

1624
01:27:40,460 --> 01:27:43,520
that it shares to all Go routines based on which ones running.

1625
01:27:43,880 --> 01:27:44,270
Yes.

1626
01:27:44,920 --> 01:27:46,240
Oh, that makes sense, yeah.

1627
01:27:47,580 --> 01:27:51,600
Is there like any C C++ equivalent,

1628
01:27:51,750 --> 01:27:56,130
like, could you could you do something like that to save to save some memory.

1629
01:27:56,340 --> 01:27:59,010
Yeah people have done, you know able to manage like high performance,

1630
01:27:59,010 --> 01:28:02,310
you know so C libraries are thread libraries,

1631
01:28:02,310 --> 01:28:05,730
that way you can create thousands of you know threads for millions of threads,

1632
01:28:05,730 --> 01:28:08,070
you know do a similar style.

1633
01:28:14,160 --> 01:28:17,100
Okay, you guys have a good break, I must head out.

1634
01:28:17,130 --> 01:28:17,730
You too.

1635
01:28:18,060 --> 01:28:24,000
See you next week or two weeks.

1636
01:28:24,490 --> 01:28:25,750
Yeah, oh.

1637
01:28:26,660 --> 01:28:27,410
Oh, go ahead.

1638
01:28:29,220 --> 01:28:34,620
I'm sorry, so I have a maybe basic question about the shims

1639
01:28:34,620 --> 01:28:36,900
and I guess I think also maybe,

1640
01:28:36,900 --> 01:28:41,580
I'm just not familiar with kind of specifically what like a runtime is

1641
01:28:41,790 --> 01:28:44,370
and I guess like my confusion comes from the fact,

1642
01:28:44,370 --> 01:28:48,510
that like from a metal model of how xv6 and C works,

1643
01:28:48,510 --> 01:28:51,540
is that C compiles C is a compiled language

1644
01:28:51,840 --> 01:28:54,690
and so it goes directly to assembly or machine code,

1645
01:28:54,930 --> 01:28:58,110
and so it kind of just runs on the CPU,

1646
01:28:58,230 --> 01:28:59,520
and so I guess,

1647
01:28:59,550 --> 01:29:04,320
like there is no need for a shim for like xv6 OS,

1648
01:29:04,740 --> 01:29:08,220
but I guess my understanding is Go is also a compiled language,

1649
01:29:08,220 --> 01:29:10,050
so it also goes to the assembly,

1650
01:29:10,170 --> 01:29:13,410
so why is there a need for like a shim in this case,

1651
01:29:13,410 --> 01:29:16,800
why, why is there maybe is a shim for like xv6,

1652
01:29:16,800 --> 01:29:18,420
or you know what, what is different here

1653
01:29:18,420 --> 01:29:22,590
and why are there, why are there things that can't just be done on the CPU.

1654
01:29:23,070 --> 01:29:24,630
Yeah yeah, a great question,

1655
01:29:24,660 --> 01:29:27,270
so I think the answer to your question is that,

1656
01:29:27,270 --> 01:29:30,570
the Go runtime provides all kinds of features,

1657
01:29:30,570 --> 01:29:32,850
that like you know you don't have, right,

1658
01:29:32,850 --> 01:29:35,910
in running when you're running C in xv6,

1659
01:29:36,210 --> 01:29:38,100
so Go runtime provides threads,

1660
01:29:38,250 --> 01:29:40,350
Go runtime provides scheduler,

1661
01:29:40,350 --> 01:29:43,380
Go runtime provides hash tables,

1662
01:29:43,380 --> 01:29:45,150
Go runtime provides garbage collector,

1663
01:29:45,420 --> 01:29:47,130
that actually needs to run at runtime, right,

1664
01:29:47,130 --> 01:29:49,410
then there's no garbage collector in xv6,

1665
01:29:49,860 --> 01:29:51,780
and we implement the threads,

1666
01:29:52,200 --> 01:29:55,020
and for example to support the garbage collector,

1667
01:29:55,020 --> 01:29:57,420
it needs a heap you know for to allocate memory from,

1668
01:29:57,450 --> 01:30:00,270
and so like ask the operating system underlying operating system,

1669
01:30:00,270 --> 01:30:01,980
so please give me you know some memories,

1670
01:30:01,980 --> 01:30:02,940
that I can use it as a heap.

1671
01:30:03,610 --> 01:30:08,350
And basically the shim layer implements exactly so that kind of functionality,

1672
01:30:08,350 --> 01:30:13,330
that the Go runtime needs to do a job at runtime.

1673
01:30:23,120 --> 01:30:24,590
I see.

1674
01:30:25,200 --> 01:30:28,860
Yeah, you have a slightly makes sense,

1675
01:30:28,860 --> 01:30:30,930
I actually a follow-up question is,

1676
01:30:32,550 --> 01:30:34,140
maybe this is a dumb question,

1677
01:30:34,140 --> 01:30:42,250
but like like can we just compile to runtime down to machine code, or.

1678
01:30:42,250 --> 01:30:44,290
Runtime is compile to run.

1679
01:30:44,290 --> 01:30:44,740
Okay.

1680
01:30:44,860 --> 01:30:47,350
It goes like you know the runtime itself is also compiled,

1681
01:30:47,350 --> 01:30:48,700
but it's like you're part of the program

1682
01:30:48,700 --> 01:30:51,460
that needs to run always running run Go code.

1683
01:30:52,190 --> 01:30:53,090
It has to be there,

1684
01:30:53,180 --> 01:30:55,190
like even like C has a small runtime,

1685
01:30:55,190 --> 01:30:57,620
if you think about you know we have like printf

1686
01:30:57,620 --> 01:30:59,210
is part of should live to C runtime,

1687
01:30:59,600 --> 01:31:02,180
string operations are part of the C runtime, right,

1688
01:31:02,480 --> 01:31:03,650
they're compile too,

1689
01:31:03,650 --> 01:31:07,280
but there's a bunch of small number of functions that the C runtime has,

1690
01:31:07,430 --> 01:31:10,760
but their runtime is so small compared to the Go runtime,

1691
01:31:10,760 --> 01:31:12,890
that you know have to support many more features,

1692
01:31:13,160 --> 01:31:16,010
because of you know programs Go programs rely on them.

1693
01:31:17,320 --> 01:31:18,850
I see, I see,

1694
01:31:18,850 --> 01:31:20,800
and I guess the last question would maybe be like,

1695
01:31:20,800 --> 01:31:23,530
is it it kind of sounds like that in this case,

1696
01:31:23,530 --> 01:31:27,010
the Go runtime or like the actually the shim in this case

1697
01:31:27,010 --> 01:31:30,610
is almost taking on some of the functionality normally,

1698
01:31:30,820 --> 01:31:32,860
like it's almost like it's like a mini,

1699
01:31:33,640 --> 01:31:36,880
it's almost kind of like, it's like a mini OS layer,

1700
01:31:36,910 --> 01:31:39,070
like in terms that it's just like another layer

1701
01:31:39,070 --> 01:31:43,030
that's performing some low-level system functionality, like reasonable.

1702
01:31:44,260 --> 01:31:45,430
Yeah you can,

1703
01:31:45,460 --> 01:31:47,440
maybe one way to think about xv6,

1704
01:31:47,440 --> 01:31:49,540
it also has a very very minimal shim,

1705
01:31:49,660 --> 01:31:51,460
you know maybe like when the boots correct,

1706
01:31:51,460 --> 01:31:53,590
the first thing it does actually allocates some stack,

1707
01:31:53,590 --> 01:31:56,350
so that you can actually call the C main function.

1708
01:31:57,240 --> 01:31:59,640
And you can think about that little fragment of code

1709
01:31:59,640 --> 01:32:00,960
which is only a couple statements,

1710
01:32:00,960 --> 01:32:02,670
the shim layer for xv6.

1711
01:32:03,710 --> 01:32:06,410
And once you know you're through a couple of instructions,

1712
01:32:06,410 --> 01:32:08,120
you're actually C code and everything is fine.

1713
01:32:08,790 --> 01:32:12,690
And you know the shim layer to Go runtime is slightly bigger,

1714
01:32:12,690 --> 01:32:14,790
because there's a bunch of more features that need to be set up,

1715
01:32:14,790 --> 01:32:16,950
before the Go runtime actually happily execute.

1716
01:32:18,700 --> 01:32:22,150
Okay, yeah that's helpful, that makes sense, cool.

1717
01:32:23,150 --> 01:32:23,540
Thank you.

1718
01:32:23,960 --> 01:32:24,590
You're welcome.

1719
01:32:25,460 --> 01:32:26,180
Happy Thanksgiving.

1720
01:32:26,420 --> 01:32:27,050
Yeah, you too.

1721
01:32:28,240 --> 01:32:32,650
Oh, I had a question about the ping-pong program that I forgot to ask,

1722
01:32:32,680 --> 01:32:36,790
so I remember we also did a ping-pong program in one of the labs.

1723
01:32:38,070 --> 01:32:43,950
It was not a hundred, those a thousand lines of code, why is.

1724
01:32:44,420 --> 01:32:48,050
Because like one, I think you're referring to a lab one,

1725
01:32:48,050 --> 01:32:50,210
you do the ping-pong with a byte across the pipe.

1726
01:32:50,860 --> 01:32:51,400
Yeah.

1727
01:32:51,610 --> 01:32:54,580
Okay, so that's the user side of the benchmark,

1728
01:32:54,610 --> 01:32:57,550
the kernel side correctly is the other side of it,

1729
01:32:57,550 --> 01:33:00,130
and basically you know that what we did is

1730
01:33:00,130 --> 01:33:02,170
implement the kernel passed in identical manner.

1731
01:33:04,530 --> 01:33:05,130
Okay.

1732
01:33:05,190 --> 01:33:08,550
So like you know you executing the start the system call,

1733
01:33:08,550 --> 01:33:10,890
using variables in the stack frame,

1734
01:33:10,890 --> 01:33:13,200
you know calling into looking up the pipe,

1735
01:33:13,200 --> 01:33:17,800
you know then running maybe the scheduler to wake up you know the receiver

1736
01:33:18,010 --> 01:33:20,020
and that whole code path,

1737
01:33:20,020 --> 01:33:23,620
you know on the kernel side, we tried to implement it,

1738
01:33:23,620 --> 01:33:25,240
identically in C and in Go.

1739
01:33:25,700 --> 01:33:26,690
Okay I see.

1740
01:33:26,720 --> 01:33:28,910
But the benchmark is basically the same as your benchmark,

1741
01:33:28,910 --> 01:33:31,010
that you implemented actually in lab one,

1742
01:33:31,100 --> 01:33:32,330
the user level side of it.

1743
01:33:33,250 --> 01:33:35,020
Right, right, okay that makes sense,

1744
01:33:35,050 --> 01:33:37,430
so, so does that mean like,

1745
01:33:38,140 --> 01:33:40,390
I mean I think that if you do that in xv6

1746
01:33:40,390 --> 01:33:44,080
would be significantly less than a thousand lines of code,

1747
01:33:44,080 --> 01:33:47,560
if you like take all the kernel code.

1748
01:33:47,560 --> 01:33:50,470
There's thousand thousand lines for assembly instructions correct,

1749
01:33:50,620 --> 01:33:54,370
so you know I I don't know I will have to look at,

1750
01:33:54,370 --> 01:33:57,340
but you know you're gonna use the trapframe code,

1751
01:33:57,340 --> 01:33:58,750
you know syscall dispatch,

1752
01:33:59,140 --> 01:34:04,240
going to the the fd layer, correct, the file descriptors,

1753
01:34:04,510 --> 01:34:07,600
then a little bit of pipe code,

1754
01:34:07,720 --> 01:34:10,810
then copyin and copyout,

1755
01:34:11,020 --> 01:34:12,640
then the scheduler

1756
01:34:12,940 --> 01:34:16,780
and then basically all [] and then bailing out again or returning.

1757
01:34:17,750 --> 01:34:21,390
Yeah, okay, that make sence.

1758
01:34:21,690 --> 01:34:26,610
I don't know how to talk my head on my client what code is,

1759
01:34:26,610 --> 01:34:26,640
but you know.

