1
00:00:00,030 --> 00:00:00,930
Anyone hear me?

2
00:00:02,860 --> 00:00:03,430
I can hear you.

3
00:00:03,850 --> 00:00:05,170
Thank you, alright.

4
00:00:06,500 --> 00:00:10,520
I'd like to spend today's lecture talking about threads

5
00:00:10,520 --> 00:00:12,680
and how xv6 does thread switching,

6
00:00:12,980 --> 00:00:17,870
this is a sort of one of the under the hood lectures about [xv6],

7
00:00:17,870 --> 00:00:21,560
we've had lectures before about how the system calls interrupts,

8
00:00:21,560 --> 00:00:23,120
page tables and locks work

9
00:00:23,510 --> 00:00:25,880
and today we're going to talk about

10
00:00:26,660 --> 00:00:29,480
how does the xv6 switches among different processes.

11
00:00:32,040 --> 00:00:34,230
The reason for this the highest level reason for this is

12
00:00:34,230 --> 00:00:36,180
that people like their computer to be able to

13
00:00:36,180 --> 00:00:39,090
do more than one task at the same time,

14
00:00:39,540 --> 00:00:42,330
so the reason might be that you're supporting time sharing,

15
00:00:42,330 --> 00:00:45,540
like Athena allows many users to login at the same time

16
00:00:45,540 --> 00:00:47,310
and you know they call run processes

17
00:00:47,610 --> 00:00:49,530
or even a single user machine

18
00:00:49,560 --> 00:00:50,670
or even your iPhone,

19
00:00:51,040 --> 00:00:53,440
you may run many different processes

20
00:00:53,440 --> 00:00:56,680
and expect the computer to do all the things you ask of it,

21
00:00:56,680 --> 00:00:57,490
not just one thing.

22
00:01:00,260 --> 00:01:03,860
Another reason that people like to support multiple tasks is

23
00:01:03,860 --> 00:01:06,350
because it can ease program structure,

24
00:01:06,470 --> 00:01:08,690
threads in particular today's topic,

25
00:01:08,810 --> 00:01:11,930
are sometimes used as a way to help people,

26
00:01:11,930 --> 00:01:16,970
to help programmers put together a program in a sort of simple elegant way,

27
00:01:17,000 --> 00:01:18,140
to reduce complexity,

28
00:01:18,290 --> 00:01:19,910
and you actually saw an example of this

29
00:01:19,910 --> 00:01:22,010
in the first lab with the prime number sieve,

30
00:01:22,010 --> 00:01:26,060
which didn't use threads exactly but use multiple processes,

31
00:01:26,060 --> 00:01:30,590
in order to help structure this your prime number sieve software

32
00:01:30,590 --> 00:01:34,850
and arguably it's sort of a more convenient or elegant or simpler way

33
00:01:34,850 --> 00:01:36,850
to to write that software.

34
00:01:37,620 --> 00:01:40,920
And the final reason why people use threads is,

35
00:01:40,920 --> 00:01:44,460
to get parallel speedup from multi core machines,

36
00:01:44,700 --> 00:01:48,120
so it's common to break up your program

37
00:01:48,120 --> 00:01:51,030
in a way that using threads

38
00:01:51,030 --> 00:01:55,110
to allow different parts of the same program to run on different cores,

39
00:01:55,110 --> 00:01:57,600
and if you can, maybe if you're lucky,

40
00:01:57,600 --> 00:02:01,710
if you can split your program up to run on four threads on four cores,

41
00:02:01,740 --> 00:02:05,370
you might be able to get a factor of four speedup how fast it runs.

42
00:02:06,780 --> 00:02:11,850
And indeed, you can view the xv6 kernel as a multi-core parallel program.

43
00:02:12,700 --> 00:02:15,070
So what threads are,

44
00:02:15,070 --> 00:02:18,760
is an abstraction to simplify programming,

45
00:02:18,940 --> 00:02:20,650
when you have many tasks,

46
00:02:20,650 --> 00:02:22,600
when you want to juggle many tasks,

47
00:02:22,630 --> 00:02:24,430
so what a thread is is,

48
00:02:24,670 --> 00:02:28,960
you can think of a thread as just being a single serial execution,

49
00:02:28,960 --> 00:02:31,390
if you just write a program that does one thing after another,

50
00:02:31,390 --> 00:02:32,560
and you run that program,

51
00:02:32,830 --> 00:02:37,570
that you know you can view the program as a sort of single thread of control.

52
00:02:38,290 --> 00:02:41,830
So, this is a loose definition.

53
00:02:44,160 --> 00:02:47,790
Because there's many different sort of flavors of what people mean by threads,

54
00:02:47,790 --> 00:02:52,920
but we'll say it's one serial execution,

55
00:02:53,250 --> 00:02:58,990
so it's what you get if you fire up one CPU

56
00:02:58,990 --> 00:03:02,560
and have it you know just execute one instruction after another in the ordinary way.

57
00:03:05,580 --> 00:03:07,770
We often talk about thread having state,

58
00:03:07,800 --> 00:03:08,820
because it's going to turn out,

59
00:03:08,820 --> 00:03:10,830
we're going to want to save away thread state

60
00:03:10,830 --> 00:03:11,760
and restore it later,

61
00:03:12,780 --> 00:03:15,630
and so the right way to think about thread state,

62
00:03:15,630 --> 00:03:16,620
for the most part,

63
00:03:16,740 --> 00:03:21,120
the most important part perhaps of the thread state is its program counter,

64
00:03:21,420 --> 00:03:22,830
because it's an execution,

65
00:03:22,830 --> 00:03:25,530
we care a lot about where is it in its execution

66
00:03:25,710 --> 00:03:27,960
and what address is it executing instructions,

67
00:03:29,010 --> 00:03:32,910
but also we care about the rest of the microprocessor state,

68
00:03:32,910 --> 00:03:34,860
that's required to support this execution

69
00:03:35,100 --> 00:03:36,300
and so that means it's.

70
00:03:38,520 --> 00:03:42,960
The state of a thread includes the registers that the compiler uses to hold variables

71
00:03:43,200 --> 00:03:47,070
and also because the just the way the compiler generates code,

72
00:03:47,130 --> 00:03:49,200
thread state includes a stack,

73
00:03:49,970 --> 00:03:53,150
so typically each thread has its own stack

74
00:03:53,150 --> 00:03:54,770
dedicated to executing that thread,

75
00:03:54,770 --> 00:03:58,130
and the stack records, the record of function calls,

76
00:03:59,960 --> 00:04:04,490
that the reflect the current point in the execution of that thread.

77
00:04:05,630 --> 00:04:10,130
And so what a threading system xv6 includes a threading system inside it,

78
00:04:10,160 --> 00:04:11,720
what a threading system does is

79
00:04:11,780 --> 00:04:18,200
manage this interleave, automate the interleaving of multiple threads,

80
00:04:18,410 --> 00:04:22,820
you would like to be able to fire up two or 400 or a thousand threads

81
00:04:22,820 --> 00:04:28,380
and the threading system figure out how to juggle all those threads

82
00:04:28,380 --> 00:04:30,780
and cause them all to make [progress] and all to execute.

83
00:04:32,030 --> 00:04:36,920
There's really two main strategy,

84
00:04:36,920 --> 00:04:38,390
so we want to [interleave],

85
00:04:39,210 --> 00:04:43,430
this is gonna be big topic here,

86
00:04:43,430 --> 00:04:46,790
how to interleave threads, many threads,

87
00:04:47,240 --> 00:04:50,090
one way to interleave the execution of many threads is

88
00:04:50,090 --> 00:04:51,920
to have multiple CPUs,

89
00:04:55,070 --> 00:04:57,890
maybe as on a multi-core processor

90
00:04:58,490 --> 00:05:00,770
and then each CPU can run its own thread,

91
00:05:00,770 --> 00:05:01,970
so if you have four CPUs,

92
00:05:01,970 --> 00:05:05,540
there's an obvious way to run four threads,

93
00:05:05,540 --> 00:05:07,070
to run one thread per CPU,

94
00:05:07,070 --> 00:05:11,030
and then each thread automatically gets its own program counter registers,

95
00:05:11,030 --> 00:05:15,320
that is the program counter register is associated with the CPU is running on,

96
00:05:15,680 --> 00:05:18,440
but if you have four CPUs and you have a thousand threads,

97
00:05:18,590 --> 00:05:23,830
then you know how using one core of thread,

98
00:05:23,830 --> 00:05:26,320
is not going to be enough of an answer.

99
00:05:26,530 --> 00:05:29,740
And so the other main strategy that we'll see,

100
00:05:30,620 --> 00:05:34,280
indeed the topic of most of this lecture is

101
00:05:34,310 --> 00:05:41,360
how each CPU is going to switch among different threads,

102
00:05:41,390 --> 00:05:43,580
so if I have one CPU and a thousand threads,

103
00:05:43,670 --> 00:05:47,810
we're going to see how xv6 builds switching system,

104
00:05:48,050 --> 00:05:50,300
that allows xv6 to run one thread for a while

105
00:05:50,300 --> 00:05:52,340
and then switch and set aside

106
00:05:52,340 --> 00:05:53,900
and save the state of that one thread

107
00:05:53,900 --> 00:05:56,540
and switch to executing a second thread for a while

108
00:05:56,540 --> 00:05:57,950
and then a third thread and so forth,

109
00:05:57,950 --> 00:06:00,080
until it's executed a little bit of each thread,

110
00:06:00,080 --> 00:06:04,340
and then go back and execute more of the of the first thread and so on.

111
00:06:06,170 --> 00:06:08,690
And indeed, xv6 like most operating system,

112
00:06:08,690 --> 00:06:13,640
[] the xv6 will run threads on all the cores that are available

113
00:06:13,640 --> 00:06:16,820
and each core will switch among threads,

114
00:06:17,030 --> 00:06:19,760
because there's typical typically, although not always,

115
00:06:19,760 --> 00:06:23,030
there's typically many more threads than there are CPUs.

116
00:06:24,780 --> 00:06:31,290
One of the many ways in which different threading systems

117
00:06:31,320 --> 00:06:33,930
or instances of threading systems differ is

118
00:06:33,930 --> 00:06:35,580
in whether or not they share memory.

119
00:06:36,960 --> 00:06:44,380
So this is important point.

120
00:06:45,040 --> 00:06:48,640
One possibility is that you could have a single address space,

121
00:06:48,670 --> 00:06:50,920
with many threads executing in that address space

122
00:06:50,920 --> 00:06:52,810
and then they see each other's changes,

123
00:06:52,990 --> 00:06:56,950
if one of the threads sharing some memory modifies variable,

124
00:06:56,950 --> 00:06:59,770
then the other thread sharing that memory will see the modification.

125
00:07:01,430 --> 00:07:04,520
And so it's in the context of threads running and sharing memory

126
00:07:04,520 --> 00:07:06,080
that we need things like the locks

127
00:07:06,080 --> 00:07:08,390
that you saw in the last lecture.

128
00:07:10,330 --> 00:07:13,510
Xv6 kernel is shared memory,

129
00:07:13,750 --> 00:07:16,840
so xv6, there's,

130
00:07:17,170 --> 00:07:19,810
xv6 has, supports the [notion] of kernel threads,

131
00:07:19,810 --> 00:07:22,630
there's one kernel thread per process

132
00:07:22,630 --> 00:07:24,670
that executes system calls for that process,

133
00:07:24,820 --> 00:07:27,520
all those kernel threads share kernel memory,

134
00:07:28,240 --> 00:07:30,730
so xv6 kernel threads do share memory.

135
00:07:34,400 --> 00:07:37,500
And on the other hand,

136
00:07:37,650 --> 00:07:39,420
xv6 has another kind of threads,

137
00:07:39,420 --> 00:07:43,020
each user process essentially has a single thread of control,

138
00:07:43,020 --> 00:07:45,870
that executes the user instructions for that process,

139
00:07:46,080 --> 00:07:51,270
and indeed a lot of the xv6 kernel threading machinery is

140
00:07:51,270 --> 00:07:56,940
automatically in support of being able to support and switch them on many user processes,

141
00:07:56,970 --> 00:07:59,010
each user process has a memory

142
00:07:59,010 --> 00:08:01,080
and a single thread that runs in that memory,

143
00:08:01,290 --> 00:08:05,750
so xv6 user processes,

144
00:08:11,160 --> 00:08:13,110
each process has one thread

145
00:08:13,110 --> 00:08:20,520
and so there's no sharing of memory among threads within a single xv6 user process,

146
00:08:20,520 --> 00:08:21,840
because you get multiple processes,

147
00:08:21,840 --> 00:08:25,770
but each of those processes is an address space with a single thread,

148
00:08:27,000 --> 00:08:29,310
the processes in xv6 don't share memory.

149
00:08:29,790 --> 00:08:32,400
In other more sophisticated operating systems,

150
00:08:32,400 --> 00:08:33,510
for example Linux,

151
00:08:35,110 --> 00:08:42,310
Linux user level does allow multiple threads in a process

152
00:08:42,310 --> 00:08:46,750
and the processes and those threads share the memory of that single process

153
00:08:47,380 --> 00:08:49,780
and that's super cool if you want to write user level programs

154
00:08:49,780 --> 00:08:54,940
that user level parallel programs that get speed up from multiple cores

155
00:08:55,060 --> 00:08:56,650
better requires sort of another,

156
00:08:56,770 --> 00:08:59,290
it uses a lot of the same basic techniques,

157
00:08:59,290 --> 00:09:00,580
we're going to talk about today,

158
00:09:00,580 --> 00:09:04,390
but there's a certain amount more sophistication in Linux to get it,

159
00:09:05,000 --> 00:09:08,600
keep track of multiple threads per process instead of just one.

160
00:09:12,250 --> 00:09:15,970
Okay, at a sort of high level, I just want to mention,

161
00:09:15,970 --> 00:09:17,890
that there's other ways

162
00:09:17,890 --> 00:09:22,150
to support the interleaving of multiple tasks on a single computer

163
00:09:22,510 --> 00:09:24,940
and we're not going to talk about them,

164
00:09:25,030 --> 00:09:29,020
if you're curious you can look up things like event-driven programming

165
00:09:29,020 --> 00:09:30,910
or state machines

166
00:09:31,060 --> 00:09:34,030
and these are non-thread techniques

167
00:09:34,060 --> 00:09:37,360
to share one computer among many different tasks,

168
00:09:37,480 --> 00:09:42,730
it turns out you know sort of on the spectrum of different schemes

169
00:09:42,730 --> 00:09:44,770
for supporting multiple tasks on a computer,

170
00:09:44,860 --> 00:09:46,570
threads are not very efficient,

171
00:09:46,600 --> 00:09:48,130
there's a more efficient schemes,

172
00:09:48,700 --> 00:09:51,670
but threads are usually the most convenient way,

173
00:09:51,700 --> 00:09:53,500
most programmer friendly way,

174
00:09:53,680 --> 00:09:57,220
to support lots of different tasks.

175
00:09:59,770 --> 00:10:04,610
Okay, there's a couple of challenges,

176
00:10:04,610 --> 00:10:05,840
that we're gonna have to bite off,

177
00:10:05,870 --> 00:10:08,420
if we want to implement a threading system.

178
00:10:12,590 --> 00:10:15,800
The, so this is just high level challenges.

179
00:10:22,880 --> 00:10:24,350
One is as I mentioned before,

180
00:10:24,350 --> 00:10:28,710
how to actually implement the switching for interleave,

181
00:10:28,800 --> 00:10:37,030
the switching that allows us to interleave the execution of multiple threads,

182
00:10:37,570 --> 00:10:43,680
and this sort of broad name for this process of switching deciding,

183
00:10:43,680 --> 00:10:47,190
or going to leave off one thread and start executing another thread,

184
00:10:47,340 --> 00:10:48,600
it's often called scheduling.

185
00:10:51,520 --> 00:10:55,810
And we'll see that xv6 [] as an actual piece of code,

186
00:10:55,810 --> 00:10:56,650
that's the scheduler,

187
00:10:56,650 --> 00:10:58,750
indeed has multiple schedulers one per core,

188
00:10:59,530 --> 00:11:02,830
but the general idea of how do you drive,

189
00:11:02,830 --> 00:11:04,660
the decision to switch from one to another,

190
00:11:04,660 --> 00:11:06,310
how to pick the next thread to run,

191
00:11:07,000 --> 00:11:07,780
its called scheduling.

192
00:11:10,120 --> 00:11:12,100
Another question is,

193
00:11:12,100 --> 00:11:14,800
if you want to actually implement the switch from one thread to another,

194
00:11:14,800 --> 00:11:16,750
you need to save and restore,

195
00:11:16,930 --> 00:11:19,840
so we need to decide what needs to be saved

196
00:11:20,170 --> 00:11:21,970
and where to save it,

197
00:11:22,900 --> 00:11:26,260
when, it needs to be saved when we leave off executing one thread

198
00:11:26,260 --> 00:11:30,820
and restored when we want to resume executing that thread at some later time,

199
00:11:31,180 --> 00:11:34,870
and final question is what to do about compute bound threads.

200
00:11:37,960 --> 00:11:41,560
The, many of the options,

201
00:11:41,740 --> 00:11:44,500
many of the most straightforward options for thread switching

202
00:11:44,770 --> 00:11:46,930
involve the threads are voluntarily saying

203
00:11:46,930 --> 00:11:48,370
well I'm going to save away my state

204
00:11:48,370 --> 00:11:51,400
and sort of run another, let another thread be run,

205
00:11:51,460 --> 00:11:56,050
but what do we have a user program that's doing some long running calculation,

206
00:11:56,050 --> 00:11:57,280
that might take hours,

207
00:11:57,430 --> 00:12:00,190
it's not going to be particularly thinking about,

208
00:12:00,190 --> 00:12:02,560
oh now be time to good time to let something else run,

209
00:12:02,770 --> 00:12:06,340
so it's most convenient to have some way of

210
00:12:06,340 --> 00:12:11,260
sort of automatically revoking control from some long running compute bound process,

211
00:12:11,440 --> 00:12:14,260
setting it aside and maybe running it later.

212
00:12:16,780 --> 00:12:18,850
Alright, so I'm going to talk about these,

213
00:12:18,850 --> 00:12:21,070
I'm actually going to talk about

214
00:12:21,070 --> 00:12:24,850
the machinery for dealing with compute bound threads first.

215
00:12:27,220 --> 00:12:31,860
And, scheme for that is something you've come up before,

216
00:12:32,520 --> 00:12:34,110
that's timer interrupts.

217
00:12:38,340 --> 00:12:41,150
And the idea here is that,

218
00:12:41,180 --> 00:12:46,040
there's a, a piece of hardware on each CPU on each core,

219
00:12:46,280 --> 00:12:49,640
that generates periodic interrupts,

220
00:12:49,850 --> 00:12:52,160
and the xv6 or any operating system

221
00:12:52,160 --> 00:12:54,920
really arranges to have those interrupts delivered to the kernel,

222
00:12:54,920 --> 00:12:58,250
so even if we're running at user level at some loop

223
00:12:58,250 --> 00:13:01,640
that's you know computing the first billion digits of pi,

224
00:13:01,730 --> 00:13:05,960
nevertheless the timer interrupts go off at some point

225
00:13:05,960 --> 00:13:07,430
maybe every ten milliseconds

226
00:13:07,640 --> 00:13:10,820
and transfer control from that user level code

227
00:13:10,910 --> 00:13:12,770
into the interrupt handler in the kernel,

228
00:13:12,860 --> 00:13:18,320
and so that's the first step in the kernel being able to gain control

229
00:13:18,320 --> 00:13:22,370
to switch among different user level processes, user level threads,

230
00:13:22,580 --> 00:13:25,820
even if those user level threads aren't cooperative.

231
00:13:26,900 --> 00:13:32,720
And the basic scheme is that in the interrupt handler,

232
00:13:32,720 --> 00:13:36,920
so we're gonna have you know kernel handler for these interrupts.

233
00:13:38,170 --> 00:13:39,640
And we'll see,

234
00:13:40,500 --> 00:13:44,370
that the kernel handler yields,

235
00:13:44,430 --> 00:13:47,010
this is the sort of name for this it yields,

236
00:13:49,290 --> 00:13:53,730
the kernel handler sort of voluntarily yields the CPU back to the scheduler

237
00:13:53,730 --> 00:13:56,400
it tell the scheduler look, you can let something else run now.

238
00:13:58,240 --> 00:14:03,060
And this yields is really a form of thread switch,

239
00:14:03,060 --> 00:14:06,330
that saves away the state of the current thread,

240
00:14:06,480 --> 00:14:07,920
so it can be restored later.

241
00:14:10,320 --> 00:14:12,390
And we'll see the full story here,

242
00:14:12,390 --> 00:14:13,920
actually you've seen a lot of the full story here,

243
00:14:13,920 --> 00:14:15,360
because it involves an interrupt,

244
00:14:16,050 --> 00:14:18,330
what you already know about the full story some complex,

245
00:14:18,330 --> 00:14:20,640
but the basic idea is that a timer interrupt

246
00:14:21,000 --> 00:14:22,500
gives control to the kernel

247
00:14:22,500 --> 00:14:25,560
and the kernel voluntarily yields the CPU,

248
00:14:26,820 --> 00:14:30,900
this is called as a piece of terminology preemptive scheduling.

249
00:14:38,980 --> 00:14:40,990
And what that means is that,

250
00:14:41,140 --> 00:14:42,880
the preemptive means is that,

251
00:14:43,780 --> 00:14:47,740
even if the code that's running doesn't, you know doesn't want to,

252
00:14:48,070 --> 00:14:51,010
you know doesn't explicitly yield the CPU,

253
00:14:51,160 --> 00:14:53,500
the timer interrupt is going to take control away

254
00:14:53,710 --> 00:14:55,570
and we're going to yield for it

255
00:14:55,570 --> 00:15:02,150
and the opposite of preemptive scheduling might be called maybe voluntary scheduling.

256
00:15:05,040 --> 00:15:06,330
And the interesting thing is that

257
00:15:06,330 --> 00:15:11,370
the the implementation in xv6 and other operating systems of preemptive scheduling is

258
00:15:11,370 --> 00:15:14,100
this timer interrupt forcibly takes away the CPU

259
00:15:14,400 --> 00:15:18,840
and then the kernel basically does a voluntary yield

260
00:15:18,840 --> 00:15:21,300
thread thread switch on [have] of that process.

261
00:15:24,100 --> 00:15:30,850
Now, another just piece of terminology that comes up here is

262
00:15:30,850 --> 00:15:33,610
that while the threads running,

263
00:15:33,640 --> 00:15:36,630
there's need to distinguish,

264
00:15:36,990 --> 00:15:42,510
systems that distinguish between threads that are currently actually running on some CPU

265
00:15:42,630 --> 00:15:48,330
versus threads that would like to run, but aren't currently running on any CPU,

266
00:15:48,330 --> 00:15:51,150
but you know could run if a CPU became free

267
00:15:51,390 --> 00:15:54,060
versus threads that actually don't want to run,

268
00:15:54,060 --> 00:15:55,590
because they're waiting for IO

269
00:15:55,590 --> 00:15:58,020
or waiting for some event.

270
00:15:58,600 --> 00:16:01,780
And [] this distinction is often called state,

271
00:16:02,980 --> 00:16:05,110
even though the full state of the thread is

272
00:16:05,110 --> 00:16:07,540
actually much more complicated than that.

273
00:16:09,830 --> 00:16:11,390
Since this is going to come up,

274
00:16:12,420 --> 00:16:15,300
I just want a list out a couple of states that will be seeing,

275
00:16:16,370 --> 00:16:19,310
and these are states that xv6 maintains,

276
00:16:19,310 --> 00:16:20,930
there's a state called RUNNING,

277
00:16:20,930 --> 00:16:25,160
which means it's actually executing on some core, some CPU right now,

278
00:16:25,370 --> 00:16:31,650
there's RUNNABLE, which means not currently executing anywhere,

279
00:16:31,650 --> 00:16:36,030
but just a saved state, but would like to run as soon as possible

280
00:16:36,360 --> 00:16:38,490
and then it turns out there's a state,

281
00:16:38,490 --> 00:16:41,580
which won't come out much today, but will come up next week,

282
00:16:41,580 --> 00:16:45,270
called SLEEPING, just means the threads waiting for some IO event

283
00:16:45,480 --> 00:16:48,120
and only wants to run after the IO event occurs,

284
00:16:48,300 --> 00:16:51,420
so today we're mostly concerned with running and runnable threads

285
00:16:51,420 --> 00:16:53,610
and what this preemptive switch does,

286
00:16:53,610 --> 00:16:55,650
what this timer interrupt does and the yield is

287
00:16:55,650 --> 00:16:57,660
basically convert a running thread

288
00:16:57,690 --> 00:16:59,640
whatever thread was interrupted by the timer

289
00:16:59,760 --> 00:17:01,800
into a runnable thread.

290
00:17:02,160 --> 00:17:02,760
That is a thread,

291
00:17:02,760 --> 00:17:06,180
that's by yielding or converting that thread into a thread,

292
00:17:06,180 --> 00:17:07,140
that's not running right now,

293
00:17:07,140 --> 00:17:08,910
but would actually like to clearly,

294
00:17:08,910 --> 00:17:12,060
because it was running at the timer timer interrupt.

295
00:17:14,220 --> 00:17:14,940
Okay so,

296
00:17:14,970 --> 00:17:21,180
running thread its program counter registers are actually in the CPU,

297
00:17:21,360 --> 00:17:24,420
you know in the hardware registers of the CPU that's executing it,

298
00:17:24,690 --> 00:17:27,870
a runnable thread though has no,

299
00:17:28,470 --> 00:17:30,840
it's now doesn't have a CPU associated with it

300
00:17:31,620 --> 00:17:33,270
and therefore we need to save,

301
00:17:33,270 --> 00:17:39,510
for every runnable state, we need to save whatever CPU state,

302
00:17:39,510 --> 00:17:45,540
whatever state the CPU was keeping when that thread was running,

303
00:17:45,720 --> 00:17:47,730
so we need to copy the CPU contents,

304
00:17:47,730 --> 00:17:49,890
you know which is not RAM, but just registers really,

305
00:17:50,040 --> 00:17:54,180
from the CPU into memory somewhere to save them,

306
00:17:54,240 --> 00:17:56,550
when we turn to thread from running to runnable.

307
00:17:56,640 --> 00:18:01,380
And again this is the basically the state we have to explicitly save yours,

308
00:18:01,410 --> 00:18:04,650
just the state, the executing state of the CPU,

309
00:18:04,650 --> 00:18:10,910
which is the program counter and registers at CPU,

310
00:18:10,910 --> 00:18:12,170
so these need to be saved,

311
00:18:13,000 --> 00:18:14,980
only convert a thread to runnable.

312
00:18:15,520 --> 00:18:19,480
When some scheduler finally decides to run a runnable thread,

313
00:18:19,690 --> 00:18:23,890
then as part of the many steps in getting that thread going again and resuming it,

314
00:18:24,100 --> 00:18:27,010
we're going to see that the program counter,

315
00:18:27,010 --> 00:18:32,650
the saved program counter registers are copied back into the CPUs,

316
00:18:32,980 --> 00:18:35,830
actual register on the CPU that the scheduler decides to run it on.

317
00:18:38,910 --> 00:18:42,960
Alright, any questions about these terminology?

318
00:18:49,880 --> 00:18:50,720
Alright, I'm gonna,

319
00:18:50,750 --> 00:18:56,210
now sort of talk about a sort of more xv6 [] view of things.

320
00:18:57,760 --> 00:19:00,010
I'm gonna draw two pictures,

321
00:19:00,010 --> 00:19:02,500
really of threads and xv6

322
00:19:02,500 --> 00:19:05,320
are kind of simplified picture and a more detailed picture.

323
00:19:05,470 --> 00:19:09,870
So as usually the user stuff up here

324
00:19:09,870 --> 00:19:11,460
and the kernel down here.

325
00:19:14,040 --> 00:19:17,940
We might be running multiple processes at user level,

326
00:19:17,940 --> 00:19:23,160
maybe you know the C compiler and ls and shell,

327
00:19:23,520 --> 00:19:27,420
they may or may not be all wanting to run at the same time,

328
00:19:28,260 --> 00:19:37,180
at user level, each of these processes has,

329
00:19:38,400 --> 00:19:39,570
you know it has memory,

330
00:19:39,690 --> 00:19:42,840
and of particular interest to us,

331
00:19:42,870 --> 00:19:45,510
each of these processes has a user stack,

332
00:19:46,840 --> 00:19:52,660
and while it's running it has registers in the RISC-V hardware,

333
00:19:52,660 --> 00:19:54,610
so PC plus registers.

334
00:19:55,590 --> 00:19:57,900
Alright, so while programs running,

335
00:19:57,900 --> 00:20:01,920
you know there's essentially a thread of control that's running at user level

336
00:20:02,760 --> 00:20:04,710
and the way I'm going to talk about it is,

337
00:20:04,710 --> 00:20:07,830
as if there's a user thread,

338
00:20:07,860 --> 00:20:13,290
that consists of the user stack, user memory, user program counter, user registers,

339
00:20:13,530 --> 00:20:15,360
if the program makes a system call,

340
00:20:15,360 --> 00:20:18,240
interrupted and goes into the kernel,

341
00:20:19,020 --> 00:20:23,010
then this stuff saved away in this program's trapframe

342
00:20:23,640 --> 00:20:30,570
and a kernel, the kernel thread for this program is activated

343
00:20:30,660 --> 00:20:35,760
and so now this is the trapframe hold saved user stuff

344
00:20:35,760 --> 00:20:38,700
after we saved a way the user [] program counter registers,

345
00:20:38,850 --> 00:20:42,510
then we switch the CPU to using the kernel stack.

346
00:20:45,520 --> 00:20:47,590
And you know we don't need to restore registers,

347
00:20:47,590 --> 00:20:53,900
because, through the the kernel thread for process isn't really running,

348
00:20:53,900 --> 00:20:58,960
it has no real save state, when the user thread is running

349
00:20:58,990 --> 00:21:02,950
instead it's sort of the kernel thread is kind of activated on its stack.

350
00:21:02,980 --> 00:21:08,800
So the first time in in the trampoline and a user trap code.

351
00:21:11,940 --> 00:21:13,710
And then the kernel runs for a while,

352
00:21:13,710 --> 00:21:16,320
maybe running a system call or interrupt handler,

353
00:21:16,320 --> 00:21:17,250
whatever it maybe,

354
00:21:18,150 --> 00:21:22,740
and sometimes it is a system call particular will just simply return,

355
00:21:22,740 --> 00:21:25,140
from this point back to the same process

356
00:21:25,140 --> 00:21:30,690
and return to user space or restore this program's program counter registers,

357
00:21:30,720 --> 00:21:34,320
but it could also be that instead of simply returning,

358
00:21:34,890 --> 00:21:36,180
for one reason or another,

359
00:21:36,180 --> 00:21:37,590
maybe because it was a timer interrupt,

360
00:21:37,710 --> 00:21:39,900
we're actually going to switch to another process

361
00:21:39,900 --> 00:21:42,270
and the very high-level view of that is that

362
00:21:43,110 --> 00:21:48,930
if the xv6 scheduler decides switch from this process to a different process,

363
00:21:49,590 --> 00:21:51,480
what the first thing that really happens is

364
00:21:51,480 --> 00:21:54,540
that we're going to switch kernel threads

365
00:21:54,570 --> 00:21:58,260
from this processe's kernel thread to the other process's kernel thread,

366
00:21:58,350 --> 00:22:01,530
and then the other processes kernel thread will return back to user space.

367
00:22:01,530 --> 00:22:04,950
So supposing that the C compiler [] needs to read the disk

368
00:22:05,070 --> 00:22:07,380
and so it's going to yield the CPU

369
00:22:07,860 --> 00:22:10,230
while sleeping to wait for the disk to complete,

370
00:22:10,410 --> 00:22:13,710
maybe ls wants to execute and is in runnable state,

371
00:22:14,610 --> 00:22:17,250
what the xv6 scheduler maybe they do is that,

372
00:22:17,400 --> 00:22:20,280
well, if ls is in runnable state,

373
00:22:20,280 --> 00:22:22,080
that means it left off somewhere

374
00:22:22,080 --> 00:22:25,740
and its state was saved away possibly by a timer interrupt

375
00:22:25,920 --> 00:22:28,890
and so ls will actually have a saved trapframe

376
00:22:28,890 --> 00:22:32,880
with user registers and its own kernel stack,

377
00:22:33,150 --> 00:22:40,910
and as it turns out save set of kernel registers associated with the kernel thread

378
00:22:40,940 --> 00:22:43,280
which is going to be called the context.

379
00:22:43,520 --> 00:22:48,980
So, if xv6 switches from the compiler kernel thread to ls's kernel thread,

380
00:22:49,500 --> 00:22:53,700
xv6 will save away the kernel registers,

381
00:22:53,790 --> 00:23:01,500
in a context with a C the compilers kernel thread switch to the ls thread,

382
00:23:02,730 --> 00:23:06,450
complex scheme which I'll describe a little bit later,

383
00:23:06,750 --> 00:23:11,560
we'll restore ls's kernel thread registers

384
00:23:11,560 --> 00:23:15,670
from the previously save context from one ls last left off,

385
00:23:16,000 --> 00:23:19,510
maybe ls will finish whatever system call was executing,

386
00:23:19,510 --> 00:23:23,800
you know on the ls's kernel thread stack,

387
00:23:24,160 --> 00:23:26,620
and then return back to ls system call,

388
00:23:26,620 --> 00:23:28,480
on the way to return to user space

389
00:23:28,480 --> 00:23:32,810
that will restore these previously saved user registers for ls

390
00:23:33,440 --> 00:23:35,660
and then resume executing ls.

391
00:23:35,690 --> 00:23:39,980
So, there's bunch of details here which we'll talk about,

392
00:23:39,980 --> 00:23:42,800
but maybe the main point here is that

393
00:23:42,830 --> 00:23:47,930
we never in xv6 see direct user to user context switches,

394
00:23:47,930 --> 00:23:50,330
when we're switching from one process to another,

395
00:23:50,330 --> 00:23:54,760
always the sort of strategy

396
00:23:54,760 --> 00:23:59,110
by which xv6 switches from executing one process to another process is

397
00:23:59,170 --> 00:24:01,570
you jump in the kernel, saves the process state,

398
00:24:01,570 --> 00:24:06,550
run this process's kernel thread switch to the kernel thread of another process,

399
00:24:06,550 --> 00:24:07,720
that suspended itself

400
00:24:07,720 --> 00:24:10,000
and then return and restore user register,

401
00:24:10,000 --> 00:24:12,100
so it's always this indirect strategy,

402
00:24:12,430 --> 00:24:13,990
actually even more indirect than this,

403
00:24:14,260 --> 00:24:15,790
to threads swhich,

404
00:24:15,910 --> 00:24:20,380
where the net effect is to switch from one user process to another user process.

405
00:24:23,310 --> 00:24:26,400
Question about this diagram or anything?

406
00:24:29,260 --> 00:24:32,890
Switched the scheduler, that happens in between those two, right.

407
00:24:33,220 --> 00:24:36,700
Yep, all right, let me talk about the scheduler,

408
00:24:36,700 --> 00:24:42,400
so the real picture is actually a significantly more complex than that.

409
00:24:43,070 --> 00:24:47,890
This is a may or more, gonna be more full diagram,

410
00:24:47,890 --> 00:24:52,170
let's say we have process one which is executing,

411
00:24:52,170 --> 00:24:57,150
and process two which is runnable, but not currently running,

412
00:24:58,020 --> 00:24:59,550
now the additional layer of details,

413
00:24:59,550 --> 00:25:02,730
we actually have multiple cores in xv6,

414
00:25:02,730 --> 00:25:04,050
let's say we have two cores,

415
00:25:04,050 --> 00:25:06,960
so that means that at the hardware level,

416
00:25:07,680 --> 00:25:12,360
we have CPU0 which is one of the cores

417
00:25:12,660 --> 00:25:14,850
and let's say CPU1.

418
00:25:20,030 --> 00:25:21,710
And the more full story about

419
00:25:21,710 --> 00:25:27,340
how we get from executing user space to,

420
00:25:27,370 --> 00:25:29,920
in one process executing in user space

421
00:25:29,920 --> 00:25:33,460
in another runnable but not yet running process,

422
00:25:33,880 --> 00:25:36,910
the first part is about the same as I talked about

423
00:25:37,060 --> 00:25:39,790
and may say a timer interrupt forces

424
00:25:40,390 --> 00:25:43,540
transfer control from the user process into the kernel,

425
00:25:43,600 --> 00:25:49,480
the trampoline code saves the user registers and trapframe for process one,

426
00:25:51,530 --> 00:25:54,900
and then executes usertrap

427
00:25:54,900 --> 00:25:57,630
which figures out what to do with this trap or interrupt,

428
00:25:57,630 --> 00:25:59,940
you know system call, let's say as a,

429
00:26:00,090 --> 00:26:04,440
so for a little while, we're executing ordinary kernel C code,

430
00:26:04,560 --> 00:26:07,830
on the kernel stack of process one.

431
00:26:10,240 --> 00:26:15,250
Let's say process one, the kernel code process once decides it wants to yield the CPU,

432
00:26:16,120 --> 00:26:18,580
it has a bunch of things, which we'll see the details of,

433
00:26:18,610 --> 00:26:22,270
that end up in a call to this routine switch,

434
00:26:22,730 --> 00:26:26,060
just sort of one of the central routines in this story,

435
00:26:26,150 --> 00:26:29,900
switch saves away this context,

436
00:26:29,900 --> 00:26:33,740
that registers for the kernel thread that's running in context one,

437
00:26:33,740 --> 00:26:35,450
so there's two sets of registers,

438
00:26:35,450 --> 00:26:40,450
the user registers, the trapframe, the kernel thread registers in the context.

439
00:26:41,510 --> 00:26:43,580
Switch doesn't actually,

440
00:26:43,580 --> 00:26:47,240
switch switches from one content from one thread to another,

441
00:26:47,240 --> 00:26:50,600
but in fact the way xv6 is designed,

442
00:26:50,750 --> 00:26:54,890
the only place that a user thread,

443
00:26:54,920 --> 00:26:57,920
sorry, the kernel thread running on a CPU can switch to is

444
00:26:57,920 --> 00:27:00,710
what's called the scheduler thread for that CPU.

445
00:27:02,910 --> 00:27:05,550
So we can't even switch directly to another process,

446
00:27:05,790 --> 00:27:07,680
can only switch to the scheduler thread,

447
00:27:07,680 --> 00:27:09,150
so there's a,

448
00:27:10,360 --> 00:27:16,510
the complete thread apparatus dedicated to the scheduler for CPU0,

449
00:27:16,540 --> 00:27:17,830
since we're running on CPUs,

450
00:27:17,830 --> 00:27:25,390
this switch is going to switch to the previously saved registers for the scheduler thread,

451
00:27:25,420 --> 00:27:27,340
so let's say schedule zero.

452
00:27:29,650 --> 00:27:33,070
And in the scheduler for CPU0,

453
00:27:33,100 --> 00:27:36,100
switch will, by restoring these registers,

454
00:27:36,100 --> 00:27:38,440
since registers include the stack pointer,

455
00:27:38,440 --> 00:27:40,450
the return from switch as we see,

456
00:27:40,630 --> 00:27:53,650
will now actually return up to the scheduler function on CPU,

457
00:27:53,650 --> 00:27:56,410
and scheduler function will do some cleanup

458
00:27:56,410 --> 00:27:58,630
to finish putting process one to sleep,

459
00:27:58,720 --> 00:28:02,020
then it'll look in the process table for another process,

460
00:28:02,020 --> 00:28:03,400
a runnable process,

461
00:28:03,640 --> 00:28:06,130
and if it finds one,

462
00:28:07,160 --> 00:28:10,550
and so we've sort of gone down here and up into the scheduler,

463
00:28:10,670 --> 00:28:12,620
if the scheduler finds another process to run

464
00:28:12,620 --> 00:28:16,430
or even finds process one is runnable and still wants to run,

465
00:28:16,430 --> 00:28:19,010
find process one nothing else nothing else wants to run,

466
00:28:19,850 --> 00:28:22,940
but in any case, the scheduler will call switch again,

467
00:28:22,940 --> 00:28:27,020
to switch contexts to say process two,

468
00:28:27,960 --> 00:28:32,490
in the process switch will save its own registers again in its own context,

469
00:28:33,020 --> 00:28:36,680
they'll be a previously saved context too,

470
00:28:36,680 --> 00:28:38,720
from whenever process two left off

471
00:28:38,900 --> 00:28:41,870
that those this set of registers will be restored,

472
00:28:42,440 --> 00:28:44,870
process two will have made a previous call,

473
00:28:44,870 --> 00:28:50,560
to switch, to switch to the scheduler thread just like process one did

474
00:28:50,560 --> 00:28:51,550
when it left off,

475
00:28:51,730 --> 00:28:53,200
that called a switch return to,

476
00:28:53,200 --> 00:28:56,150
whatever system call or interrupt,

477
00:28:56,150 --> 00:28:58,910
process two is end, when that's finished,

478
00:28:59,030 --> 00:29:02,510
there will be a previously saved trapframe for process two,

479
00:29:02,720 --> 00:29:04,220
that will contain user registers,

480
00:29:04,220 --> 00:29:08,360
also be restored or return back up into user space.

481
00:29:09,050 --> 00:29:15,920
And there's a complete, a separate scheduler thread for each CPU,

482
00:29:15,920 --> 00:29:18,950
so, they'll also be saved,

483
00:29:19,700 --> 00:29:22,940
context for the scheduler thread for CPU,

484
00:29:23,210 --> 00:29:28,960
and a scheduler loop running on scheduler one

485
00:29:28,960 --> 00:29:32,950
and whatever process, you know process three or something is running on CPU one

486
00:29:32,950 --> 00:29:34,750
when it decides to give up the CPU,

487
00:29:34,750 --> 00:29:41,410
it'll switch into scheduler thread for it for its CPU.

488
00:29:43,800 --> 00:29:44,850
Alright, there's a question,

489
00:29:44,850 --> 00:29:46,980
where the context stored,

490
00:29:47,310 --> 00:29:52,370
it turns out that for the operations I've been talking about,

491
00:29:52,550 --> 00:29:54,140
the saved,

492
00:29:54,200 --> 00:30:00,190
in fact always the for a thread switch,

493
00:30:01,030 --> 00:30:04,450
these contexts, these saved register sets for kernel threads,

494
00:30:04,450 --> 00:30:05,860
all in the process structure,

495
00:30:06,190 --> 00:30:11,890
so any given kernel thread can only have one set of saved kernel registers.

496
00:30:12,380 --> 00:30:15,680
Because each thread is only executing a single place

497
00:30:15,680 --> 00:30:20,690
and its context kind of reflects that place that it was executing on it left off

498
00:30:20,690 --> 00:30:23,420
a thread is a single thread of control,

499
00:30:23,420 --> 00:30:27,350
so a thread really only needs one context for registers,

500
00:30:27,380 --> 00:30:33,190
so it's in the process structures p arrow, p->context.

501
00:30:35,310 --> 00:30:38,670
And the scheduler, each scheduler thread has its own context

502
00:30:38,670 --> 00:30:39,900
which actually not in the,

503
00:30:40,230 --> 00:30:43,380
there's no process associated with this scheduler thread,

504
00:30:43,620 --> 00:30:50,710
this is actually scheduler's context is stored in the struct CPU for that core.

505
00:30:51,480 --> 00:30:54,120
There's an array of these CPU's struct one per core,

506
00:30:54,120 --> 00:30:55,860
each one has a context.

507
00:30:58,210 --> 00:31:02,200
Question, why can't we include the registers in the trapframe for the process,

508
00:31:02,230 --> 00:31:08,890
that is, you know actually the those registers could be stored in the trapframe

509
00:31:08,890 --> 00:31:14,050
which may, because there's only one save set of kernel thread registers,

510
00:31:14,260 --> 00:31:17,050
for process, we could save them in any data structure

511
00:31:17,230 --> 00:31:22,030
for which there's one element of instance of that data structure process,

512
00:31:22,270 --> 00:31:24,400
there's one struct process,

513
00:31:24,400 --> 00:31:26,380
there's one struct trapframe for process,

514
00:31:26,500 --> 00:31:28,450
we could store the registers in the trapframe.

515
00:31:31,730 --> 00:31:33,140
I mean just sort of for,

516
00:31:33,720 --> 00:31:35,730
maybe simplicity or clarity of code,

517
00:31:35,730 --> 00:31:40,040
the trapframe I think, entirely consists of data,

518
00:31:40,040 --> 00:31:42,500
that's needed when entering and leaving the kernel.

519
00:31:43,060 --> 00:31:47,920
And the struct context is consists of the stuff that needs to be saved and restored,

520
00:31:47,920 --> 00:31:52,540
when switching to and from between the kernel thread and the scheduler thread.

521
00:31:54,640 --> 00:31:55,360
Okay, question is,

522
00:31:55,360 --> 00:31:58,210
yield something that's called by the user of the kernel, called by the kernel,

523
00:31:59,060 --> 00:32:02,630
so the user threads, there's not really a direct way in xv6

524
00:32:02,630 --> 00:32:08,240
for user threads to talk about, yielding the CPU or switching,

525
00:32:08,480 --> 00:32:12,320
that it's done by the kernel kind of transparently,

526
00:32:13,210 --> 00:32:16,510
you know it points in time when the kernel feels that it needs to happen,

527
00:32:16,810 --> 00:32:17,920
if there are threads,

528
00:32:18,460 --> 00:32:22,960
there are sometimes when, you can sort of guess

529
00:32:22,960 --> 00:32:26,500
that probably a certain system call will result in yield,

530
00:32:26,500 --> 00:32:30,130
like if a process does a read on a pipe,

531
00:32:30,130 --> 00:32:33,190
where it knows that really nothing is waiting to be read in that pipe,

532
00:32:33,400 --> 00:32:35,290
then the read will block,

533
00:32:35,870 --> 00:32:37,370
you can predict the read block

534
00:32:37,370 --> 00:32:40,620
and that the kernel will run some other process,

535
00:32:40,620 --> 00:32:42,930
while we're waiting for data to appear in the pipe.

536
00:32:44,800 --> 00:32:47,800
So the times when yield is called in the kernel,

537
00:32:47,800 --> 00:32:48,940
there's really two main times,

538
00:32:48,940 --> 00:32:53,650
one is if a timer interrupt goes off, kernel always yields,

539
00:32:53,860 --> 00:32:56,760
you know just on the theory that,

540
00:32:57,640 --> 00:33:02,500
we should interleave the execution of of all the process

541
00:33:02,500 --> 00:33:06,500
that want to run on timer interrupt periods,

542
00:33:06,770 --> 00:33:09,020
so timer interrupt also always calls yield

543
00:33:09,410 --> 00:33:13,280
and whenever a process system calls waiting for IO,

544
00:33:13,370 --> 00:33:15,200
waiting for you know type the next keystroke,

545
00:33:15,200 --> 00:33:16,910
does a read of the console

546
00:33:16,910 --> 00:33:18,110
and you haven't typed key yet,

547
00:33:18,410 --> 00:33:24,070
then the the machinery to wait for IO calls yields,

548
00:33:24,070 --> 00:33:26,650
called for sleep, something we'll talk about next week.

549
00:33:29,200 --> 00:33:29,800
Alright.

550
00:33:31,540 --> 00:33:32,950
Okay, so.

551
00:33:32,950 --> 00:33:34,390
I have another question.

552
00:33:34,420 --> 00:33:35,020
Yes.

553
00:33:35,700 --> 00:33:37,530
Oh, if it is a sleep,

554
00:33:37,560 --> 00:33:40,020
is it gonna do the same thing roughly,

555
00:33:40,080 --> 00:33:42,660
so it's gonna be some system call,

556
00:33:42,690 --> 00:33:44,520
there's gonna save the trapframe

557
00:33:44,790 --> 00:33:47,850
and then basically the same [picture],

558
00:33:47,850 --> 00:33:51,570
but it's just then the thing

559
00:33:51,570 --> 00:33:55,800
that made the process go into the kernel without a timer interrupt,

560
00:33:55,800 --> 00:33:57,350
but, the process's own decision?

561
00:33:57,900 --> 00:34:03,510
Yeah, so the process make there's a read system call

562
00:34:03,510 --> 00:34:04,710
and that's why it's in the kernel,

563
00:34:05,330 --> 00:34:11,210
and the read requires the process to wait for the disk to do to finish reading

564
00:34:11,210 --> 00:34:13,220
or to wait for data to appear on a pipe,

565
00:34:13,310 --> 00:34:16,310
then actually the diagram is exactly the same as this,

566
00:34:18,510 --> 00:34:20,100
and the kernel with a system call,

567
00:34:20,100 --> 00:34:22,200
trapframe, all the saved user register,

568
00:34:22,200 --> 00:34:23,970
execute the system call, the system will [],

569
00:34:24,000 --> 00:34:26,550
need to wait for the disk to finish reading something,

570
00:34:27,480 --> 00:34:31,770
the system call code will call sleep which ends up coding switch,

571
00:34:32,180 --> 00:34:38,150
which saves away the kernel thread registers in the process context

572
00:34:38,450 --> 00:34:41,090
switches to this current CPU scheduler,

573
00:34:41,090 --> 00:34:42,680
to let some other thread run,

574
00:34:42,710 --> 00:34:45,980
while this thread is waiting for the disk read to finish.

575
00:34:46,590 --> 00:34:48,690
So everything we're going to talk about now,

576
00:34:48,720 --> 00:34:50,520
except for the timer interrupt,

577
00:34:50,970 --> 00:34:53,100
is pretty much the same

578
00:34:53,100 --> 00:34:55,680
if what's going on is we're in a system called,

579
00:34:55,680 --> 00:34:58,470
the system call needs to wait for some for IO

580
00:34:58,680 --> 00:34:59,700
and give up the CPU.

581
00:35:02,250 --> 00:35:04,230
For the purposes of today's discussion,

582
00:35:04,230 --> 00:35:06,090
the two situations are almost identical.

583
00:35:08,970 --> 00:35:12,090
Okay, so the question does each per CPU scheduler has its own stack.

584
00:35:12,090 --> 00:35:13,020
Yes,

585
00:35:13,290 --> 00:35:15,030
there's a stack,

586
00:35:18,600 --> 00:35:22,350
this scheduler stack for this separate stack,

587
00:35:23,730 --> 00:35:26,520
for scheduler for CPU one.

588
00:35:31,920 --> 00:35:35,010
Yeah, and indeed the stacks for this scheduler also setup,

589
00:35:36,300 --> 00:35:38,040
in fact, all this stuff

590
00:35:38,070 --> 00:35:41,910
you know the context and the stacks with a scheduler threads

591
00:35:41,910 --> 00:35:45,060
are set up in a different way than for user process's.

592
00:35:46,450 --> 00:35:48,130
They're set up at boot time,

593
00:35:48,280 --> 00:35:54,220
if you poke around and start.S or start.c, start.S probably,

594
00:35:54,220 --> 00:35:59,020
you'll see some of the setup for each core's scheduler thread,

595
00:35:59,020 --> 00:36:03,070
there's a place with stack very early in the assembly code during boot,

596
00:36:03,100 --> 00:36:06,220
where the stack is set up for each CPU

597
00:36:06,220 --> 00:36:07,300
and it's on that stack,

598
00:36:07,300 --> 00:36:09,460
that's a CPU boots on

599
00:36:09,640 --> 00:36:11,320
and then runs its scheduler thread.

600
00:36:15,590 --> 00:36:16,880
Okay, um.

601
00:36:18,930 --> 00:36:20,430
One piece of jargon,

602
00:36:20,430 --> 00:36:23,700
when people talk about context switch,

603
00:36:24,840 --> 00:36:32,240
they're talking about usually this act of

604
00:36:32,240 --> 00:36:34,070
switching from one thread to another

605
00:36:34,070 --> 00:36:37,040
by saving one set of register sets for the old thread

606
00:36:37,220 --> 00:36:40,730
and restoring previously saved registers for the threads were switching to,

607
00:36:41,600 --> 00:36:43,070
that's what's usually meant by context,

608
00:36:43,070 --> 00:36:47,700
which also sometimes it's applied to the complete done is

609
00:36:47,700 --> 00:36:50,280
that goes on when switching from one user process to another

610
00:36:50,280 --> 00:36:54,810
and occasionally you'll see context which apply to switching between user and kernel,

611
00:36:54,930 --> 00:36:57,060
but for us we mostly mean it,

612
00:36:58,260 --> 00:37:03,670
switching from one kernel thread typically to scheduler thread.

613
00:37:04,980 --> 00:37:08,130
Just some pieces of information,

614
00:37:10,100 --> 00:37:13,430
the [] [] to keep in mind,

615
00:37:13,610 --> 00:37:16,580
every core just does one thing at a time,

616
00:37:16,610 --> 00:37:21,320
each core you know is either is just running one thread at any given time,

617
00:37:21,320 --> 00:37:25,700
it's either running some processes user thread some process kernel thread

618
00:37:25,730 --> 00:37:27,890
or that core's scheduler thread,

619
00:37:28,190 --> 00:37:30,830
so at any given time the core is not doing multiple things,

620
00:37:30,830 --> 00:37:31,850
it's just doing one thing

621
00:37:31,850 --> 00:37:34,700
and it's this switching that sort of creates the illusion

622
00:37:34,700 --> 00:37:39,200
of multiple threads running at different times on that core,

623
00:37:39,620 --> 00:37:46,360
similarly, each thread is running on,

624
00:37:46,870 --> 00:37:50,320
is either running on exactly one core

625
00:37:50,740 --> 00:37:54,130
or its state has been state has been saved

626
00:37:54,130 --> 00:37:55,570
and we switched away from it.

627
00:37:56,280 --> 00:37:57,840
So, so a thread,

628
00:37:57,840 --> 00:38:00,270
just to be clear, thread never runs on more than one core,

629
00:38:00,270 --> 00:38:02,160
thread is either running on just one core

630
00:38:02,190 --> 00:38:05,310
or it's not running at all, as a saved state somewhere.

631
00:38:06,610 --> 00:38:10,690
Another interesting thing about the xv6 setup is that

632
00:38:10,780 --> 00:38:17,190
these contexts that hold saved kernel thread registers,

633
00:38:17,430 --> 00:38:20,280
they're always produced by a call to switch

634
00:38:20,670 --> 00:38:27,210
and so these contexts basically always refer to the state of the thread

635
00:38:27,240 --> 00:38:30,420
and it was executing inside a call to swtch.

636
00:38:33,240 --> 00:38:35,820
And you know the way we'll see that come up is that

637
00:38:35,910 --> 00:38:38,610
when we switch from one to another

638
00:38:38,610 --> 00:38:40,950
and restore the target thread's context,

639
00:38:41,100 --> 00:38:45,240
the first thing it will do is return from a previous call to swtch,

640
00:38:45,390 --> 00:38:47,700
these contexts are always saved state,

641
00:38:48,270 --> 00:38:50,160
in as it is in swtch.

642
00:38:52,490 --> 00:38:53,180
Okay.

643
00:38:56,060 --> 00:39:00,320
Any more questions about the diagram level situation?

644
00:39:05,240 --> 00:39:06,470
I have a question,

645
00:39:06,740 --> 00:39:08,900
you are using the term thread all the time,

646
00:39:08,900 --> 00:39:12,500
but it seems to me, like our implementation for xv6,

647
00:39:12,860 --> 00:39:15,710
a process is if there's only one thread,

648
00:39:15,860 --> 00:39:19,460
so it could it be possible that one process could have multiple threads

649
00:39:19,460 --> 00:39:21,050
or am I wrong here.

650
00:39:21,740 --> 00:39:24,630
In xv6, alright.

651
00:39:26,030 --> 00:39:29,870
There's definitely some confusing things about the way we use the words here,

652
00:39:29,900 --> 00:39:31,460
in xv6,

653
00:39:32,260 --> 00:39:34,150
a process,

654
00:39:38,470 --> 00:39:43,930
a process is either executing instructions user level

655
00:39:44,080 --> 00:39:49,000
or it's executing instructions in the kernel

656
00:39:49,780 --> 00:39:52,900
or it's not executed at all

657
00:39:52,900 --> 00:39:59,440
and its state has been saved away into this combination of context and trapframe.

658
00:40:02,300 --> 00:40:03,920
So that's the actual situation,

659
00:40:03,980 --> 00:40:05,570
which you want to call that,

660
00:40:09,060 --> 00:40:10,740
you call it what you like,

661
00:40:10,800 --> 00:40:13,950
I I don't know of a simple explanation for this structure,

662
00:40:14,160 --> 00:40:16,320
we've been calling it I've been calling it,

663
00:40:16,590 --> 00:40:20,640
I've been saying that each process has two threads,

664
00:40:21,100 --> 00:40:24,930
a user level thread and a kernel level thread

665
00:40:25,080 --> 00:40:26,190
and that's a process,

666
00:40:26,190 --> 00:40:27,090
there's this restriction

667
00:40:27,090 --> 00:40:28,860
that a process is only execute,

668
00:40:28,860 --> 00:40:31,770
either executing in the kernel in the user space

669
00:40:31,860 --> 00:40:35,070
or executing in the kernel, inter- interrupt system call,

670
00:40:35,430 --> 00:40:36,240
but never both.

671
00:40:38,680 --> 00:40:39,820
That makes sense.

672
00:40:39,850 --> 00:40:43,780
Yeah, I apologize for the complexity of this.

673
00:40:47,540 --> 00:40:48,110
Okay.

674
00:40:48,710 --> 00:40:50,690
Okay, so let me switch to code,

675
00:40:50,690 --> 00:40:52,220
looking at the xv6 code.

676
00:41:01,130 --> 00:41:04,250
Alright, so first of all,

677
00:41:10,090 --> 00:41:15,520
I just wanna, just show some of the stuff we've been talking about,

678
00:41:15,520 --> 00:41:20,300
I'm going to look at the process structure.

679
00:41:20,970 --> 00:41:22,440
And we can see in the process structure,

680
00:41:22,440 --> 00:41:23,880
a lot of the things we've been talking about,

681
00:41:24,390 --> 00:41:26,250
just for review,

682
00:41:26,250 --> 00:41:34,980
there's the trapframe, that saves the user level registers.

683
00:41:36,440 --> 00:41:43,810
There's a context here, that saves the kernel thread registers,

684
00:41:43,810 --> 00:41:46,000
which switch to the scheduler thread.

685
00:41:46,580 --> 00:41:51,320
There's a pointer to this process is kernel stack,

686
00:41:51,320 --> 00:41:53,810
which is where function calls are saved,

687
00:41:53,960 --> 00:41:55,490
while we're executing in the kernel.

688
00:41:56,560 --> 00:41:58,780
There's the state variable,

689
00:41:58,780 --> 00:42:03,490
which records whether this process is running or runnable

690
00:42:03,490 --> 00:42:05,800
or sleeping or not allocated at all.

691
00:42:06,550 --> 00:42:13,500
And then finally, there's a lock that protects various things as we'll see,

692
00:42:15,710 --> 00:42:22,670
for now, we can observe that at least protects changes to the state variable.

693
00:42:23,480 --> 00:42:26,360
So that for example two scheduler threads,

694
00:42:26,360 --> 00:42:29,720
don't try to grab a runnable process and run it at the same time.

695
00:42:30,300 --> 00:42:32,970
One of the many things this lock does is prevent that from happening.

696
00:42:35,280 --> 00:42:40,680
I'm gonna run a simple demo program for you, the spin program.

697
00:42:41,460 --> 00:42:44,760
I'm using it mostly just to drive the

698
00:42:44,970 --> 00:42:47,220
sort of create a predictable situation

699
00:42:47,220 --> 00:42:49,710
in which we switch from one thread to another,

700
00:42:49,890 --> 00:42:52,290
but this is this program,

701
00:42:52,290 --> 00:42:54,840
spin program creates two processes

702
00:42:54,840 --> 00:42:56,850
and the processes both compute forever,

703
00:42:57,320 --> 00:42:59,570
you know call fork here,

704
00:43:00,320 --> 00:43:02,330
I make a child

705
00:43:02,420 --> 00:43:07,310
and then forever, both children both children just sit in this loop

706
00:43:07,310 --> 00:43:08,990
and everyone's while print a character,

707
00:43:08,990 --> 00:43:10,760
so we can see they're making progress,

708
00:43:11,000 --> 00:43:13,340
but they don't print characters very often,

709
00:43:13,340 --> 00:43:17,570
and they never sort of intentionally give up the CPU.

710
00:43:17,600 --> 00:43:21,620
So what we have here is two, essentially two compute bound processes

711
00:43:21,740 --> 00:43:23,390
and in order for both of them to run,

712
00:43:23,390 --> 00:43:25,940
I'm gonna run them on a single CPU,

713
00:43:26,780 --> 00:43:29,660
xv6 that is only one core

714
00:43:29,720 --> 00:43:31,490
and so in order for both of them to execute,

715
00:43:31,610 --> 00:43:38,800
you know it's going to be necessary to to switching between the two processes.

716
00:43:40,440 --> 00:43:45,600
Let me fire up spin program under gdb,

717
00:43:51,020 --> 00:43:53,180
run the spin program and you can see it's printing,

718
00:43:53,210 --> 00:43:56,720
one of the two processes prints forward slash

719
00:43:56,720 --> 00:43:58,400
and the other prints backward slash

720
00:43:58,400 --> 00:44:00,500
and you can see that every once in a while,

721
00:44:00,740 --> 00:44:03,170
xv6 is switching between them

722
00:44:03,170 --> 00:44:05,270
and only has one core the way I've configured it,

723
00:44:05,480 --> 00:44:08,960
so we see a bunch of forward slashes printing

724
00:44:08,960 --> 00:44:11,000
and then apparently a timer interrupt,

725
00:44:11,000 --> 00:44:15,410
must go off switch the one CPU to the other process

726
00:44:15,410 --> 00:44:17,930
and then prints the other kind of slash for a while.

727
00:44:17,990 --> 00:44:20,660
So what I want to observe is the timer interrupt going off,

728
00:44:20,810 --> 00:44:24,630
so I'm gonna put a breakpoint in trap,

729
00:44:26,940 --> 00:44:30,360
and in particular at line 207 in trap,

730
00:44:35,060 --> 00:44:45,240
which is a code in trap in devintr,

731
00:44:45,240 --> 00:44:49,320
that recognizes that we're in interrupt

732
00:44:49,320 --> 00:44:52,050
and the interrupt was caused by a timer interrupt.

733
00:44:53,430 --> 00:44:56,040
So I put a breakpoint here,

734
00:44:56,040 --> 00:44:59,880
at trap.c 207

735
00:44:59,880 --> 00:45:04,080
and continue [boom] the trap I triggers right away,

736
00:45:04,080 --> 00:45:05,850
because timer interrupt are pretty frequent

737
00:45:06,150 --> 00:45:06,900
and we can tell from

738
00:45:06,900 --> 00:45:10,860
where that indeed when user trap and user trap has called devintr

739
00:45:11,400 --> 00:45:12,930
to handle this interrupt.

740
00:45:13,480 --> 00:45:18,400
I wanna take finish to get out of devintr back into user trap.

741
00:45:20,420 --> 00:45:21,230
Because, in fact we don't,

742
00:45:22,640 --> 00:45:25,250
the code devintr into timer interrupt you know says almost nothing.

743
00:45:27,340 --> 00:45:34,810
However, once we're back at in usertrap,

744
00:45:36,200 --> 00:45:39,440
we can see that this line here,

745
00:45:39,440 --> 00:45:41,630
that we just returned from devintr.

746
00:45:47,860 --> 00:45:52,930
And the interesting thing about this is that,

747
00:45:54,340 --> 00:45:56,020
what we're about to do,

748
00:45:56,530 --> 00:45:59,020
I'm looking forward we're currently at this line here

749
00:45:59,140 --> 00:46:03,010
and we're looking forward to this call the yield,

750
00:46:03,730 --> 00:46:07,300
when devintr return two, you can see from this,

751
00:46:07,450 --> 00:46:08,980
the return is two,

752
00:46:08,980 --> 00:46:11,140
two is basically the device number

753
00:46:11,320 --> 00:46:13,450
and we're going to see that by and by,

754
00:46:13,480 --> 00:46:15,880
because which devices two,

755
00:46:16,480 --> 00:46:18,820
usertrap going to call yield,

756
00:46:18,820 --> 00:46:22,320
which go to CPU and allows [pushing] the process,

757
00:46:22,770 --> 00:46:23,700
you'll see that in a moment,

758
00:46:23,910 --> 00:46:27,960
meantime let's look at what was currently executing when the interrupt happened.

759
00:46:28,140 --> 00:46:29,820
I'm going to print p,

760
00:46:31,010 --> 00:46:35,960
the variable p holds a pointer to the current processes struct proc.

761
00:46:38,890 --> 00:46:42,790
Okay, the question what makes each processes kernel thread different.

762
00:46:43,260 --> 00:46:47,310
Every process has a separate kernel thread,

763
00:46:47,930 --> 00:46:52,490
there's really two things that differentiate different processes kernel thread,

764
00:46:52,490 --> 00:46:55,670
because more than one could be executing on different cores.

765
00:46:57,650 --> 00:47:01,940
One is indeed that every process has a separate kernel stack

766
00:47:01,940 --> 00:47:05,990
and that's what's pointed to by that kstack element of struct proc

767
00:47:06,470 --> 00:47:08,150
and the other is that,

768
00:47:13,490 --> 00:47:15,680
early in,

769
00:47:16,600 --> 00:47:19,600
when usertrap which is you know the C code is called,

770
00:47:20,380 --> 00:47:22,720
by trampoline, when interrupt occurs.

771
00:47:25,640 --> 00:47:29,630
We can tell by this call them by any, any kernel code,

772
00:47:29,630 --> 00:47:36,050
can tell by calling myproc what the processes is running on the current CPU.

773
00:47:36,620 --> 00:47:39,290
And that's another thing that differentiates,

774
00:47:40,500 --> 00:47:43,500
that allows each, that allows kernel code to tell

775
00:47:43,500 --> 00:47:47,850
what process it's part of that is which processes kernel thread is executing

776
00:47:47,850 --> 00:47:50,640
and what myproc does basically use the tp register

777
00:47:50,640 --> 00:47:51,630
which you may recall,

778
00:47:52,080 --> 00:47:58,300
is set up to contain the current cores hartid or core number,

779
00:47:58,360 --> 00:48:01,000
it uses that to index into an array of structures,

780
00:48:01,000 --> 00:48:04,270
that say for each core that the scheduler sets

781
00:48:04,300 --> 00:48:07,420
whenever it switches processes to indicate for each core

782
00:48:07,420 --> 00:48:09,490
which process is running on that core.

783
00:48:10,660 --> 00:48:13,720
And so that's how different kernels are differentiated.

784
00:48:15,730 --> 00:48:17,650
Okay, so I was going to use that p value,

785
00:48:17,710 --> 00:48:21,910
the name and that p value to figure out what process is running,

786
00:48:21,910 --> 00:48:26,680
xv6 remembers the name it's that spin process just exactly as expected,

787
00:48:26,920 --> 00:48:31,570
there were two of them, I think with process ids three and four.

788
00:48:32,150 --> 00:48:32,780
Oops.

789
00:48:34,680 --> 00:48:37,260
We're currently executed again process id three,

790
00:48:37,440 --> 00:48:41,400
so after the switch we'd expect to be in process id four,

791
00:48:41,430 --> 00:48:42,900
the other spin process,

792
00:48:43,170 --> 00:48:48,390
and we we can look at the saved user registers in the trapframe.

793
00:48:54,600 --> 00:48:56,910
And these are the 32 registers

794
00:48:56,940 --> 00:49:01,890
that a trampoline code saves a way to save the user state,

795
00:49:02,130 --> 00:49:08,130
there's the user ra return address register user stack pointer,

796
00:49:08,520 --> 00:49:11,070
user program counter at hex 62.

797
00:49:12,050 --> 00:49:16,580
These are familiar things from when we looked at traps.

798
00:49:17,290 --> 00:49:20,170
And you may be the most interest is that,

799
00:49:23,290 --> 00:49:27,040
the trapframe saves the user program counter and that value 62,

800
00:49:28,090 --> 00:49:35,620
if we cared, we can look in the assembly code for spin.c.

801
00:49:36,540 --> 00:49:40,160
Oops, spin.asm look for 62,

802
00:49:41,100 --> 00:49:44,190
now we can see that owes a interrupt timer,

803
00:49:44,190 --> 00:49:50,490
interrupt occurred during this add instruction in that infinite loop in spin,

804
00:49:50,700 --> 00:49:51,900
so it's not too surprising.

805
00:49:55,390 --> 00:50:01,630
Okay, so back to a trap code [] just returned,

806
00:50:01,780 --> 00:50:06,700
I'm going to take step a few times to get us to the,

807
00:50:09,130 --> 00:50:11,590
just being about to execute this yield.

808
00:50:12,260 --> 00:50:16,040
And yield is sort of the first step in the process of giving up the CPU

809
00:50:16,040 --> 00:50:17,090
switching to the scheduler,

810
00:50:17,090 --> 00:50:20,840
letting the scheduler choose another kernel thread in process to run.

811
00:50:24,240 --> 00:50:27,000
Alright, so let's actually step into yield,

812
00:50:27,480 --> 00:50:28,710
now we're in yield, yield's.

813
00:50:30,110 --> 00:50:30,980
If you have a question.

814
00:50:31,580 --> 00:50:31,910
No.

815
00:50:43,450 --> 00:50:46,840
Okay, we're in yield,

816
00:50:47,500 --> 00:50:49,180
yield does a couple of things,

817
00:50:49,180 --> 00:50:54,370
it acquires the lock for this process,

818
00:50:54,400 --> 00:50:57,070
because it's about to make a bunch of changes to this process,

819
00:50:57,070 --> 00:50:58,960
and it doesn't want any other

820
00:50:59,140 --> 00:51:01,540
and in fact until it gives up the lock,

821
00:51:01,540 --> 00:51:04,450
the state of this process will be sort of in consistent

822
00:51:04,540 --> 00:51:09,460
like for example, it's about yield is about to change the state of the process to runnable,

823
00:51:09,460 --> 00:51:12,010
which would you know indicates that,

824
00:51:12,420 --> 00:51:14,640
the process is not running, but would like to,

825
00:51:14,940 --> 00:51:17,550
but this process is running, right,

826
00:51:17,640 --> 00:51:19,290
I mean we're running the process right now,

827
00:51:19,290 --> 00:51:21,870
that's what's executing is the kernel thread for this process

828
00:51:22,020 --> 00:51:25,530
and so the one of the many things that acquire this lock does is,

829
00:51:25,890 --> 00:51:29,070
makes it, so that even though we just changed the state runnable,

830
00:51:29,100 --> 00:51:33,430
no other core's scheduling thread will look at this process

831
00:51:33,580 --> 00:51:35,350
and because of lock

832
00:51:35,560 --> 00:51:38,020
and see that it's runnable and try to run it

833
00:51:38,290 --> 00:51:40,000
while we're still running it on this core,

834
00:51:40,030 --> 00:51:41,200
would be a disaster right,

835
00:51:41,200 --> 00:51:44,770
running the same process on two different cores,

836
00:51:44,800 --> 00:51:46,780
you know process has only one stack,

837
00:51:47,050 --> 00:51:51,070
so that means like two different cores are calling [subroutines] on the same stack,

838
00:51:51,070 --> 00:51:53,920
which is just a [recipe] for disaster.

839
00:51:54,680 --> 00:51:56,750
So we take the lock out,

840
00:51:59,380 --> 00:52:03,430
we, yield changes the state to runnable.

841
00:52:04,130 --> 00:52:05,300
And what this mean is that,

842
00:52:05,420 --> 00:52:08,290
you know we finally given up the,

843
00:52:11,960 --> 00:52:14,060
when we finally yield the CPU and given it up

844
00:52:14,060 --> 00:52:15,470
and switch the scheduler process,

845
00:52:15,470 --> 00:52:17,450
this state will be left in this runnable state,

846
00:52:17,450 --> 00:52:18,710
so that it will run again,

847
00:52:18,980 --> 00:52:20,750
because after all this was a timer interrupt,

848
00:52:20,750 --> 00:52:22,970
that interrupted a running user level process,

849
00:52:22,970 --> 00:52:24,590
that would like to continue computing.

850
00:52:25,790 --> 00:52:27,140
We're gonna leave state runnable,

851
00:52:27,140 --> 00:52:28,100
so that it will run again,

852
00:52:28,100 --> 00:52:30,440
as soon as the scheduler decides to.

853
00:52:33,290 --> 00:52:37,670
And then, the only other thing that,

854
00:52:44,090 --> 00:52:46,610
yield does is call this scheduler function.

855
00:52:48,580 --> 00:52:50,380
So I'm going to step into the scheduler function.

856
00:52:51,210 --> 00:52:53,190
I'll show this whole thing here.

857
00:53:01,470 --> 00:53:04,590
This scheduler is something does almost nothing,

858
00:53:04,590 --> 00:53:05,790
it does a bunch of checks,

859
00:53:05,880 --> 00:53:09,360
it does a whole bunch of sanity checks and panics

860
00:53:09,360 --> 00:53:11,220
and the reason for that is actually that,

861
00:53:13,550 --> 00:53:15,710
this code in xv6,

862
00:53:15,710 --> 00:53:20,840
over its many year lifetime had been among the most bug [prone]

863
00:53:20,960 --> 00:53:24,830
and have most surprises unhappy surprises,

864
00:53:24,830 --> 00:53:28,650
so there's a lot of sanity checks and panics here,

865
00:53:28,650 --> 00:53:33,250
because, because there's often bugs associated with this code.

866
00:53:35,650 --> 00:53:40,540
Alright, I'm gonna skip over these sanity checks

867
00:53:40,540 --> 00:53:47,720
and proceed to the call to swtch,

868
00:53:47,720 --> 00:53:49,670
this call to switch is where the real action happens,

869
00:53:49,670 --> 00:53:50,420
this is call the switch

870
00:53:50,420 --> 00:53:57,330
is going to save away the current kernel threads registers in p->context,

871
00:53:57,330 --> 00:54:01,950
which is the current process's saved kernel thread context, save set of registers,

872
00:54:02,640 --> 00:54:08,760
c->context, c is the pointer to this core's struct cpu,

873
00:54:09,090 --> 00:54:16,490
struct cpu has context to save registers of this core's scheduler threads,

874
00:54:16,490 --> 00:54:18,140
we're going to be switching from this thread

875
00:54:18,140 --> 00:54:19,550
and saving this thread state,

876
00:54:19,640 --> 00:54:24,140
restoring the thread state of this core's scheduler

877
00:54:24,140 --> 00:54:29,990
and continuing the execution of this core's core's scheduler thread.

878
00:54:32,760 --> 00:54:34,200
Okay, so let's see what,

879
00:54:36,250 --> 00:54:40,990
let's take a quick preview at the context

880
00:54:40,990 --> 00:54:42,550
that we're going to be switching to

881
00:54:43,030 --> 00:54:44,680
and I can get that turns out that,

882
00:54:45,860 --> 00:54:47,810
I can't actually print c->context,

883
00:54:47,810 --> 00:54:52,340
but I happen to know that c prints to cpus zero,

884
00:54:52,430 --> 00:54:54,560
just because we're on the zero with core,

885
00:54:54,560 --> 00:54:55,550
there's only one core,

886
00:54:55,820 --> 00:54:57,950
and I can print its context.

887
00:55:00,960 --> 00:55:08,610
And so this is the saved registers from this core's, scheduler thread.

888
00:55:10,560 --> 00:55:12,510
And of particular interest is the ra,

889
00:55:12,510 --> 00:55:19,150
because the ra register is where the current function call is going to return to,

890
00:55:19,150 --> 00:55:20,770
so we're going to switch the scheduler thread,

891
00:55:20,770 --> 00:55:24,070
it's going to do return and return to that ra.

892
00:55:24,930 --> 00:55:28,330
And we can find out

893
00:55:28,330 --> 00:55:32,500
where that where that return address by looking in kernel.asm.

894
00:55:35,000 --> 00:55:36,050
Actually that's.

895
00:55:43,800 --> 00:55:45,930
And as you can see this x/i,

896
00:55:45,930 --> 00:55:48,750
you know prints instructions that at a certain address,

897
00:55:48,750 --> 00:55:53,460
but it also prints the label of the name of the function

898
00:55:53,460 --> 00:55:54,600
that those instructions are in it,

899
00:55:54,600 --> 00:55:58,140
so we're going to be returning to scheduler by and by,

900
00:55:58,620 --> 00:56:01,350
that's just you know as you might expect.

901
00:56:05,040 --> 00:56:05,460
Okay.

902
00:56:11,120 --> 00:56:13,550
I want to look at what swtch actually does,

903
00:56:13,550 --> 00:56:14,720
about to call swtch.

904
00:56:18,910 --> 00:56:20,140
So I put a breakpoint on swtch

905
00:56:20,140 --> 00:56:20,830
and putting a breakpoint,

906
00:56:20,830 --> 00:56:22,330
because there's a bunch of setup code

907
00:56:22,540 --> 00:56:26,710
that pulls the values of context out of those structures,

908
00:56:26,710 --> 00:56:27,580
I'll skip over it.

909
00:56:28,820 --> 00:56:30,920
Okay, so now,

910
00:56:31,520 --> 00:56:32,870
when a breakpoint at swtch,

911
00:56:33,230 --> 00:56:36,250
the gdb won't show us the instructions,

912
00:56:36,250 --> 00:56:38,140
but we can look at switch.S

913
00:56:38,620 --> 00:56:40,690
to look at the instructions we're about to execute.

914
00:56:41,350 --> 00:56:43,210
So as you can see we're on the very first instruction,

915
00:56:43,210 --> 00:56:48,340
the store of ra to the address pointed to by a0,

916
00:56:48,640 --> 00:56:50,890
you may remember in the call to swtch,

917
00:56:50,890 --> 00:56:54,430
that the first argument was the current thread's context

918
00:56:54,430 --> 00:56:58,390
and the second argument was the context of the thread we're switching to,

919
00:56:58,420 --> 00:57:00,610
the two arguments going a0 and a1.

920
00:57:01,000 --> 00:57:05,350
And so the reason why we see all these stores through register a0 is

921
00:57:05,350 --> 00:57:09,670
because we're storing away a bunch of registers in the memory that a0 points to

922
00:57:09,670 --> 00:57:13,000
that is in the context of the thread we're switching from

923
00:57:13,300 --> 00:57:15,850
and the loads load from address a1,

924
00:57:15,850 --> 00:57:19,720
because that's a pointer to the context of the thread, we're switching to.

925
00:57:26,740 --> 00:57:34,870
Okay, and so thread, you know, swtch saves registers, loads registers

926
00:57:34,870 --> 00:57:37,000
from the target thread's context and then return

927
00:57:37,510 --> 00:57:39,550
and that's why the ra was interesting,

928
00:57:39,550 --> 00:57:43,180
because it's going to return to the place that ra pointed to namely into scheduler.

929
00:57:44,290 --> 00:57:47,260
Alright, so one question is you may notice here

930
00:57:47,260 --> 00:57:51,040
that while swtch saves ra sp and a bunch of s registers,

931
00:57:51,070 --> 00:57:53,860
one that does not save is the program counter,

932
00:57:54,360 --> 00:57:57,150
there's no mention of the program counter here,

933
00:57:57,660 --> 00:57:58,710
so why is that.

934
00:58:04,650 --> 00:58:10,380
Is it because the program counter is updated with like the function calls anyway.

935
00:58:10,980 --> 00:58:15,840
Yeah it's it's the program counter, there's no actual information value in the program counter,

936
00:58:15,840 --> 00:58:20,320
we know that we're executing right now is in switch, right.

937
00:58:20,350 --> 00:58:22,780
So there be no point in saving the program counter,

938
00:58:22,780 --> 00:58:27,040
because it has an extremely predictable value namely this instruction,

939
00:58:27,040 --> 00:58:28,900
the address of this instruction of swtch

940
00:58:29,890 --> 00:58:33,070
what we really care about is where we we're called from,

941
00:58:33,370 --> 00:58:35,410
because when we switch back to this thread,

942
00:58:35,980 --> 00:58:39,580
we want to continue executing out whatever points swhich was called from

943
00:58:39,730 --> 00:58:40,660
and it's ra,

944
00:58:40,660 --> 00:58:44,740
that holds the address of the instruction that swtch was called from.

945
00:58:45,320 --> 00:58:48,590
So it's ra, that's being saved away here,

946
00:58:48,980 --> 00:58:55,140
and ra is the point at which will be executing out again.

947
00:58:55,790 --> 00:58:56,480
Let's swtch return,

948
00:58:56,480 --> 00:58:58,040
so we even print that we can print,

949
00:58:58,930 --> 00:59:01,120
ra, oops.

950
00:59:03,740 --> 00:59:04,550
We can print ra

951
00:59:04,550 --> 00:59:06,680
and you know we haven't actually switched threads yet,

952
00:59:06,770 --> 00:59:09,260
you remember we came here from this sched function,

953
00:59:09,710 --> 00:59:13,640
so ra as you might expect the pointer back into this sched function.

954
00:59:14,960 --> 00:59:15,620
Another question is,

955
00:59:15,620 --> 00:59:18,800
how come swtch only saves 14 registers,

956
00:59:18,800 --> 00:59:19,520
I counted them,

957
00:59:19,550 --> 00:59:22,040
it only saves and restores 14 registers,

958
00:59:22,730 --> 00:59:25,520
even though the RISC-V has 32 registers

959
00:59:25,520 --> 00:59:29,300
available for the or use it for code to use.

960
00:59:29,820 --> 00:59:33,390
Why, why only half the registers are saved?

961
00:59:33,780 --> 00:59:36,960
Well, when swtch was called, it was called as a normal function,

962
00:59:36,960 --> 00:59:40,560
so whoever called swtch already assumed will swtch might modify those,

963
00:59:40,560 --> 00:59:44,490
so that that function already saved that on its stack,

964
00:59:44,520 --> 00:59:49,020
meaning that like when we jump from one to the other,

965
00:59:49,020 --> 00:59:54,030
that one is gonna self restore its caller-saved registers.

966
00:59:54,090 --> 00:59:55,350
That's exactly right,

967
00:59:55,350 --> 00:59:58,110
the swtch are called from C code,

968
00:59:58,170 --> 01:00:04,050
we know that the C compiler saves on the current stack,

969
01:00:04,140 --> 01:00:06,960
any caller saved registers,

970
01:00:06,960 --> 01:00:10,020
that have values in them that the compiler's going to need later.

971
01:00:10,590 --> 01:00:14,730
And those caller saved registers actually include,

972
01:00:15,030 --> 01:00:18,680
I think there's 18,

973
01:00:19,010 --> 01:00:20,240
depending on how you count them,

974
01:00:20,240 --> 01:00:24,020
there's somewhere between fifteen and eighteen caller saved registers,

975
01:00:24,980 --> 01:00:29,750
so the registers we see here are all the registers that aren't caller saved

976
01:00:29,750 --> 01:00:32,150
and that the compiler doesn't promise to save,

977
01:00:32,180 --> 01:00:37,640
but nevertheless may hold values that are needed by the calling function,

978
01:00:37,670 --> 01:00:42,530
so we all need to save the callee saved registers, when we're switching threads.

979
01:00:45,830 --> 01:00:47,150
Okay.

980
01:00:48,340 --> 01:00:50,830
Final thing I want to print is the,

981
01:00:50,890 --> 01:00:52,960
we do save and restore the stack pointer,

982
01:00:53,080 --> 01:00:55,690
the current stack pointer, it's like hard to tell from this value,

983
01:00:55,690 --> 01:00:59,350
what that means, that it's the kernel stack of the current process,

984
01:00:59,350 --> 01:01:01,660
which I don't know if you recall,

985
01:01:01,660 --> 01:01:05,380
but is allocated is mapped by the virtual memory system at high memory.

986
01:01:07,340 --> 01:01:15,200
Okay, so, okay, so we're going to save away the current registers

987
01:01:15,200 --> 01:01:20,240
and restore registers from scheduler thread's context,

988
01:01:20,240 --> 01:01:23,660
I don't want to execute every single one of these load or store,

989
01:01:23,660 --> 01:01:25,190
so I'm gonna step over,

990
01:01:25,580 --> 01:01:29,360
all the 14 loads, the 14 stores and the 14 loads,

991
01:01:29,360 --> 01:01:32,900
going to proceed directly to the return instructions.

992
01:01:32,900 --> 01:01:35,900
Okay, so we executed everything in swtch, except the return,

993
01:01:36,170 --> 01:01:38,690
before we do the return,

994
01:01:38,690 --> 01:01:41,000
we'll just print the interesting registers again,

995
01:01:41,000 --> 01:01:41,750
to see where we are.

996
01:01:41,780 --> 01:01:46,610
So stack pointer, now it has a different value,

997
01:01:46,820 --> 01:01:51,170
stack pointer now points into this stack zero area in memory,

998
01:01:51,170 --> 01:01:55,580
and this is actually the place it very very early in the boot sequence

999
01:01:55,640 --> 01:01:58,610
where start.S puts the stack,

1000
01:01:58,610 --> 01:02:01,310
so it may call the very first C function,

1001
01:02:01,610 --> 01:02:04,520
so actually back on the original boot stack for this CPU

1002
01:02:04,520 --> 01:02:07,820
which just happens to be where the scheduler runs.

1003
01:02:10,500 --> 01:02:14,460
Okay, the program counter actually interesting,

1004
01:02:14,460 --> 01:02:16,260
we're in swtch because we haven't returned yet

1005
01:02:16,350 --> 01:02:21,240
and the ra register now points the scheduler,

1006
01:02:21,240 --> 01:02:22,290
because we've loaded,

1007
01:02:22,620 --> 01:02:27,450
we've restored the register set previously saved by the scheduler thread.

1008
01:02:28,990 --> 01:02:31,540
And indeed, we're really now in the scheduler thread,

1009
01:02:31,540 --> 01:02:32,530
if I run on where,

1010
01:02:32,890 --> 01:02:35,680
the where now looks totally different from the last time we ran,

1011
01:02:35,680 --> 01:02:38,260
it were now indeed a call to swtch,

1012
01:02:38,260 --> 01:02:40,420
but now we're in a call from swtch

1013
01:02:40,420 --> 01:02:43,990
to switch the scheduler may at some point in the past

1014
01:02:44,290 --> 01:02:46,750
and the scheduler was run long ago during boot

1015
01:02:46,990 --> 01:02:50,860
was called as the last thing that [] did during the boot process.

1016
01:02:53,310 --> 01:02:58,500
So I'm gonna ask you know one instruction to return from switch now into scheduler.

1017
01:02:59,890 --> 01:03:01,570
So now we're in this core's scheduler,

1018
01:03:02,110 --> 01:03:03,250
look the full code,

1019
01:03:08,320 --> 01:03:10,810
so this is the scheduler code,

1020
01:03:11,480 --> 01:03:12,710
this function called scheduler,

1021
01:03:12,710 --> 01:03:15,950
now we're executing in the scheduler thread for the CPU,

1022
01:03:16,280 --> 01:03:18,050
and we're just at the point,

1023
01:03:18,170 --> 01:03:23,060
we just returned from a previous call to swtch

1024
01:03:23,060 --> 01:03:24,920
the scheduler made a while ago,

1025
01:03:25,250 --> 01:03:28,370
when it decided it was going to start running that process,

1026
01:03:28,370 --> 01:03:32,180
you know pid 3 which was the spin process that was interrupted,

1027
01:03:32,750 --> 01:03:37,070
so now it's this swtch process id 3,

1028
01:03:37,070 --> 01:03:38,510
that's spin called swtch,

1029
01:03:38,510 --> 01:03:40,550
but it's not swtch that swtch this returning

1030
01:03:40,550 --> 01:03:41,810
last swtch hasn't returned yet

1031
01:03:42,080 --> 01:03:48,310
is still saved away in process id three stack and context

1032
01:03:48,340 --> 01:03:50,770
just return from this earlier called swtch.

1033
01:03:53,160 --> 01:03:55,530
Alright, so the stuff that happens here in the scheduler,

1034
01:03:55,740 --> 01:03:59,820
were stopped running this process

1035
01:03:59,820 --> 01:04:04,170
and so you want to forget about the various things we did,

1036
01:04:04,950 --> 01:04:06,630
in the process of running this process,

1037
01:04:06,630 --> 01:04:09,600
we want to [forget] the c->proc equals zero,

1038
01:04:09,660 --> 01:04:11,310
basically means that we're forgetting that.

1039
01:04:11,970 --> 01:04:14,310
We're no longer running this process in this core's,

1040
01:04:14,310 --> 01:04:17,670
so we don't want to have anybody be confused about that,

1041
01:04:17,700 --> 01:04:23,480
let me set this per core proc pointer to zero [] this process,

1042
01:04:23,900 --> 01:04:25,220
the next thing that happens is that,

1043
01:04:25,340 --> 01:04:30,260
you remember yield, acquired the lock for this process,

1044
01:04:30,320 --> 01:04:32,420
because it didn't want any other core's scheduler

1045
01:04:32,420 --> 01:04:34,220
to look at this process and maybe run it

1046
01:04:34,760 --> 01:04:37,970
until the process was completely put to sleep.

1047
01:04:39,290 --> 01:04:41,930
We've now completed the switch away from this process,

1048
01:04:42,140 --> 01:04:45,710
so we can release the lock on the process that just yield,

1049
01:04:46,880 --> 01:04:47,930
that's the release,

1050
01:04:48,740 --> 01:04:52,910
at this point, we're still in the scheduler,

1051
01:04:52,910 --> 01:04:54,170
if there was another core,

1052
01:04:54,230 --> 01:04:58,730
at this point, some other core's scheduler could find that process,

1053
01:04:58,730 --> 01:05:00,170
because it's runnable and run it.

1054
01:05:00,700 --> 01:05:01,360
But that's okay,

1055
01:05:01,360 --> 01:05:04,210
because we've completely saved its registers,

1056
01:05:04,240 --> 01:05:07,540
we're no longer executing on its that processes stack,

1057
01:05:07,540 --> 01:05:12,050
because now executing on the this core's scheduler stack,

1058
01:05:12,290 --> 01:05:15,830
so it's actually fine if some other core decides to run that process.

1059
01:05:16,840 --> 01:05:18,430
Okay, but there's no other core,

1060
01:05:18,430 --> 01:05:21,040
so that doesn't actually happen in this demonstration.

1061
01:05:26,440 --> 01:05:31,240
Actually, I want to spend a moment talking about the p->lock a little bit more,

1062
01:05:31,510 --> 01:05:36,190
p->lock actually does a couple of things,

1063
01:05:39,120 --> 01:05:42,470
it does really two things from the point of view of scheduling,

1064
01:05:42,500 --> 01:05:45,290
one is that yielding the CPU,

1065
01:05:45,650 --> 01:05:47,420
involves multiple steps,

1066
01:05:47,420 --> 01:05:49,370
we have to set the state to runnable,

1067
01:05:49,370 --> 01:05:50,930
change the state from running to runnable,

1068
01:05:51,080 --> 01:05:55,460
we save the registers in the yielding processes context,

1069
01:05:55,550 --> 01:05:58,730
now we have to stop using the yielding processes stack,

1070
01:05:58,760 --> 01:06:00,620
there's at least three steps,

1071
01:06:00,620 --> 01:06:07,640
which take time in order to do all the steps required to yield the CPU

1072
01:06:07,910 --> 01:06:10,100
and so one of the things that lock does

1073
01:06:10,130 --> 01:06:10,910
as I mentioned is

1074
01:06:10,910 --> 01:06:13,940
prevent any other core scheduler from looking at our process

1075
01:06:13,940 --> 01:06:16,070
until all three steps have completed,

1076
01:06:16,070 --> 01:06:19,580
so the lock is basically making those steps atomic,

1077
01:06:19,860 --> 01:06:22,680
they either all happened from the point of view of other cores,

1078
01:06:22,800 --> 01:06:24,900
or none of them happened.

1079
01:06:26,160 --> 01:06:30,030
It's going to turn out also when we start running a process

1080
01:06:30,270 --> 01:06:35,490
that the p->lock is going to have a similar protective function,

1081
01:06:36,750 --> 01:06:39,210
we're going to set the state of a process to running,

1082
01:06:39,210 --> 01:06:40,800
when we start executing a process

1083
01:06:40,800 --> 01:06:46,260
and we're going to move its registers from its process context into the RISC-V registers,

1084
01:06:46,320 --> 01:06:51,910
but, if an interrupt should happen in the middle of that process,

1085
01:06:51,910 --> 01:06:54,220
the interrupt going to see the process in a weird state

1086
01:06:54,220 --> 01:06:56,560
like maybe in the state of mark running,

1087
01:06:56,560 --> 01:07:01,960
but hasn't yet finished moving its registers from the context into the RISC-V registers,

1088
01:07:02,170 --> 01:07:03,250
and that would be a disaster,

1089
01:07:03,250 --> 01:07:04,990
if a timer interrupt happened in,

1090
01:07:04,990 --> 01:07:07,000
because we might switch away from that process,

1091
01:07:07,210 --> 01:07:09,880
before it had restored its registers.

1092
01:07:11,220 --> 01:07:12,690
And switching away from that process

1093
01:07:12,690 --> 01:07:18,690
would save now uninitialized RISC-V registers into the context process's context,

1094
01:07:18,780 --> 01:07:20,520
overwriting its real registers.

1095
01:07:20,940 --> 01:07:25,620
So indeed we want starting a process that also be effectively atomic,

1096
01:07:26,430 --> 01:07:28,530
and in this case holding a lock,

1097
01:07:28,530 --> 01:07:32,070
holding p->lock across switching to a process

1098
01:07:32,790 --> 01:07:35,220
as well as preventing other cores from looking at that process

1099
01:07:35,220 --> 01:07:41,100
also turns off interrupts for the duration of [firing] up of switching to that thread

1100
01:07:41,190 --> 01:07:43,920
which prevents a timer interrupt from ever seeing

1101
01:07:43,920 --> 01:07:48,030
a process that's only midway through being switched to.

1102
01:07:51,320 --> 01:07:51,800
Okay.

1103
01:07:53,670 --> 01:07:55,470
So we're in the scheduler,

1104
01:07:55,770 --> 01:07:57,870
we're executing this loop in the scheduler's,

1105
01:07:57,870 --> 01:07:59,850
loop in the scheduler's that looks at all the process

1106
01:07:59,850 --> 01:08:01,500
in turn to find one to run

1107
01:08:01,860 --> 01:08:04,590
and in this case, we know there's another process,

1108
01:08:04,590 --> 01:08:08,870
because, there's that other spin process that we've forked,

1109
01:08:09,170 --> 01:08:13,070
but there's a lot of process lots to examine,

1110
01:08:13,460 --> 01:08:18,230
I'm going to skip over the actual process scanning of the process table

1111
01:08:18,320 --> 01:08:22,390
and go direct to the point of which, the scheduler finds the next process,

1112
01:08:22,390 --> 01:08:28,270
so I'm gonna put a breakpoint at line 474,

1113
01:08:28,270 --> 01:08:29,950
where it's actually found a new process to run.

1114
01:08:31,920 --> 01:08:36,180
[] here we are, the schedulers scan the process table

1115
01:08:36,180 --> 01:08:37,740
and found another process to run.

1116
01:08:38,840 --> 01:08:41,990
And it's going to call that process run,

1117
01:08:41,990 --> 01:08:43,670
you can see a line 468,

1118
01:08:43,670 --> 01:08:45,800
it acquired that process's lock,

1119
01:08:45,830 --> 01:08:48,530
so now it's entitled to do the various steps

1120
01:08:48,530 --> 01:08:50,510
were required to switch to that process,

1121
01:08:50,930 --> 01:08:54,860
in line 473, it set the process of state to running,

1122
01:08:55,440 --> 01:08:56,550
it's now at 474,

1123
01:08:56,550 --> 01:09:01,680
we're going to record in the CPU structure which process the CPU is executing

1124
01:09:02,730 --> 01:09:06,990
and then call swtch to save the scheduler's registers

1125
01:09:06,990 --> 01:09:10,440
and restore the target processor's registers,

1126
01:09:10,500 --> 01:09:12,030
so you can see what process is found

1127
01:09:12,030 --> 01:09:15,060
by looking at the new process name,

1128
01:09:15,150 --> 01:09:16,590
surprisingly its spin,

1129
01:09:17,590 --> 01:09:19,330
it's process id is now four,

1130
01:09:19,540 --> 01:09:21,670
used to be running three now running four.

1131
01:09:23,960 --> 01:09:25,580
We've already set the state to running,

1132
01:09:25,580 --> 01:09:31,790
so just the states running.

1133
01:09:32,550 --> 01:09:35,400
We can see where this thread is going to switch to

1134
01:09:35,430 --> 01:09:37,770
in the call to swtch line 475,

1135
01:09:39,020 --> 01:09:42,050
print this context the saved registers,

1136
01:09:42,470 --> 01:09:44,600
so where is the ra,

1137
01:09:45,140 --> 01:09:46,280
we're going to call swtch,

1138
01:09:46,280 --> 01:09:49,340
but switch as we know it returns,

1139
01:09:49,520 --> 01:09:52,430
when it returns it returns to the restore ra,

1140
01:09:52,550 --> 01:09:57,080
so we really care about is where is it that ra points to,

1141
01:09:57,080 --> 01:09:58,820
we can find that out by.

1142
01:10:00,740 --> 01:10:01,310
Oops.

1143
01:10:02,100 --> 01:10:04,290
Using x/i,

1144
01:10:07,440 --> 01:10:10,500
it's going to return ra points to some point instead,

1145
01:10:10,530 --> 01:10:11,640
that's not too surprising,

1146
01:10:11,640 --> 01:10:18,210
presumably that other spin process was suspended due to a timer interrupt,

1147
01:10:18,210 --> 01:10:21,930
which as we know called sched, what's called swtch.

1148
01:10:26,450 --> 01:10:29,750
Alright, so about to call swtch,

1149
01:10:29,750 --> 01:10:31,730
let me just bring up the swtch code again.

1150
01:10:38,660 --> 01:10:40,790
Actually enter swtch were still,

1151
01:10:41,390 --> 01:10:44,450
where shows that were still in the scheduler's context.

1152
01:10:46,940 --> 01:10:50,240
I want to again execute all of the instructions of swtch,

1153
01:10:50,240 --> 01:10:53,660
this time switching from the scheduler to the new process.

1154
01:10:54,400 --> 01:10:57,250
We skip over the 28 stores and loads.

1155
01:11:00,460 --> 01:11:04,300
Just convince ourselves that we are actually about to return to sched,

1156
01:11:04,300 --> 01:11:07,270
so now since we're about to return to sched and not scheduler,

1157
01:11:07,390 --> 01:11:10,180
we must now be in a process's kernel thread

1158
01:11:10,180 --> 01:11:13,940
and no longer the scheduler of thread,

1159
01:11:13,940 --> 01:11:16,070
indeed if we look at the backtrace,

1160
01:11:16,190 --> 01:11:18,110
we had a usertrap call,

1161
01:11:18,140 --> 01:11:19,790
that must have been a timer interrupt,

1162
01:11:19,880 --> 01:11:21,860
from long you know sometime in the past,

1163
01:11:22,310 --> 01:11:24,860
that as we've seen called yield and sched,

1164
01:11:24,860 --> 01:11:27,470
but it was the timer interrupted the other process now,

1165
01:11:27,860 --> 01:11:30,260
not in the process that we originally looked at.

1166
01:11:35,600 --> 01:11:39,320
Okay, any questions about,

1167
01:11:39,380 --> 01:11:43,010
I think I'm gonna leave off stepping through the code at this point,

1168
01:11:43,580 --> 01:11:47,330
any questions about any of the material we've seen.

1169
01:11:49,790 --> 01:11:53,540
Oh, sorry if it was, for example this [],

1170
01:11:53,810 --> 01:11:59,300
then we would see that ra would point somewhere to

1171
01:11:59,720 --> 01:12:02,780
like sleep or something like that, right.

1172
01:12:03,360 --> 01:12:06,240
Um, yes.

1173
01:12:07,040 --> 01:12:10,470
Well, we see that the where at this point

1174
01:12:10,470 --> 01:12:13,950
would include some system call implementation functions

1175
01:12:13,950 --> 01:12:14,880
and a call to sleep,

1176
01:12:14,910 --> 01:12:17,890
as it happens I think this is,

1177
01:12:18,280 --> 01:12:20,230
you're basically answering questions, yes,

1178
01:12:20,410 --> 01:12:24,850
if we had just left off executing this process

1179
01:12:24,850 --> 01:12:26,650
for some reason other than the timer interrupt,

1180
01:12:26,920 --> 01:12:32,680
swtch would be basically returning to some system call code instead of to sched,

1181
01:12:32,680 --> 01:12:35,320
as it happens I think sleep may call sched, so.

1182
01:12:37,750 --> 01:12:38,950
The backtrace would look different,

1183
01:12:38,950 --> 01:12:40,840
what just happened include sched, I guess,

1184
01:12:41,230 --> 01:12:42,940
I've chosen just one way of,

1185
01:12:43,650 --> 01:12:47,980
you know just one way of switching between processes due to timer interrupts.

1186
01:12:49,700 --> 01:12:52,340
But you also get switches to wait for user IO,

1187
01:12:52,370 --> 01:12:55,100
wait for other processes to do things like write to pipe.

1188
01:12:58,380 --> 01:13:01,980
Okay, one thing to you probably noticed is that

1189
01:13:02,190 --> 01:13:04,410
scheduler call swtch,

1190
01:13:04,940 --> 01:13:07,460
and what about to return from swtch here,

1191
01:13:07,730 --> 01:13:11,360
but we're returning really from a different call to swtch

1192
01:13:11,360 --> 01:13:12,830
than them on the scheduler are made,

1193
01:13:12,950 --> 01:13:16,160
we're returning from a call to swtch that this process made a long time ago.

1194
01:13:18,110 --> 01:13:21,290
So, you know, this is potentially a little bit confusing,

1195
01:13:21,290 --> 01:13:24,660
but, you know, this is how the [] of a thread switch work.

1196
01:13:25,730 --> 01:13:27,320
Another thing to notice is that,

1197
01:13:27,860 --> 01:13:29,000
the code we're looking at,

1198
01:13:29,000 --> 01:13:32,990
this swtch code, this is really the heart of thread switching.

1199
01:13:33,600 --> 01:13:37,320
And really all you have to do to switch switch threads is

1200
01:13:37,830 --> 01:13:40,470
save registers and restore registers,

1201
01:13:40,560 --> 01:13:42,900
threads have a lot more state than just registers,

1202
01:13:42,900 --> 01:13:45,540
they have variables and stuff in the heap

1203
01:13:45,540 --> 01:13:47,490
and who knows what,

1204
01:13:47,580 --> 01:13:50,580
all that other state is in-memory

1205
01:13:50,760 --> 01:13:52,380
and isn't going to be disturbed,

1206
01:13:52,380 --> 01:13:56,750
we've done nothing to disturb any of these threads stacks,

1207
01:13:56,750 --> 01:13:59,240
for example or heap values,

1208
01:14:00,320 --> 01:14:04,700
so the registers in the microprocessor are really the only kind of volatile state,

1209
01:14:04,730 --> 01:14:07,730
that actually needs to be saved and restored to do a thread switch,

1210
01:14:07,760 --> 01:14:10,280
all the stuffs in memory stack, for example,

1211
01:14:10,280 --> 01:14:12,800
will still be in memory on undisturbed

1212
01:14:12,890 --> 01:14:15,620
and so it doesn't have to be explicitly saved and restored.

1213
01:14:16,380 --> 01:14:19,830
Now we're only saving and restoring this microprocessor, the CPU registers,

1214
01:14:19,980 --> 01:14:24,330
because we want to reuse those [] registers in the CPU for the new thread

1215
01:14:24,330 --> 01:14:26,490
and overwrite whatever values they have.

1216
01:14:26,850 --> 01:14:31,800
So, register, that's why we have to save the old thread's registers.

1217
01:14:32,970 --> 01:14:35,820
What about, other processors state,

1218
01:14:35,850 --> 01:14:39,870
so I don't know the RISC-V processor that we're using has other flags,

1219
01:14:39,870 --> 01:14:44,970
but I know like some x86 Intel chips have like like the floating point unit state

1220
01:14:44,970 --> 01:14:46,620
and like things like that,

1221
01:14:46,620 --> 01:14:48,990
do we do we just not have that in RISC-V?

1222
01:14:50,620 --> 01:14:54,550
Your point's very well taken on other microprocessors like x86,

1223
01:14:54,550 --> 01:14:58,960
the details switching are a bit different,

1224
01:14:58,960 --> 01:15:01,690
because they have different registers in different state

1225
01:15:02,830 --> 01:15:06,370
and so the code this is very very RISC-V dependent code

1226
01:15:06,370 --> 01:15:12,050
and the switch routine for some other processor might look quite different,

1227
01:15:12,050 --> 01:15:15,050
like indeed might have to save floating point registers,

1228
01:15:15,260 --> 01:15:19,090
you know RISC-V actually uses the general purpose registers.

1229
01:15:20,220 --> 01:15:22,860
Actually, I'm not sure what it does for floating point,

1230
01:15:22,950 --> 01:15:25,020
but the kernel doesn't use floating point,

1231
01:15:25,020 --> 01:15:26,100
so you don't have to worry about it,

1232
01:15:27,300 --> 01:15:29,550
but yeah this is totally microprocessor dependent.

1233
01:15:31,710 --> 01:15:33,870
A question about the timer interrupts.

1234
01:15:34,680 --> 01:15:40,010
So it sounds like the the core of all of this scheduling working is that

1235
01:15:40,010 --> 01:15:41,390
there will be a timer interrupt,

1236
01:15:41,750 --> 01:15:45,020
what happens in cases where that malfunctions.

1237
01:15:45,260 --> 01:15:47,300
There is going to be a timer interrupt.

1238
01:15:49,070 --> 01:15:55,150
So the, I know so,

1239
01:15:55,180 --> 01:16:02,010
okay, so the reasoning for how come preemptive scheduling of user process's works is

1240
01:16:02,010 --> 01:16:07,510
that user processes execute with interrupts turned on always,

1241
01:16:07,570 --> 01:16:13,000
xv6 just ensures that interrupts are enabled before returning to user space

1242
01:16:13,000 --> 01:16:16,810
and that means that a timer interrupt can happen if you're executing in user space,

1243
01:16:17,200 --> 01:16:19,840
so there's nothing a user process,

1244
01:16:20,170 --> 01:16:22,240
if on user space that timer interrupt just will happen,

1245
01:16:22,820 --> 01:16:24,020
when the time comes.

1246
01:16:24,200 --> 01:16:25,520
So a little trickier in the kernel,

1247
01:16:25,610 --> 01:16:27,320
the kernel sometimes turns off interrupts,

1248
01:16:27,320 --> 01:16:28,850
like when you acquire a lock,

1249
01:16:28,970 --> 01:16:30,170
the interrupts are going to be turned off,

1250
01:16:30,170 --> 01:16:31,010
until you release it.

1251
01:16:31,370 --> 01:16:31,910
So.

1252
01:16:34,600 --> 01:16:38,110
So if there were some bugs in the kernel,

1253
01:16:39,040 --> 01:16:40,570
if the kernel turned off interrupts

1254
01:16:40,570 --> 01:16:42,340
and never turn them back on

1255
01:16:42,760 --> 01:16:45,910
and the code in the kernel never gave up the CPU

1256
01:16:45,970 --> 01:16:47,290
never called sleep,

1257
01:16:47,290 --> 01:16:49,030
gave up the CPU for any other reason,

1258
01:16:49,510 --> 01:16:53,050
then indeed timer interrupt would occur

1259
01:16:53,050 --> 01:16:54,310
and that would mean that,

1260
01:16:55,470 --> 01:17:00,300
this kernel code, may you know, would never give the CPU,

1261
01:17:00,300 --> 01:17:05,330
but in fact, as far as we know xv6 [] xv6,

1262
01:17:05,330 --> 01:17:08,150
so that it always turns interrupts back on,

1263
01:17:08,150 --> 01:17:12,510
or you know, if there's code in xv6, it turns off interrupts,

1264
01:17:12,600 --> 01:17:14,250
it has to turns them back on

1265
01:17:14,280 --> 01:17:18,070
and so timer interrupt can then occur in the kernel,

1266
01:17:18,070 --> 01:17:20,020
and we can switch away from this kernel thread

1267
01:17:20,170 --> 01:17:23,350
or the code returns back to user space,

1268
01:17:23,380 --> 01:17:25,150
kernel code turns back to user space,

1269
01:17:25,180 --> 01:17:27,250
we believe there's never a situation

1270
01:17:27,250 --> 01:17:31,900
in which kernel code will simply like loop with interrupts turned off forever.

1271
01:17:33,990 --> 01:17:36,090
I got, my question was more about like,

1272
01:17:36,120 --> 01:17:39,090
so I assume the interrupts are actually coming from some piece of hardware,

1273
01:17:39,330 --> 01:17:41,490
what if that piece of hardware malfunctions.

1274
01:17:41,520 --> 01:17:41,970
No.

1275
01:17:45,330 --> 01:17:47,790
It's all right, then your computer is broken, you should buy a new one.

1276
01:17:50,560 --> 01:17:50,920
Okay.

1277
01:17:51,010 --> 01:17:53,380
I mean that's a valid question for,

1278
01:17:53,410 --> 01:17:56,740
there's you know 10 billion transistors in your computer,

1279
01:17:56,740 --> 01:18:01,050
and indeed sometimes the hardware just like has bugs in it,

1280
01:18:01,050 --> 01:18:04,290
but that's beyond our reach for.

1281
01:18:05,220 --> 01:18:08,340
I mean, if you add 1 and 1 and the computer says 3,

1282
01:18:08,340 --> 01:18:12,160
then you just have deep problems,

1283
01:18:12,160 --> 01:18:14,110
that xv6 can't help you with.

1284
01:18:16,710 --> 01:18:18,900
So we're assuming that the computer works.

1285
01:18:20,370 --> 01:18:24,350
The only time when that when software,

1286
01:18:24,470 --> 01:18:28,100
I mean, there are times when software tries to compensate for hardware level errors,

1287
01:18:28,100 --> 01:18:30,740
like if you're sending packets across the network,

1288
01:18:31,100 --> 01:18:32,720
you always send a checksum,

1289
01:18:33,240 --> 01:18:36,420
so that if the network hardware flips a bit,

1290
01:18:36,720 --> 01:18:38,220
malfunctions flips a bit,

1291
01:18:38,220 --> 01:18:39,720
then you can correct that,

1292
01:18:39,720 --> 01:18:41,340
but for stuff inside the computer,

1293
01:18:42,100 --> 01:18:43,180
people tend not to,

1294
01:18:43,360 --> 01:18:43,840
it's just,

1295
01:18:45,740 --> 01:18:50,290
people basically don't try to make the software compensate for hardware errors.

1296
01:18:54,170 --> 01:18:55,340
Oh, I have a question,

1297
01:18:55,340 --> 01:18:59,720
why so like in trampoline dot, actually in swtch,

1298
01:18:59,780 --> 01:19:02,570
we write the code in assembly,

1299
01:19:02,570 --> 01:19:03,350
is that why,

1300
01:19:03,380 --> 01:19:08,450
is that because we want to make sure that exactly this thing happen,

1301
01:19:08,630 --> 01:19:11,140
so we cannot, you cann't write in C,

1302
01:19:11,140 --> 01:19:12,880
because we just need,

1303
01:19:13,440 --> 01:19:16,500
it feels like those exact things to happen basically.

1304
01:19:19,530 --> 01:19:21,540
Yeah yeah.

1305
01:19:22,800 --> 01:19:26,460
Yes, certainly we want this exact sequence to happen,

1306
01:19:26,460 --> 01:19:33,430
and C it it's very hard to talk about things like ra in C or sp

1307
01:19:34,120 --> 01:19:36,700
certainly there's no way within the C language

1308
01:19:36,700 --> 01:19:42,130
to talk about changing the stack pointer with ra register,

1309
01:19:42,660 --> 01:19:48,760
so these are things that, just can't be you can't see it in ordinary C,

1310
01:19:49,270 --> 01:19:51,430
the only way you can see it in C is

1311
01:19:51,460 --> 01:19:56,920
there, there is possible in C to sort of embed assembly instructions in C code,

1312
01:19:57,340 --> 01:20:00,760
so we could have just embedded these assembly instructions in the C function,

1313
01:20:00,760 --> 01:20:02,750
but, would amount to the same thing.

1314
01:20:03,510 --> 01:20:07,920
We're basically we're operating at a level below below C,

1315
01:20:07,920 --> 01:20:11,080
so we can't really can't really use C here.

1316
01:20:13,950 --> 01:20:14,700
I have a question,

1317
01:20:14,700 --> 01:20:17,280
about when a thread finishes executing

1318
01:20:17,280 --> 01:20:19,290
and assuming that happens in the user space

1319
01:20:19,290 --> 01:20:23,600
when we call the exec, I'm sorry, [exec] system call

1320
01:20:24,230 --> 01:20:27,620
and that also ends the process,

1321
01:20:27,980 --> 01:20:30,110
the thread assuming in the kernel space,

1322
01:20:30,200 --> 01:20:36,110
but if the thread ends within before a new timer interrupt happens,

1323
01:20:36,170 --> 01:20:38,540
does it still look like,

1324
01:20:39,570 --> 01:20:42,720
is this like the CPU still acquire by that thread

1325
01:20:42,720 --> 01:20:45,060
or do we end that thread and start a new one

1326
01:20:45,060 --> 01:20:46,380
before the new timer interrupt.

1327
01:20:46,560 --> 01:20:47,460
Oh yeah,

1328
01:20:49,460 --> 01:20:53,930
the thread, the thread yields the CPU,

1329
01:20:54,820 --> 01:20:57,160
there's the exec exec yields the CPU,

1330
01:20:57,190 --> 01:20:58,690
so there's actually many points,

1331
01:20:58,690 --> 01:21:01,960
that even though I've been driving this discussion with a timer interrupt,

1332
01:21:01,960 --> 01:21:06,070
in fact, in almost almost all cases

1333
01:21:06,070 --> 01:21:08,200
where xv6 switches between threads,

1334
01:21:08,200 --> 01:21:09,670
it's not due to timer interrupts,

1335
01:21:09,760 --> 01:21:13,960
it's because some system calls waiting for something

1336
01:21:13,960 --> 01:21:17,740
or decides that it needs to give up the CPU

1337
01:21:17,770 --> 01:21:20,950
and so for example exec does various things

1338
01:21:20,950 --> 01:21:23,590
and then calls yield to give up the CPU

1339
01:21:23,590 --> 01:21:26,240
and it does that, there's really nothing,

1340
01:21:26,240 --> 01:21:28,940
does that independently of whether there's timer interrupt.

1341
01:21:31,730 --> 01:21:32,300
Yes.

1342
01:21:37,740 --> 01:21:40,890
All right, the time is up for this lecture,

1343
01:21:40,890 --> 01:21:44,190
I think I'll continue some of this discussion next week,

1344
01:21:44,190 --> 01:21:47,370
but I'm happy to take more questions right now,

1345
01:21:47,550 --> 01:21:48,210
if people have them.

1346
01:21:52,150 --> 01:21:54,160
So let's say the operating system,

1347
01:21:54,160 --> 01:21:58,870
actually I takes on a the thread implementation,

1348
01:21:58,900 --> 01:22:05,560
so so for example you want to run multiple threads of a process on multiple CPUs,

1349
01:22:05,560 --> 01:22:07,480
like that has to be handled by the OS,

1350
01:22:07,480 --> 01:22:09,940
that cannot just be handled in user space, right?

1351
01:22:10,360 --> 01:22:11,950
How does that kind of switching work

1352
01:22:11,950 --> 01:22:15,130
is each, each thread now becomes the same as a process,

1353
01:22:15,130 --> 01:22:18,280
like is always going to loop through all existing threads

1354
01:22:18,400 --> 01:22:22,630
or you know because like each CPU will still switch between,

1355
01:22:22,630 --> 01:22:24,610
even if one process give eight cores,

1356
01:22:24,610 --> 01:22:28,330
like it's still gonna switch switch each of the CPUs between those

1357
01:22:28,330 --> 01:22:30,070
and a couple of other processes

1358
01:22:30,550 --> 01:22:32,020
and also we don't want to really

1359
01:22:32,020 --> 01:22:35,770
switch between one and the other thread on the same CPU

1360
01:22:36,130 --> 01:22:37,540
or do we, I don't know.

1361
01:22:38,640 --> 01:22:39,660
Wait, can I.

1362
01:22:41,300 --> 01:22:43,280
I'm not sure what the question is.

1363
01:22:43,880 --> 01:22:44,930
Yeah, I guess I guess,

1364
01:22:44,930 --> 01:22:47,780
can you just explain more like how does that happen.

1365
01:22:48,380 --> 01:22:49,610
Sorry, how does what happened.

1366
01:22:50,270 --> 01:22:53,750
Let's say we have multiple threads per process,

1367
01:22:53,750 --> 01:22:56,540
so that they can and they can run on different CPUs,

1368
01:22:56,660 --> 01:22:58,880
like how do we go, how do you go about there.

1369
01:22:59,480 --> 01:23:03,650
So Linux, for example supports multiple threads per process

1370
01:23:03,650 --> 01:23:09,000
and in Linux, the implementation, it's a complex implementation,

1371
01:23:09,000 --> 01:23:11,430
but maybe the simplest way to explain it is that,

1372
01:23:11,910 --> 01:23:18,230
each, it's almost as if each thread in Linux is a complete process,

1373
01:23:19,200 --> 01:23:22,170
and the the threads of a given,

1374
01:23:22,380 --> 01:23:25,050
what we would call the threads of a particular process

1375
01:23:25,080 --> 01:23:29,310
are essentially separate processes that share the same memory,

1376
01:23:30,040 --> 01:23:35,530
so Linux has sort of separated out the notion of thread of execution from address space

1377
01:23:35,530 --> 01:23:39,140
and you know you can have them separately,

1378
01:23:39,140 --> 01:23:41,150
and if you make two threads in one process,

1379
01:23:41,150 --> 01:23:44,360
it basically makes two processes that share one address space,

1380
01:23:44,690 --> 01:23:45,710
and then from then on,

1381
01:23:45,710 --> 01:23:51,320
the scheduling is not unlike what xv6 does for individual processes.

1382
01:23:51,740 --> 01:23:54,530
I see and then is there anything,

1383
01:23:54,530 --> 01:23:56,570
like does the user have to specify,

1384
01:23:56,570 --> 01:23:59,510
like okay pin each thread to a CPU

1385
01:23:59,810 --> 01:24:02,990
or how does the OS make sure

1386
01:24:02,990 --> 01:24:05,570
that different threads of the same process don't run on the same core,

1387
01:24:05,570 --> 01:24:07,400
because that's kind of defeating the purpose,

1388
01:24:07,430 --> 01:24:08,690
or not, I guess, I don't know.

1389
01:24:09,160 --> 01:24:14,050
The the, it's actually just like it's much like xv6, namely the,

1390
01:24:15,820 --> 01:24:17,200
you know there's four cores

1391
01:24:17,200 --> 01:24:21,610
and Linux will just find four things for one of those four cores,

1392
01:24:22,000 --> 01:24:25,810
they maybe, you know if there's not much going on,

1393
01:24:25,810 --> 01:24:28,780
then maybe they'll be four threads of the same process,

1394
01:24:29,370 --> 01:24:32,430
or if there's a hundred users logged in on Athena machine,

1395
01:24:32,430 --> 01:24:36,930
maybe it's one thread each from multiple different processes,

1396
01:24:36,930 --> 01:24:40,650
you know, there's not any one answer

1397
01:24:40,710 --> 01:24:43,890
or the kernel basically find something for each core to do

1398
01:24:43,890 --> 01:24:45,330
and then that core does that thing.

1399
01:24:46,500 --> 01:24:47,580
Okay, that makes sense.

1400
01:24:48,520 --> 01:24:52,120
You can, you know if you're if you want to do careful measurements,

1401
01:24:52,120 --> 01:24:53,980
there is a way to pin threads to cores,

1402
01:24:53,980 --> 01:24:57,490
but people only do it when they're up to something strange.

1403
01:25:00,540 --> 01:25:02,910
So you share the virtual table.

1404
01:25:03,930 --> 01:25:04,380
Can you say it again.

1405
01:25:04,380 --> 01:25:05,280
Virtual memory.

1406
01:25:05,490 --> 01:25:10,020
So they say they have the same page table, those threads.

1407
01:25:10,140 --> 01:25:15,510
Yeah yeah, if you're on Linux, if you create two threads in one process,

1408
01:25:15,510 --> 01:25:16,680
then you have these two threads.

1409
01:25:19,080 --> 01:25:23,760
I don't know if they like literally share the exact same page table

1410
01:25:23,760 --> 01:25:26,100
or whether their page tables are identical,

1411
01:25:26,520 --> 01:25:27,330
one or the other.

1412
01:25:28,620 --> 01:25:31,860
Is there a reason why they would have to be separate, ever,

1413
01:25:32,220 --> 01:25:34,740
if you manually map memory, or.

1414
01:25:36,540 --> 01:25:41,640
I I don't know enough to know whether which which Linux does.

1415
01:25:44,320 --> 01:25:47,860
Okay, I have another question about a small detail,

1416
01:25:48,130 --> 01:25:52,030
so basically like from my understanding when you call swtch,

1417
01:25:52,450 --> 01:25:56,050
you switch from one call to switch to another,

1418
01:25:56,080 --> 01:25:57,640
so the first time you call swtch,

1419
01:25:57,670 --> 01:26:03,310
you have to like kind of artificially create other endpoint to come back to, right.

1420
01:26:03,430 --> 01:26:04,060
Yes.

1421
01:26:04,510 --> 01:26:07,030
Because you can't just randomly jump in to [] any code.

1422
01:26:07,300 --> 01:26:13,360
Yes, you want to know where that, where that fake,

1423
01:26:14,930 --> 01:26:16,790
where that context was cooked up.

1424
01:26:17,750 --> 01:26:21,290
Probably somewhere where the process is created, I guess,

1425
01:26:21,290 --> 01:26:21,560
I don't know.

1426
01:26:21,560 --> 01:26:24,170
Yeah yeah, maybe userinit,

1427
01:26:24,820 --> 01:26:27,760
or not using allocproc.

1428
01:26:31,030 --> 01:26:32,560
I don't know.

1429
01:26:32,920 --> 01:26:35,680
There's something called fork trap or something.

1430
01:26:35,680 --> 01:26:39,250
Yeah, look at this, yeah we got forkret,

1431
01:26:39,250 --> 01:26:40,630
okay so an allocproc

1432
01:26:40,630 --> 01:26:45,790
which is called both for the very first process at boot time and by fork,

1433
01:26:46,000 --> 01:26:52,720
allocproc sets up the critical elements of the context for the new processes,

1434
01:26:53,770 --> 01:26:56,320
it, it sets up the new process's context,

1435
01:26:56,380 --> 01:26:59,230
it actually doesn't matter what most of the registers are,

1436
01:26:59,260 --> 01:27:00,820
but it does matter what ra is,

1437
01:27:00,820 --> 01:27:03,550
because that's where the swtch the very first swtch

1438
01:27:03,550 --> 01:27:05,710
and that process is going to return to ra,

1439
01:27:07,450 --> 01:27:10,120
And that process is going to need to use its own stack,

1440
01:27:10,150 --> 01:27:14,100
so ra and sp setup, faked essentially,

1441
01:27:14,370 --> 01:27:17,430
so the very first swtch or process works.

1442
01:27:18,130 --> 01:27:20,410
So, so if I understand this correctly,

1443
01:27:20,410 --> 01:27:21,670
when this swtch will happen,

1444
01:27:21,670 --> 01:27:27,490
then it'll basically just start executing the first instruction inside of the forkret,

1445
01:27:27,550 --> 01:27:30,460
as if forkret just called swtch and return from.

1446
01:27:30,910 --> 01:27:36,370
Yeah yeah, the return from switch is gonna be jump to the beginning of forkret.

1447
01:27:37,330 --> 01:27:39,800
Alright, interesting,

1448
01:27:40,370 --> 01:27:43,880
do we ever call forkret or is it always happens,

1449
01:27:44,060 --> 01:27:45,650
I think it always happens like this.

1450
01:27:45,860 --> 01:27:48,710
I don't think anything ever calls forkret for [],

1451
01:27:48,860 --> 01:27:51,600
because just,

1452
01:27:51,660 --> 01:27:57,300
yeah it's only executed in this weird way from first [] process is run.

1453
01:27:58,510 --> 01:28:04,180
It is really its job is to release the lock the scheduler took

1454
01:28:04,330 --> 01:28:05,500
and then return

1455
01:28:05,530 --> 01:28:08,890
and then this usertrapret, of course, is also fake.

1456
01:28:09,500 --> 01:28:11,610
That it's, it's,

1457
01:28:11,610 --> 01:28:14,610
yeah it's like it's as if returning from a trap,

1458
01:28:14,610 --> 01:28:17,340
except the trapframe is faked also,

1459
01:28:17,760 --> 01:28:23,490
to to have like jump to the first instruction in the user code.

1460
01:28:24,760 --> 01:28:26,080
Oh, but the trapframe,

1461
01:28:26,110 --> 01:28:29,110
it's again the same like you don't need to initialize any registers,

1462
01:28:29,110 --> 01:28:31,930
because it's like well we're going to the beginning,

1463
01:28:31,930 --> 01:28:33,970
so you don't need to assume anything.

1464
01:28:34,330 --> 01:28:36,220
Yeah, the program counter I think is.

1465
01:28:37,400 --> 01:28:37,670
Yeah.

1466
01:28:37,670 --> 01:28:39,920
It needs to be initialized to zero,

1467
01:28:40,340 --> 01:28:41,510
I don't know what else,

1468
01:28:42,320 --> 01:28:43,700
you maybe maybe it.

1469
01:28:46,440 --> 01:28:48,870
They probably if we call them it doesn't right,

1470
01:28:48,990 --> 01:28:50,730
because if we already do the call,

1471
01:28:50,730 --> 01:28:52,590
then that's going to set the program counter.

1472
01:28:52,770 --> 01:28:54,840
Yeah, so here's this only happens,

1473
01:28:54,840 --> 01:28:58,290
because fork copies, fork copies the program counter,

1474
01:28:58,840 --> 01:29:00,160
the user program counter

1475
01:29:00,580 --> 01:29:02,590
and so the only time when we're not doing fork is

1476
01:29:02,590 --> 01:29:03,670
for the very first process

1477
01:29:03,670 --> 01:29:05,830
where it's like explicitly deceptive.

1478
01:29:05,830 --> 01:29:06,490
Oh.

1479
01:29:07,310 --> 01:29:08,030
And stack pointer.

1480
01:29:08,030 --> 01:29:08,480
Oh yeah.

1481
01:29:08,510 --> 01:29:09,560
Also needs to be set up.

1482
01:29:11,130 --> 01:29:14,100
Oh yeah, because it's that EPC that's not PC,

1483
01:29:14,100 --> 01:29:17,370
that's the one that's gonna get swapped by the trap trampoline.

1484
01:29:17,610 --> 01:29:18,330
Yes.

1485
01:29:19,660 --> 01:29:20,620
Oh, I see.

1486
01:29:21,820 --> 01:29:25,540
Because the real PC is actually gonna be in trap like inside trampoline,

1487
01:29:26,020 --> 01:29:28,180
but then we're gonna switch it to jump to there.

1488
01:29:29,220 --> 01:29:30,480
Yeah.

1489
01:29:32,870 --> 01:29:33,950
Can I just ask,

1490
01:29:33,950 --> 01:29:36,590
like can you go back to the allocproc.

1491
01:29:42,940 --> 01:29:45,370
I think there's,

1492
01:29:45,960 --> 01:29:47,730
oh no sorry, forkret

1493
01:29:48,000 --> 01:29:52,920
there is something there, that happens I think for the first process only.

1494
01:29:53,960 --> 01:29:58,970
What's this for a first call, I wasn't really sure what happened.

1495
01:29:59,090 --> 01:29:59,390
Let's see,

1496
01:29:59,390 --> 01:30:03,620
the file system, the file system needs to be initialized

1497
01:30:03,620 --> 01:30:07,070
and in particular some stuff needs to be read off the disk

1498
01:30:07,190 --> 01:30:08,960
in order to get the file system going,

1499
01:30:09,520 --> 01:30:12,940
like there's this thing called the super block,

1500
01:30:12,940 --> 01:30:15,280
which describes how big the file system are,

1501
01:30:15,280 --> 01:30:17,560
and where the various things are in the file system

1502
01:30:17,620 --> 01:30:19,900
and there's also a crash recovery log,

1503
01:30:19,900 --> 01:30:22,130
that needs to be replayed

1504
01:30:22,250 --> 01:30:26,000
in order to recover from a previous crash,

1505
01:30:26,000 --> 01:30:26,600
if there was one.

1506
01:30:27,770 --> 01:30:31,040
But in order to do anything in the file system,

1507
01:30:31,040 --> 01:30:35,060
you need to be able to wait for disk operations to complete,

1508
01:30:35,060 --> 01:30:36,770
but the way xv6 works,

1509
01:30:37,010 --> 01:30:41,420
you really can only execute the file system code in the context of process,

1510
01:30:42,110 --> 01:30:44,930
in order to like wait for IO

1511
01:30:45,320 --> 01:30:49,340
and so therefore the initialization of the file system has to be deferred

1512
01:30:49,340 --> 01:30:51,650
until the first time we have a process running.

1513
01:30:53,140 --> 01:30:56,800
And that occurs in the very first process in forkret.

1514
01:31:00,350 --> 01:31:01,070
I see.

1515
01:31:01,640 --> 01:31:04,550
And I'm guessing we'll learn more about this later.

1516
01:31:05,320 --> 01:31:07,450
Yeah, not about this horrible mess,

1517
01:31:07,450 --> 01:31:10,390
but about how file systems work.

1518
01:31:11,050 --> 01:31:13,030
All right okay well, thank you,

1519
01:31:13,030 --> 01:31:15,100
I'm sorry for holding on so long.

1520
01:31:16,920 --> 01:31:18,900
Thanks for all the answers.

1521
01:31:21,840 --> 01:31:23,910
Sorry, it that init that process,

1522
01:31:23,910 --> 01:31:26,400
when this thing is executed ...

