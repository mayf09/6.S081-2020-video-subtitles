1
00:00:00,080 --> 00:00:03,840
anyone hear me can hear you

2
00:00:03,840 --> 00:00:07,440
thank you all right um i'd like to spend

3
00:00:07,440 --> 00:00:08,800
today's lecture

4
00:00:08,800 --> 00:00:11,679
talking about um threads and how xv6

5
00:00:11,679 --> 00:00:13,280
does thread switching

6
00:00:13,280 --> 00:00:15,599
this is a sort of one of our under the

7
00:00:15,599 --> 00:00:16,640
hood um

8
00:00:16,640 --> 00:00:19,439
lectures about x36 we've had lectures

9
00:00:19,439 --> 00:00:21,520
before about how system calls interrupts

10
00:00:21,520 --> 00:00:24,560
page tables and locks work um today

11
00:00:24,560 --> 00:00:26,560
we're going to talk about

12
00:00:26,560 --> 00:00:28,400
how it is that xv6 switches among

13
00:00:28,400 --> 00:00:31,990
different processes now

14
00:00:32,000 --> 00:00:33,440
the reason for this the highest level

15
00:00:33,440 --> 00:00:34,880
reason for this is that people like

16
00:00:34,880 --> 00:00:36,399
their computer to be able to do more

17
00:00:36,399 --> 00:00:38,000
than one task

18
00:00:38,000 --> 00:00:40,640
at the same time so the reason might be

19
00:00:40,640 --> 00:00:41,760
that you're supporting time

20
00:00:41,760 --> 00:00:44,399
sharing like a female allows many users

21
00:00:44,399 --> 00:00:45,760
to log in at the same time and then you

22
00:00:45,760 --> 00:00:47,520
know they can all run processes

23
00:00:47,520 --> 00:00:49,600
um or even on a single user machine or

24
00:00:49,600 --> 00:00:50,960
even your iphone

25
00:00:50,960 --> 00:00:53,360
um you may run many different processes

26
00:00:53,360 --> 00:00:55,120
and expect the computer to

27
00:00:55,120 --> 00:00:56,800
to do all the things you ask of it not

28
00:00:56,800 --> 00:01:00,229
just one thing

29
00:01:00,239 --> 00:01:02,480
another reason that people like to

30
00:01:02,480 --> 00:01:03,680
support multiple tasks

31
00:01:03,680 --> 00:01:06,640
is because it can ease program structure

32
00:01:06,640 --> 00:01:09,439
threads in particular today's topic

33
00:01:09,439 --> 00:01:11,439
are sometimes used as a way to help

34
00:01:11,439 --> 00:01:13,760
people to help programmers

35
00:01:13,760 --> 00:01:15,840
put together a program in a sort of

36
00:01:15,840 --> 00:01:18,320
simple elegant way to reduce complexity

37
00:01:18,320 --> 00:01:19,840
and you actually saw an example of this

38
00:01:19,840 --> 00:01:21,600
in the first lab with the prime number

39
00:01:21,600 --> 00:01:23,119
sieve which

40
00:01:23,119 --> 00:01:24,720
didn't use threads exactly but used

41
00:01:24,720 --> 00:01:26,880
multiple processes in order to help

42
00:01:26,880 --> 00:01:30,159
structure this your prime numbers

43
00:01:30,159 --> 00:01:31,759
software and arguably it's sort of a

44
00:01:31,759 --> 00:01:34,560
more convenient or elegant or simpler

45
00:01:34,560 --> 00:01:35,840
way to

46
00:01:35,840 --> 00:01:38,880
write that software and a final reason

47
00:01:38,880 --> 00:01:39,680
why people use

48
00:01:39,680 --> 00:01:43,119
threads is to get parallel speed up from

49
00:01:43,119 --> 00:01:45,360
multi-core machines

50
00:01:45,360 --> 00:01:48,000
so it's common to break up your program

51
00:01:48,000 --> 00:01:49,920
in a way that

52
00:01:49,920 --> 00:01:52,720
using threads to allow different parts

53
00:01:52,720 --> 00:01:54,000
of the same program to run

54
00:01:54,000 --> 00:01:55,840
on different cores and you know if you

55
00:01:55,840 --> 00:01:58,320
can maybe if you're lucky if you can

56
00:01:58,320 --> 00:02:00,320
split your program up to run on four

57
00:02:00,320 --> 00:02:01,759
threads on four cores

58
00:02:01,759 --> 00:02:03,200
you might be able to get a factor of

59
00:02:03,200 --> 00:02:06,719
four speed up and how fast it runs

60
00:02:06,719 --> 00:02:08,800
and indeed you can view the xv6 kernel

61
00:02:08,800 --> 00:02:10,000
as a

62
00:02:10,000 --> 00:02:14,000
multi-core parallel program so

63
00:02:14,000 --> 00:02:17,120
what threads are is an abstraction

64
00:02:17,120 --> 00:02:19,760
to simplify programming uh when you have

65
00:02:19,760 --> 00:02:21,840
many tasks when you want to juggle many

66
00:02:21,840 --> 00:02:22,640
tasks

67
00:02:22,640 --> 00:02:25,360
so what a thread is is a you can think

68
00:02:25,360 --> 00:02:26,959
of a thread as just being

69
00:02:26,959 --> 00:02:29,280
a single serial execution if you just

70
00:02:29,280 --> 00:02:30,640
write a program that does one thing

71
00:02:30,640 --> 00:02:33,680
after another and you run that program

72
00:02:33,680 --> 00:02:35,200
that you know you can view the program

73
00:02:35,200 --> 00:02:38,400
as a sort of single thread of control

74
00:02:38,400 --> 00:02:44,070
so um and this is a loose definition

75
00:02:44,080 --> 00:02:46,000
because there's many different sort of

76
00:02:46,000 --> 00:02:47,760
flavors of what people mean by threads

77
00:02:47,760 --> 00:02:51,440
but we'll say it's one

78
00:02:51,440 --> 00:02:54,800
serial execution so it's what you get if

79
00:02:54,800 --> 00:02:55,519
you

80
00:02:55,519 --> 00:02:59,680
um fire up one cpu and have it you know

81
00:02:59,680 --> 00:03:01,120
just execute one instruction after

82
00:03:01,120 --> 00:03:05,519
another in the ordinary way

83
00:03:05,519 --> 00:03:07,120
we often talk about a thread having

84
00:03:07,120 --> 00:03:08,720
state because it's going to turn out

85
00:03:08,720 --> 00:03:10,080
we're going to want to save away a

86
00:03:10,080 --> 00:03:12,959
thread state and restore it later

87
00:03:12,959 --> 00:03:15,040
and so the right way to think about a

88
00:03:15,040 --> 00:03:17,040
threat state for the most part

89
00:03:17,040 --> 00:03:19,120
the most important part perhaps the

90
00:03:19,120 --> 00:03:21,519
thread state is its program counter

91
00:03:21,519 --> 00:03:23,360
because it's an execution we care a lot

92
00:03:23,360 --> 00:03:25,680
about where is it in its execution

93
00:03:25,680 --> 00:03:27,200
and what address is it executing

94
00:03:27,200 --> 00:03:28,959
instructions

95
00:03:28,959 --> 00:03:31,680
but also we care about the rest of the

96
00:03:31,680 --> 00:03:33,519
microprocessor state that's required to

97
00:03:33,519 --> 00:03:35,120
support this execution

98
00:03:35,120 --> 00:03:38,400
and so that means it's um

99
00:03:38,400 --> 00:03:40,080
the state of a threat includes the

100
00:03:40,080 --> 00:03:42,239
registers that the compiler uses to hold

101
00:03:42,239 --> 00:03:43,360
variables

102
00:03:43,360 --> 00:03:45,440
and also because the just the way the

103
00:03:45,440 --> 00:03:47,280
compiler generates code

104
00:03:47,280 --> 00:03:50,959
a thread state includes a stack so

105
00:03:50,959 --> 00:03:53,120
typically each thread has its own stack

106
00:03:53,120 --> 00:03:54,720
dedicated to executing that thread and

107
00:03:54,720 --> 00:03:56,239
the stack records

108
00:03:56,239 --> 00:04:01,360
the record of function calls uh that the

109
00:04:01,360 --> 00:04:02,640
reflect the current point in the

110
00:04:02,640 --> 00:04:05,120
execution of of that thread

111
00:04:05,120 --> 00:04:07,680
um and so what a threading system xv6

112
00:04:07,680 --> 00:04:08,640
includes

113
00:04:08,640 --> 00:04:10,239
you know a threading system inside it

114
00:04:10,239 --> 00:04:12,000
what a threading system does is

115
00:04:12,000 --> 00:04:15,519
manages this interleave automate the

116
00:04:15,519 --> 00:04:16,560
interleaving

117
00:04:16,560 --> 00:04:19,120
of multiple threads we'd like to be able

118
00:04:19,120 --> 00:04:20,160
to fire up

119
00:04:20,160 --> 00:04:22,720
two or four or 100 or a thousand threads

120
00:04:22,720 --> 00:04:23,440
and have

121
00:04:23,440 --> 00:04:27,360
the threading system figure out how to

122
00:04:27,360 --> 00:04:28,800
juggle all those threads and cause them

123
00:04:28,800 --> 00:04:33,670
all to make progress and all to execute

124
00:04:33,680 --> 00:04:36,800
and there's really two main strategies

125
00:04:36,800 --> 00:04:39,759
so we want to interleave

126
00:04:39,759 --> 00:04:43,360
this is going to be a big topic here is

127
00:04:43,360 --> 00:04:45,680
how to interleave threads

128
00:04:45,680 --> 00:04:48,880
many threads one way to enter these

129
00:04:48,880 --> 00:04:50,240
execution of many threads is to have

130
00:04:50,240 --> 00:04:55,030
multiple cpus

131
00:04:55,040 --> 00:04:58,479
maybe as on a multi-core processor

132
00:04:58,479 --> 00:05:00,639
and then each cpu can run its own thread

133
00:05:00,639 --> 00:05:02,320
so if you have four cpus

134
00:05:02,320 --> 00:05:05,039
then an obvious way to run sorry four

135
00:05:05,039 --> 00:05:07,039
threads is to run one thread per cpu and

136
00:05:07,039 --> 00:05:08,880
then each thread automatically gets its

137
00:05:08,880 --> 00:05:09,600
own

138
00:05:09,600 --> 00:05:11,520
program counter and registers that is

139
00:05:11,520 --> 00:05:12,800
the program counter and registers

140
00:05:12,800 --> 00:05:14,240
associated with the

141
00:05:14,240 --> 00:05:16,320
cpu is running on but if you have four

142
00:05:16,320 --> 00:05:18,800
cpus and you have a thousand threads

143
00:05:18,800 --> 00:05:21,830
then um

144
00:05:21,840 --> 00:05:24,400
you know how using one corporate thread

145
00:05:24,400 --> 00:05:25,199
is not going to be

146
00:05:25,199 --> 00:05:27,680
enough of an answer and so so the other

147
00:05:27,680 --> 00:05:28,320
main

148
00:05:28,320 --> 00:05:31,840
strategy that we'll see indeed the

149
00:05:31,840 --> 00:05:35,039
topic of most of this lecture is how

150
00:05:35,039 --> 00:05:38,400
each cpu is going to switch

151
00:05:38,400 --> 00:05:41,840
among different um threads so if i have

152
00:05:41,840 --> 00:05:43,919
one cpu and a thousand threads

153
00:05:43,919 --> 00:05:45,280
we're going to we're going to see how

154
00:05:45,280 --> 00:05:47,919
xv6 builds a switching system

155
00:05:47,919 --> 00:05:49,840
that allows xv6 to run one thread for a

156
00:05:49,840 --> 00:05:51,280
while and then switch

157
00:05:51,280 --> 00:05:53,360
and set aside and save the state of that

158
00:05:53,360 --> 00:05:55,520
one thread and switch to executing a

159
00:05:55,520 --> 00:05:56,800
second thread for a while and then the

160
00:05:56,800 --> 00:05:58,560
third thread and so forth until it's

161
00:05:58,560 --> 00:06:00,080
executed a little bit of each thread and

162
00:06:00,080 --> 00:06:01,680
then go back and

163
00:06:01,680 --> 00:06:03,759
execute more of the of the first thread

164
00:06:03,759 --> 00:06:06,080
and so on

165
00:06:06,080 --> 00:06:08,319
and indeed with xv6 like most operating

166
00:06:08,319 --> 00:06:10,639
system combines the sex v6

167
00:06:10,639 --> 00:06:12,960
will run threads on all the cores that

168
00:06:12,960 --> 00:06:14,560
are available and each core will

169
00:06:14,560 --> 00:06:17,360
switch among threads because there's

170
00:06:17,360 --> 00:06:18,000
typical

171
00:06:18,000 --> 00:06:19,840
typically although not always there's

172
00:06:19,840 --> 00:06:21,520
typically many more threats than there

173
00:06:21,520 --> 00:06:22,160
are

174
00:06:22,160 --> 00:06:25,430
cpus

175
00:06:25,440 --> 00:06:29,199
one of the many ways in which

176
00:06:29,199 --> 00:06:31,919
different threading systems or instances

177
00:06:31,919 --> 00:06:33,039
of threading systems

178
00:06:33,039 --> 00:06:34,960
differ is in whether or not they share

179
00:06:34,960 --> 00:06:36,960
memory

180
00:06:36,960 --> 00:06:42,790
so this is a important

181
00:06:42,800 --> 00:06:46,639
point one possibility is that

182
00:06:46,639 --> 00:06:48,560
you could have a single address space

183
00:06:48,560 --> 00:06:50,240
with many threads executing in that

184
00:06:50,240 --> 00:06:51,680
address space and then they see each

185
00:06:51,680 --> 00:06:52,960
other's changes

186
00:06:52,960 --> 00:06:55,360
if one of the threads sharing some

187
00:06:55,360 --> 00:06:57,360
memory modifies a variable then

188
00:06:57,360 --> 00:06:58,560
the other thread sharing that memory

189
00:06:58,560 --> 00:07:01,360
will see the modification

190
00:07:01,360 --> 00:07:02,880
and so it's in the context of threads

191
00:07:02,880 --> 00:07:04,800
running and sharing memory that we need

192
00:07:04,800 --> 00:07:05,280
things like

193
00:07:05,280 --> 00:07:07,919
the locks that you saw in the last

194
00:07:07,919 --> 00:07:08,720
lecture

195
00:07:08,720 --> 00:07:12,319
um xv6 kernel

196
00:07:12,319 --> 00:07:16,080
is shared memory so xv6

197
00:07:16,080 --> 00:07:18,400
there's a actually because has a

198
00:07:18,400 --> 00:07:19,759
supports a notion of kernel threads

199
00:07:19,759 --> 00:07:21,360
there's one kernel thread

200
00:07:21,360 --> 00:07:23,840
per process that executes system calls

201
00:07:23,840 --> 00:07:24,800
for that process

202
00:07:24,800 --> 00:07:26,880
all those kernel threads share kernel

203
00:07:26,880 --> 00:07:28,240
memory

204
00:07:28,240 --> 00:07:34,469
so xv6 kernel threads do share memory um

205
00:07:34,479 --> 00:07:37,599
and on the other end

206
00:07:37,599 --> 00:07:40,080
xv6 is another kind of threads each user

207
00:07:40,080 --> 00:07:40,960
process

208
00:07:40,960 --> 00:07:42,479
essentially has a single thread of

209
00:07:42,479 --> 00:07:44,080
control that executes the user

210
00:07:44,080 --> 00:07:46,560
instructions for that process

211
00:07:46,560 --> 00:07:49,840
and indeed a lot of the xv6 kernel

212
00:07:49,840 --> 00:07:51,039
threading machinery

213
00:07:51,039 --> 00:07:53,199
is ultimately in support of being able

214
00:07:53,199 --> 00:07:55,599
to support and switch among

215
00:07:55,599 --> 00:07:58,160
many user processes each user process

216
00:07:58,160 --> 00:07:59,360
has some memory and a

217
00:07:59,360 --> 00:08:01,360
single thread that runs in that memory

218
00:08:01,360 --> 00:08:04,080
so xv6

219
00:08:04,080 --> 00:08:11,110
user processes

220
00:08:11,120 --> 00:08:13,599
each process has one thread and so

221
00:08:13,599 --> 00:08:14,879
there's no

222
00:08:14,879 --> 00:08:18,160
sharing of memory among threads um

223
00:08:18,160 --> 00:08:20,400
within a single xp6 user process of

224
00:08:20,400 --> 00:08:21,759
course you could have multiple processes

225
00:08:21,759 --> 00:08:22,319
but

226
00:08:22,319 --> 00:08:24,160
each of those processes is an address

227
00:08:24,160 --> 00:08:26,879
space with a single thread

228
00:08:26,879 --> 00:08:29,840
the processes in xv6 don't share memory

229
00:08:29,840 --> 00:08:31,440
in other more sophisticated

230
00:08:31,440 --> 00:08:35,200
operating systems for example linux

231
00:08:35,200 --> 00:08:38,389
linux at user level

232
00:08:38,399 --> 00:08:42,240
does allow multiple threads in a process

233
00:08:42,240 --> 00:08:43,120
and the processes

234
00:08:43,120 --> 00:08:45,600
and those threads share the memory of

235
00:08:45,600 --> 00:08:47,360
that single process um

236
00:08:47,360 --> 00:08:48,560
and that's super cool if you want to

237
00:08:48,560 --> 00:08:51,120
write user level programs that

238
00:08:51,120 --> 00:08:53,040
use level parallel programs that you

239
00:08:53,040 --> 00:08:55,200
know get speed up from multiple cores

240
00:08:55,200 --> 00:08:57,760
better requires sort of another it uses

241
00:08:57,760 --> 00:08:58,800
you know a lot of the same basic

242
00:08:58,800 --> 00:08:59,760
techniques we're going to

243
00:08:59,760 --> 00:09:01,920
talk about today but there's a certain

244
00:09:01,920 --> 00:09:03,839
amount more sophistication in linux to

245
00:09:03,839 --> 00:09:05,200
get it to

246
00:09:05,200 --> 00:09:06,959
keep track of multiple threads per

247
00:09:06,959 --> 00:09:12,230
process instead of just one

248
00:09:12,240 --> 00:09:15,279
okay at a sort of high level i just want

249
00:09:15,279 --> 00:09:15,920
to mention

250
00:09:15,920 --> 00:09:19,040
that there's other ways to support

251
00:09:19,040 --> 00:09:21,200
the interleaving of multiple tasks on a

252
00:09:21,200 --> 00:09:23,040
single computer

253
00:09:23,040 --> 00:09:24,959
and we're not going to talk about them

254
00:09:24,959 --> 00:09:26,240
but if you're curious

255
00:09:26,240 --> 00:09:28,240
you can look up things like event driven

256
00:09:28,240 --> 00:09:29,680
programming or

257
00:09:29,680 --> 00:09:33,120
state machines and these are non-thread

258
00:09:33,120 --> 00:09:34,000
techniques

259
00:09:34,000 --> 00:09:36,399
to share one computer among many

260
00:09:36,399 --> 00:09:37,519
different tasks

261
00:09:37,519 --> 00:09:40,800
it turns out you know in sort of on the

262
00:09:40,800 --> 00:09:43,040
spectrum of different schemes for

263
00:09:43,040 --> 00:09:44,880
supporting multiple tasks on a computer

264
00:09:44,880 --> 00:09:46,800
threads are not very efficient those are

265
00:09:46,800 --> 00:09:48,800
the more efficient schemes

266
00:09:48,800 --> 00:09:50,399
but threads are usually the most

267
00:09:50,399 --> 00:09:52,560
convenient way the most programmer

268
00:09:52,560 --> 00:09:54,399
friendly way

269
00:09:54,399 --> 00:09:56,399
to support lots of sort of different

270
00:09:56,399 --> 00:09:59,829
tasks

271
00:09:59,839 --> 00:10:03,920
okay there's a couple of uh

272
00:10:03,920 --> 00:10:05,360
challenges that we're gonna have to bite

273
00:10:05,360 --> 00:10:07,279
off if we want to implement

274
00:10:07,279 --> 00:10:12,150
a threading system

275
00:10:12,160 --> 00:10:16,829
the so this is just high level

276
00:10:16,839 --> 00:10:21,190
challenges

277
00:10:21,200 --> 00:10:24,640
um one is as i mentioned before how to

278
00:10:24,640 --> 00:10:25,760
actually

279
00:10:25,760 --> 00:10:28,800
implement the switching for inter the

280
00:10:28,800 --> 00:10:32,000
um the switching that allows us to

281
00:10:32,000 --> 00:10:35,030
interleave

282
00:10:35,040 --> 00:10:38,079
the execution of multiple threads

283
00:10:38,079 --> 00:10:41,760
and this sort of broad name for this

284
00:10:41,760 --> 00:10:42,560
process of

285
00:10:42,560 --> 00:10:44,320
switching deciding oh i'm going to leave

286
00:10:44,320 --> 00:10:46,399
off one thread and start executing

287
00:10:46,399 --> 00:10:47,279
another thread

288
00:10:47,279 --> 00:10:51,590
it's often called scheduling

289
00:10:51,600 --> 00:10:54,800
and we'll see that xv6 and detects as a

290
00:10:54,800 --> 00:10:56,079
actual piece of code that's the

291
00:10:56,079 --> 00:10:57,360
scheduler indeed it has multiple

292
00:10:57,360 --> 00:10:59,519
schedulers one per core

293
00:10:59,519 --> 00:11:02,720
but the general idea of how do you drive

294
00:11:02,720 --> 00:11:04,160
the decision to switch from one to

295
00:11:04,160 --> 00:11:05,839
another how to pick the next thread to

296
00:11:05,839 --> 00:11:06,880
run

297
00:11:06,880 --> 00:11:11,190
is called scheduling

298
00:11:11,200 --> 00:11:12,480
another question is if you want to

299
00:11:12,480 --> 00:11:14,079
actually implement the switch from one

300
00:11:14,079 --> 00:11:15,920
thread to another you need to save and

301
00:11:15,920 --> 00:11:16,880
restore

302
00:11:16,880 --> 00:11:19,120
so we need to decide what needs to be

303
00:11:19,120 --> 00:11:20,959
saved

304
00:11:20,959 --> 00:11:24,320
and where to save it when what needs to

305
00:11:24,320 --> 00:11:25,839
be saved when we leave off executing one

306
00:11:25,839 --> 00:11:28,320
thread and restored when we want to

307
00:11:28,320 --> 00:11:28,800
resume

308
00:11:28,800 --> 00:11:31,680
executing that thread at some later time

309
00:11:31,680 --> 00:11:33,279
and final question is what to do about

310
00:11:33,279 --> 00:11:37,990
compute bound threads

311
00:11:38,000 --> 00:11:42,240
the many of the options um

312
00:11:42,240 --> 00:11:43,519
many of the most straightforward options

313
00:11:43,519 --> 00:11:45,760
for thread switching involve the thread

314
00:11:45,760 --> 00:11:47,120
sort of voluntarily saying well i'm

315
00:11:47,120 --> 00:11:49,120
going to save away my state and sort of

316
00:11:49,120 --> 00:11:50,800
run another you know let another thread

317
00:11:50,800 --> 00:11:52,800
be run but what do we have a user

318
00:11:52,800 --> 00:11:53,600
program that's

319
00:11:53,600 --> 00:11:55,279
you know doing some long running

320
00:11:55,279 --> 00:11:57,519
calculation that might take hours

321
00:11:57,519 --> 00:11:59,120
um it's not going to be particularly

322
00:11:59,120 --> 00:12:00,959
thinking about oh now it'll be time to

323
00:12:00,959 --> 00:12:02,800
good time to let something else run

324
00:12:02,800 --> 00:12:06,000
so um it's most convenient to have some

325
00:12:06,000 --> 00:12:06,480
way of

326
00:12:06,480 --> 00:12:08,560
sort of automatically revoking control

327
00:12:08,560 --> 00:12:10,560
from some long running compute bound

328
00:12:10,560 --> 00:12:12,000
process

329
00:12:12,000 --> 00:12:13,839
setting it aside and maybe running it

330
00:12:13,839 --> 00:12:17,030
later

331
00:12:17,040 --> 00:12:18,240
all right so i'm going to talk about

332
00:12:18,240 --> 00:12:20,959
these i'm actually going to talk about

333
00:12:20,959 --> 00:12:22,800
the machinery for dealing with compute

334
00:12:22,800 --> 00:12:24,240
bound threads

335
00:12:24,240 --> 00:12:28,720
first and the

336
00:12:28,720 --> 00:12:31,120
scheme for that is something you've come

337
00:12:31,120 --> 00:12:32,399
up before

338
00:12:32,399 --> 00:12:38,320
and that's timer interrupts

339
00:12:38,320 --> 00:12:42,639
and the idea here is that there's a

340
00:12:42,639 --> 00:12:45,600
piece of hardware on each cpu on each

341
00:12:45,600 --> 00:12:46,320
core

342
00:12:46,320 --> 00:12:49,920
that generates periodic interrupts

343
00:12:49,920 --> 00:12:52,079
and the xp6 or any operating system

344
00:12:52,079 --> 00:12:53,600
really arranges to you know have those

345
00:12:53,600 --> 00:12:55,519
interrupts delivered to the kernel so

346
00:12:55,519 --> 00:12:57,519
even if we're running at user level and

347
00:12:57,519 --> 00:12:58,959
some loop that's you know

348
00:12:58,959 --> 00:13:01,760
computing the first billion digits of pi

349
00:13:01,760 --> 00:13:02,079
um

350
00:13:02,079 --> 00:13:05,040
nevertheless uh the timer interrupts to

351
00:13:05,040 --> 00:13:06,480
go off at some point maybe every 10

352
00:13:06,480 --> 00:13:08,160
milliseconds

353
00:13:08,160 --> 00:13:09,920
and transfer control from that user

354
00:13:09,920 --> 00:13:12,320
level code into the interrupt handler in

355
00:13:12,320 --> 00:13:13,200
the kernel and so

356
00:13:13,200 --> 00:13:16,079
that's the sort of first step in the

357
00:13:16,079 --> 00:13:17,440
kernel being able to

358
00:13:17,440 --> 00:13:19,519
gain control to switch among different

359
00:13:19,519 --> 00:13:21,760
user level processes through user level

360
00:13:21,760 --> 00:13:22,639
threads

361
00:13:22,639 --> 00:13:24,880
even if those user level threads aren't

362
00:13:24,880 --> 00:13:29,110
cooperative

363
00:13:29,120 --> 00:13:31,760
and the basic scheme is that in the

364
00:13:31,760 --> 00:13:33,680
interrupt handler so we can have a

365
00:13:33,680 --> 00:13:38,160
kernel handler for these interrupts

366
00:13:38,160 --> 00:13:41,279
and we'll see that uh

367
00:13:41,279 --> 00:13:44,320
the kernel handler yields

368
00:13:44,320 --> 00:13:46,160
this is the sort of usual name for this

369
00:13:46,160 --> 00:13:49,350
it yields

370
00:13:49,360 --> 00:13:51,839
the kernel handler sort of voluntarily

371
00:13:51,839 --> 00:13:53,760
yields the cpu back to the scheduler and

372
00:13:53,760 --> 00:13:54,800
tells the schedule look

373
00:13:54,800 --> 00:13:58,160
you can let something else run now

374
00:13:58,160 --> 00:14:02,000
and this yielding is really a form of

375
00:14:02,000 --> 00:14:05,360
thread switch that saves away the state

376
00:14:05,360 --> 00:14:06,639
of the current thread and

377
00:14:06,639 --> 00:14:09,680
so can be restored later um

378
00:14:09,680 --> 00:14:11,600
you know as we'll see the you know the

379
00:14:11,600 --> 00:14:12,880
full story here actually you've seen a

380
00:14:12,880 --> 00:14:14,160
lot of the full story here because it

381
00:14:14,160 --> 00:14:15,839
involves an interrupt

382
00:14:15,839 --> 00:14:17,120
which you already know about the full

383
00:14:17,120 --> 00:14:19,199
story somewhat complex but the basic

384
00:14:19,199 --> 00:14:21,279
idea is that a timer interrupt

385
00:14:21,279 --> 00:14:22,720
gives control to the kernel and the

386
00:14:22,720 --> 00:14:24,800
kernel voluntarily yields

387
00:14:24,800 --> 00:14:28,720
the cpu this is called as a piece of

388
00:14:28,720 --> 00:14:39,750
terminology preemptive scheduling

389
00:14:39,760 --> 00:14:41,199
what that means is that with the

390
00:14:41,199 --> 00:14:43,760
preempted means is that

391
00:14:43,760 --> 00:14:46,240
even if the code that's running doesn't

392
00:14:46,240 --> 00:14:48,079
you know doesn't want to

393
00:14:48,079 --> 00:14:50,320
you know doesn't explicitly yield the

394
00:14:50,320 --> 00:14:51,120
cpu

395
00:14:51,120 --> 00:14:52,560
the timer interrupt is going to take

396
00:14:52,560 --> 00:14:55,040
control away and we're going to yield

397
00:14:55,040 --> 00:14:56,639
for it and the opposite of preemptive

398
00:14:56,639 --> 00:14:58,240
scheduling

399
00:14:58,240 --> 00:15:01,519
might be called maybe voluntary

400
00:15:01,519 --> 00:15:04,949
scheduling

401
00:15:04,959 --> 00:15:06,880
and the interesting thing is that the

402
00:15:06,880 --> 00:15:09,040
you know the implementation in xv6 and

403
00:15:09,040 --> 00:15:10,560
other operating systems of preemptive

404
00:15:10,560 --> 00:15:12,399
scheduling is this time interrupt which

405
00:15:12,399 --> 00:15:14,639
forcibly takes away the cpu

406
00:15:14,639 --> 00:15:16,560
and then the kernel basically does a

407
00:15:16,560 --> 00:15:18,240
voluntary

408
00:15:18,240 --> 00:15:20,240
yield or thread thread switch on behalf

409
00:15:20,240 --> 00:15:24,310
of that process

410
00:15:24,320 --> 00:15:27,920
now um another

411
00:15:27,920 --> 00:15:30,320
just piece of terminology that comes up

412
00:15:30,320 --> 00:15:31,440
here is that

413
00:15:31,440 --> 00:15:34,160
while the thread's running there's a we

414
00:15:34,160 --> 00:15:35,759
need to

415
00:15:35,759 --> 00:15:38,000
distinguish the system needs to

416
00:15:38,000 --> 00:15:39,600
distinguish between threads that are

417
00:15:39,600 --> 00:15:42,800
currently actually running on some cpu

418
00:15:42,800 --> 00:15:46,000
versus threads that would like to run

419
00:15:46,000 --> 00:15:48,240
but aren't currently running on any cpu

420
00:15:48,240 --> 00:15:50,560
but you know could run if a cpu became

421
00:15:50,560 --> 00:15:51,440
free

422
00:15:51,440 --> 00:15:53,600
versus threads that actually don't want

423
00:15:53,600 --> 00:15:55,519
to run because they're waiting for i o

424
00:15:55,519 --> 00:15:56,160
or waiting

425
00:15:56,160 --> 00:16:00,000
for some event and unfortunately this

426
00:16:00,000 --> 00:16:03,199
distinction is often called state even

427
00:16:03,199 --> 00:16:04,000
though

428
00:16:04,000 --> 00:16:05,279
the full state of a thread is actually

429
00:16:05,279 --> 00:16:09,990
much more complicated than that

430
00:16:10,000 --> 00:16:12,560
since this is going to come up i just

431
00:16:12,560 --> 00:16:13,199
want to

432
00:16:13,199 --> 00:16:14,560
list out a couple of states that we'll

433
00:16:14,560 --> 00:16:16,320
be seeing

434
00:16:16,320 --> 00:16:18,720
and these are states that xv6 actually

435
00:16:18,720 --> 00:16:19,839
maintains there's

436
00:16:19,839 --> 00:16:21,759
a state called running which means ah

437
00:16:21,759 --> 00:16:23,519
it's actually executing on some

438
00:16:23,519 --> 00:16:26,639
core on some cpu right now there's

439
00:16:26,639 --> 00:16:29,829
runable

440
00:16:29,839 --> 00:16:31,279
which means not currently executing

441
00:16:31,279 --> 00:16:34,079
anywhere but and just a saved state

442
00:16:34,079 --> 00:16:35,440
but would like to run as soon as

443
00:16:35,440 --> 00:16:38,079
possible and then it turns out there's a

444
00:16:38,079 --> 00:16:40,160
state which won't come out much today

445
00:16:40,160 --> 00:16:41,759
but will come up next week called

446
00:16:41,759 --> 00:16:43,040
sleeping which is

447
00:16:43,040 --> 00:16:44,720
means the threads waiting for some i o

448
00:16:44,720 --> 00:16:47,199
event and only wants to run after the i

449
00:16:47,199 --> 00:16:48,160
o event occurs

450
00:16:48,160 --> 00:16:49,839
so today we're mostly concerned with

451
00:16:49,839 --> 00:16:51,360
running and runnable threads

452
00:16:51,360 --> 00:16:53,519
and what this preemptive switch does

453
00:16:53,519 --> 00:16:54,959
what this timer interrupt does

454
00:16:54,959 --> 00:16:57,040
and yield is basically convert a running

455
00:16:57,040 --> 00:16:58,800
thread whatever thread was interrupted

456
00:16:58,800 --> 00:16:59,839
by the timer

457
00:16:59,839 --> 00:17:02,720
into a runnable thread that is thread

458
00:17:02,720 --> 00:17:03,360
that's

459
00:17:03,360 --> 00:17:05,679
by yielding or converting that thread

460
00:17:05,679 --> 00:17:06,880
into a thread that's not running right

461
00:17:06,880 --> 00:17:07,520
now but

462
00:17:07,520 --> 00:17:09,120
would actually like to clearly because

463
00:17:09,120 --> 00:17:11,120
it was running at the time of the

464
00:17:11,120 --> 00:17:14,230
timer interrupt

465
00:17:14,240 --> 00:17:17,520
okay so um running thread it's program

466
00:17:17,520 --> 00:17:19,199
counter and registers are

467
00:17:19,199 --> 00:17:21,760
actually in the cpu you know in the

468
00:17:21,760 --> 00:17:23,600
hardware registers of the cpu that's

469
00:17:23,600 --> 00:17:25,360
executing it

470
00:17:25,360 --> 00:17:28,480
a runnable thread though has no you know

471
00:17:28,480 --> 00:17:30,400
it's not doesn't have a cpu associated

472
00:17:30,400 --> 00:17:31,520
with it

473
00:17:31,520 --> 00:17:33,679
and therefore we need to save for every

474
00:17:33,679 --> 00:17:34,960
runnable state

475
00:17:34,960 --> 00:17:38,320
we need to save whatever

476
00:17:38,320 --> 00:17:41,120
cpu state whatever state the cpu was

477
00:17:41,120 --> 00:17:42,640
keeping

478
00:17:42,640 --> 00:17:46,000
when that thread was running so we need

479
00:17:46,000 --> 00:17:47,679
to copy the cpu contents

480
00:17:47,679 --> 00:17:48,960
you know which is not ram but just

481
00:17:48,960 --> 00:17:51,919
registers really um from the cpu

482
00:17:51,919 --> 00:17:54,320
into memory somewhere to save them when

483
00:17:54,320 --> 00:17:54,960
we

484
00:17:54,960 --> 00:17:56,640
turn a thread from running to runnable

485
00:17:56,640 --> 00:17:58,000
and again this is the

486
00:17:58,000 --> 00:17:59,840
basically the state we have to

487
00:17:59,840 --> 00:18:01,360
explicitly save here's

488
00:18:01,360 --> 00:18:04,080
just the state the executing state and

489
00:18:04,080 --> 00:18:05,679
the cpu which is the

490
00:18:05,679 --> 00:18:09,039
uh program counter and the

491
00:18:09,039 --> 00:18:11,440
registers and the cpu so these need to

492
00:18:11,440 --> 00:18:12,880
be saved

493
00:18:12,880 --> 00:18:15,679
when we convert a thread to runnable

494
00:18:15,679 --> 00:18:17,760
when some scheduler finally decides to

495
00:18:17,760 --> 00:18:20,720
run a runnable thread then as part of

496
00:18:20,720 --> 00:18:22,160
the many steps and getting that thread

497
00:18:22,160 --> 00:18:23,039
going again

498
00:18:23,039 --> 00:18:26,000
and resuming it we're going to see that

499
00:18:26,000 --> 00:18:28,000
the program count the saved program

500
00:18:28,000 --> 00:18:29,679
counter registers are copied

501
00:18:29,679 --> 00:18:32,960
back into the cpus

502
00:18:32,960 --> 00:18:34,480
actual register on the cpu that the

503
00:18:34,480 --> 00:18:38,870
scheduler decides to run it on

504
00:18:38,880 --> 00:18:42,000
all right any questions about these this

505
00:18:42,000 --> 00:18:49,750
terminology

506
00:18:49,760 --> 00:18:51,679
all right i'm going to now sort of talk

507
00:18:51,679 --> 00:18:52,799
about a

508
00:18:52,799 --> 00:18:57,909
sort of more xv6 oriented view of things

509
00:18:57,919 --> 00:19:01,440
i'm going to draw two pictures really of

510
00:19:01,440 --> 00:19:03,280
threads in xy6 a kind of simplified

511
00:19:03,280 --> 00:19:05,520
picture and a more detailed picture

512
00:19:05,520 --> 00:19:08,950
so as usual we have the user

513
00:19:08,960 --> 00:19:12,880
stuff up here and the kernel down here

514
00:19:12,880 --> 00:19:16,000
um we might be running you know multiple

515
00:19:16,000 --> 00:19:16,960
processes

516
00:19:16,960 --> 00:19:19,520
at user level maybe you know the c

517
00:19:19,520 --> 00:19:20,559
compiler

518
00:19:20,559 --> 00:19:24,559
and the ls and a shell

519
00:19:24,559 --> 00:19:26,320
they may or may not be all wanting to

520
00:19:26,320 --> 00:19:29,280
run at the same time

521
00:19:29,280 --> 00:19:36,830
at user level each of these processes

522
00:19:36,840 --> 00:19:38,320
has

523
00:19:38,320 --> 00:19:41,919
you know it has memory and of particular

524
00:19:41,919 --> 00:19:43,840
interest to us each of these processes

525
00:19:43,840 --> 00:19:47,270
has a user stack

526
00:19:47,280 --> 00:19:50,960
and while it's running it as registers

527
00:19:50,960 --> 00:19:53,840
in the risc 5 hardware so pc plus

528
00:19:53,840 --> 00:19:55,600
registers

529
00:19:55,600 --> 00:19:57,360
all right so while the program is

530
00:19:57,360 --> 00:19:58,640
running you know there's

531
00:19:58,640 --> 00:20:00,080
this essentially a thread of control

532
00:20:00,080 --> 00:20:02,240
that's running up at user level

533
00:20:02,240 --> 00:20:04,240
um and the way i'm going to talk about

534
00:20:04,240 --> 00:20:06,240
it is as if

535
00:20:06,240 --> 00:20:09,600
there's a user thread that consists of

536
00:20:09,600 --> 00:20:09,919
the

537
00:20:09,919 --> 00:20:12,000
user stack user memory user program

538
00:20:12,000 --> 00:20:13,600
counter user registers

539
00:20:13,600 --> 00:20:15,360
if the program makes a system callers

540
00:20:15,360 --> 00:20:17,120
interrupted and

541
00:20:17,120 --> 00:20:20,400
goes into the kernel then this stuff

542
00:20:20,400 --> 00:20:21,440
saved away in the

543
00:20:21,440 --> 00:20:25,520
this program's trap frame and a

544
00:20:25,520 --> 00:20:29,039
kernel the kernel thread for

545
00:20:29,039 --> 00:20:32,159
this program is activated and so this is

546
00:20:32,159 --> 00:20:32,480
the

547
00:20:32,480 --> 00:20:35,679
the trap frame holds saved user stuff

548
00:20:35,679 --> 00:20:36,960
after we saved away the user

549
00:20:36,960 --> 00:20:40,000
piece program counter registers then we

550
00:20:40,000 --> 00:20:41,840
switched the cpu to using the kernel

551
00:20:41,840 --> 00:20:45,430
stack

552
00:20:45,440 --> 00:20:47,039
and you know we don't need to restore

553
00:20:47,039 --> 00:20:48,559
registers because

554
00:20:48,559 --> 00:20:52,559
um the the kernel thread for process

555
00:20:52,559 --> 00:20:53,039
isn't

556
00:20:53,039 --> 00:20:54,799
really running and has no real save

557
00:20:54,799 --> 00:20:56,480
state um

558
00:20:56,480 --> 00:20:59,120
when the user thread is running um

559
00:20:59,120 --> 00:21:00,320
instead it's sort of the

560
00:21:00,320 --> 00:21:02,080
kernel thread is kind of activated on

561
00:21:02,080 --> 00:21:05,520
its stack the first time in

562
00:21:05,520 --> 00:21:12,149
the trampoline and user trap code

563
00:21:12,159 --> 00:21:13,679
and then the kernel runs for a while

564
00:21:13,679 --> 00:21:15,520
maybe running a system call or an

565
00:21:15,520 --> 00:21:18,320
interrupt handler or whatever it may be

566
00:21:18,320 --> 00:21:20,960
and sometimes uh if it's a system call

567
00:21:20,960 --> 00:21:22,640
in particular we'll just simply return

568
00:21:22,640 --> 00:21:24,320
from this point back to the

569
00:21:24,320 --> 00:21:26,640
same process and the return to user

570
00:21:26,640 --> 00:21:27,760
space will restore

571
00:21:27,760 --> 00:21:29,919
the this program's program counter and

572
00:21:29,919 --> 00:21:30,960
registers

573
00:21:30,960 --> 00:21:33,280
but it could also be that instead of

574
00:21:33,280 --> 00:21:35,039
simply returning

575
00:21:35,039 --> 00:21:36,480
for one reason or another maybe because

576
00:21:36,480 --> 00:21:38,080
it was a timer interrupt we're actually

577
00:21:38,080 --> 00:21:40,320
going to switch to another process and

578
00:21:40,320 --> 00:21:43,120
the very high level view of that is that

579
00:21:43,120 --> 00:21:44,000
if the

580
00:21:44,000 --> 00:21:46,799
xv6 scheduler decides to switch from

581
00:21:46,799 --> 00:21:49,600
this process to a different process

582
00:21:49,600 --> 00:21:51,280
what the first thing that really happens

583
00:21:51,280 --> 00:21:53,120
is that we're going to switch

584
00:21:53,120 --> 00:21:55,520
kernel threads from this processes

585
00:21:55,520 --> 00:21:56,400
kernel thread

586
00:21:56,400 --> 00:21:58,400
to the other processor's kernel thread

587
00:21:58,400 --> 00:21:59,760
and then the other processes kernel

588
00:21:59,760 --> 00:22:02,000
thread will return back to user space so

589
00:22:02,000 --> 00:22:03,840
supposing that the c compiler rsa needs

590
00:22:03,840 --> 00:22:05,520
to read the disk

591
00:22:05,520 --> 00:22:08,000
and so it's going to yield the cpu while

592
00:22:08,000 --> 00:22:09,600
it's sleeping to wait for the disk read

593
00:22:09,600 --> 00:22:10,400
to complete

594
00:22:10,400 --> 00:22:12,720
maybe ls wants to execute and is in

595
00:22:12,720 --> 00:22:14,640
runnable state

596
00:22:14,640 --> 00:22:16,960
what the xv6 scheduler maybe may do is

597
00:22:16,960 --> 00:22:18,080
that well

598
00:22:18,080 --> 00:22:20,720
um if ls is in runnable state that means

599
00:22:20,720 --> 00:22:22,960
it left off somewhere and its state

600
00:22:22,960 --> 00:22:25,200
was saved away possibly by a timer

601
00:22:25,200 --> 00:22:27,280
interrupt and so the ls will actually

602
00:22:27,280 --> 00:22:28,240
have a saved

603
00:22:28,240 --> 00:22:30,799
trap frame with user registers and its

604
00:22:30,799 --> 00:22:31,760
own

605
00:22:31,760 --> 00:22:35,200
kernel stack and as it turns out a seed

606
00:22:35,200 --> 00:22:38,230
set of

607
00:22:38,240 --> 00:22:40,000
kernel registers associated with the

608
00:22:40,000 --> 00:22:41,520
kernel thread which

609
00:22:41,520 --> 00:22:44,480
is going to be called the context so if

610
00:22:44,480 --> 00:22:46,400
xv6 switches from the compiler

611
00:22:46,400 --> 00:22:50,320
kernel thread to ls's kernel thread

612
00:22:50,320 --> 00:22:53,840
xv6 will save away the kernel registers

613
00:22:53,840 --> 00:22:56,159
in a context for the

614
00:22:56,159 --> 00:22:59,919
seat the compiler's kernel thread

615
00:22:59,919 --> 00:23:04,159
switch to the ls thread to a complex

616
00:23:04,159 --> 00:23:05,919
scheme which i'll describe a little bit

617
00:23:05,919 --> 00:23:08,720
later will restore

618
00:23:08,720 --> 00:23:11,840
ls's kernel thread registers from the

619
00:23:11,840 --> 00:23:12,320
same

620
00:23:12,320 --> 00:23:14,640
previously saved context from when ls

621
00:23:14,640 --> 00:23:15,919
last left off

622
00:23:15,919 --> 00:23:17,760
maybe ls will finish whatever system

623
00:23:17,760 --> 00:23:20,320
call it was executing you know what

624
00:23:20,320 --> 00:23:24,320
on the ls's kernel thread stack

625
00:23:24,320 --> 00:23:25,919
um and then return back to ls from the

626
00:23:25,919 --> 00:23:27,840
system call and on the way to return to

627
00:23:27,840 --> 00:23:29,280
user space it'll restore these

628
00:23:29,280 --> 00:23:31,039
previously saved

629
00:23:31,039 --> 00:23:34,559
user registers for ls um and then resume

630
00:23:34,559 --> 00:23:37,840
executing ls so

631
00:23:37,840 --> 00:23:39,200
there's a bunch of details here which

632
00:23:39,200 --> 00:23:41,039
we'll talk about but uh

633
00:23:41,039 --> 00:23:42,880
maybe the main point here is that

634
00:23:42,880 --> 00:23:45,039
whenever in xv6

635
00:23:45,039 --> 00:23:47,840
see direct user to user context switches

636
00:23:47,840 --> 00:23:48,960
when we're switching

637
00:23:48,960 --> 00:23:52,080
from one process to another always

638
00:23:52,080 --> 00:23:55,600
the sort of strategy by which xv6

639
00:23:55,600 --> 00:23:57,440
switches from executing one process to

640
00:23:57,440 --> 00:23:58,799
another process

641
00:23:58,799 --> 00:24:00,799
is you jump into the kernel save the

642
00:24:00,799 --> 00:24:02,480
process state run this

643
00:24:02,480 --> 00:24:04,720
process as kernel thread switch to the

644
00:24:04,720 --> 00:24:05,760
kernel thread of a

645
00:24:05,760 --> 00:24:07,600
another process that's suspended itself

646
00:24:07,600 --> 00:24:09,520
and then return and restore user

647
00:24:09,520 --> 00:24:10,960
registers so it's always the sort of

648
00:24:10,960 --> 00:24:12,240
indirect strategy

649
00:24:12,240 --> 00:24:13,520
it's actually even more indirect than

650
00:24:13,520 --> 00:24:15,760
this to threat switch

651
00:24:15,760 --> 00:24:17,360
where the net effect is to switch from

652
00:24:17,360 --> 00:24:18,960
one user process

653
00:24:18,960 --> 00:24:23,909
to another user process

654
00:24:23,919 --> 00:24:28,950
questions about this diagram or anything

655
00:24:28,960 --> 00:24:31,520
the switch to the scheduler that happens

656
00:24:31,520 --> 00:24:32,000
in between

657
00:24:32,000 --> 00:24:35,520
those two right yep all right

658
00:24:35,520 --> 00:24:37,440
let me talk about the scheduler so the

659
00:24:37,440 --> 00:24:38,720
real picture's actually

660
00:24:38,720 --> 00:24:41,919
um significantly more complex than that

661
00:24:41,919 --> 00:24:46,640
um this is a more

662
00:24:46,640 --> 00:24:48,080
going to be more full diagram let's say

663
00:24:48,080 --> 00:24:51,039
we have process 1

664
00:24:51,039 --> 00:24:55,360
which is executing and process 2 which

665
00:24:55,360 --> 00:24:58,080
is runnable but not currently running

666
00:24:58,080 --> 00:24:59,600
now the additional layer of details we

667
00:24:59,600 --> 00:25:01,279
actually have multiple cores

668
00:25:01,279 --> 00:25:04,320
um in xv6 let's say we have two cores so

669
00:25:04,320 --> 00:25:06,320
that means that sort of at the hardware

670
00:25:06,320 --> 00:25:07,760
level

671
00:25:07,760 --> 00:25:11,360
um we have cpu0 which is one of the

672
00:25:11,360 --> 00:25:12,640
cores

673
00:25:12,640 --> 00:25:19,909
and let's say cpu one

674
00:25:19,919 --> 00:25:22,400
and the more full story about how we get

675
00:25:22,400 --> 00:25:23,520
from

676
00:25:23,520 --> 00:25:26,640
executing user space

677
00:25:26,640 --> 00:25:30,000
to in one process executing user spaces

678
00:25:30,000 --> 00:25:32,080
in another runnable but not yet running

679
00:25:32,080 --> 00:25:33,840
process

680
00:25:33,840 --> 00:25:36,159
now the first part's about the same as i

681
00:25:36,159 --> 00:25:36,960
talked about

682
00:25:36,960 --> 00:25:40,640
and they say a timer interrupt forces

683
00:25:40,640 --> 00:25:42,720
transfer control from the user process

684
00:25:42,720 --> 00:25:46,159
into the kernel the trampoline code

685
00:25:46,159 --> 00:25:48,320
saves the user registers the trap frame

686
00:25:48,320 --> 00:25:51,430
for process one

687
00:25:51,440 --> 00:25:55,200
and then executes user trap which

688
00:25:55,200 --> 00:25:56,159
you know figures out what to do with

689
00:25:56,159 --> 00:25:58,159
this trap or interrupt you know system

690
00:25:58,159 --> 00:25:58,720
call

691
00:25:58,720 --> 00:26:01,120
let's say it's a for a little while

692
00:26:01,120 --> 00:26:02,480
we're executing

693
00:26:02,480 --> 00:26:06,159
ordinary kernel c code on the kernel

694
00:26:06,159 --> 00:26:10,320
stack of process one

695
00:26:10,320 --> 00:26:12,080
let's say process one the kernel code

696
00:26:12,080 --> 00:26:13,919
for process ones decides it wants to

697
00:26:13,919 --> 00:26:16,880
yield the cpu um it does a bunch of

698
00:26:16,880 --> 00:26:18,559
things which we'll see the details of

699
00:26:18,559 --> 00:26:21,520
um that end up in a call to this routine

700
00:26:21,520 --> 00:26:22,799
switch

701
00:26:22,799 --> 00:26:24,799
just sort of the one of the central

702
00:26:24,799 --> 00:26:26,240
routines in this story

703
00:26:26,240 --> 00:26:30,080
switch saves away this context the

704
00:26:30,080 --> 00:26:31,279
registers

705
00:26:31,279 --> 00:26:33,039
for the kernel thread that's running in

706
00:26:33,039 --> 00:26:34,799
context want so there's two sets of

707
00:26:34,799 --> 00:26:36,480
registers the user registers

708
00:26:36,480 --> 00:26:38,640
the trap frame the kernel thread

709
00:26:38,640 --> 00:26:42,470
registers in the context

710
00:26:42,480 --> 00:26:44,320
switch doesn't actually switch switches

711
00:26:44,320 --> 00:26:46,720
from one content from one thread to

712
00:26:46,720 --> 00:26:48,559
another but in fact

713
00:26:48,559 --> 00:26:52,080
the way xp6 is designed the only place

714
00:26:52,080 --> 00:26:55,520
that a user thread sorry the kernel

715
00:26:55,520 --> 00:26:57,679
thread running on a cpu can switch to

716
00:26:57,679 --> 00:26:59,520
is what's called the scheduler thread

717
00:26:59,520 --> 00:27:02,870
for that cpu

718
00:27:02,880 --> 00:27:04,559
so we can't even switch directly to

719
00:27:04,559 --> 00:27:06,640
another process we can only switch to

720
00:27:06,640 --> 00:27:10,310
the scheduler thread so there's a

721
00:27:10,320 --> 00:27:13,600
the complete thread apparatus

722
00:27:13,600 --> 00:27:16,400
dedicated to the scheduler for cpu zero

723
00:27:16,400 --> 00:27:17,840
since we're running on cpus

724
00:27:17,840 --> 00:27:20,480
this switch is going to switch to the

725
00:27:20,480 --> 00:27:22,880
previously saved

726
00:27:22,880 --> 00:27:25,520
registers for the scheduler thread so

727
00:27:25,520 --> 00:27:29,590
let's say it's scheduler 0

728
00:27:29,600 --> 00:27:33,360
and in the scheduler for cpu 0

729
00:27:33,360 --> 00:27:36,000
switch will by restoring these registers

730
00:27:36,000 --> 00:27:37,440
since the registers include the stack

731
00:27:37,440 --> 00:27:38,399
pointer

732
00:27:38,399 --> 00:27:40,880
the return from switch as we'll see will

733
00:27:40,880 --> 00:27:43,750
now actually

734
00:27:43,760 --> 00:27:48,799
return up to the um scheduler

735
00:27:48,799 --> 00:27:52,549
function

736
00:27:52,559 --> 00:27:55,120
on cpu zero and this scheduler function

737
00:27:55,120 --> 00:27:57,520
will do some cleanup to finish putting

738
00:27:57,520 --> 00:27:59,600
process one to sleep then it'll look in

739
00:27:59,600 --> 00:28:00,960
the process

740
00:28:00,960 --> 00:28:02,399
table for another process to run a

741
00:28:02,399 --> 00:28:04,559
runnable process

742
00:28:04,559 --> 00:28:07,919
and if it finds one and so we've

743
00:28:07,919 --> 00:28:09,840
sort of gone down here and up into the

744
00:28:09,840 --> 00:28:11,919
scheduler if the scheduler finds another

745
00:28:11,919 --> 00:28:13,039
process to run or even

746
00:28:13,039 --> 00:28:15,600
finds if process one is runnable and

747
00:28:15,600 --> 00:28:17,120
still wants to run it may find process

748
00:28:17,120 --> 00:28:18,720
one nothing else nothing else wants to

749
00:28:18,720 --> 00:28:19,679
run

750
00:28:19,679 --> 00:28:21,840
but in any case the scheduler will call

751
00:28:21,840 --> 00:28:23,200
switch again to

752
00:28:23,200 --> 00:28:27,919
switch contexts to say process two

753
00:28:27,919 --> 00:28:29,679
in the process of which it will save its

754
00:28:29,679 --> 00:28:31,039
own registers again

755
00:28:31,039 --> 00:28:33,760
in its own context there'll be a

756
00:28:33,760 --> 00:28:35,840
previously saved

757
00:28:35,840 --> 00:28:37,919
context too from whenever process two

758
00:28:37,919 --> 00:28:38,960
left off

759
00:28:38,960 --> 00:28:41,039
that those this set of registers will be

760
00:28:41,039 --> 00:28:42,720
restored

761
00:28:42,720 --> 00:28:44,559
process two will have made a previous

762
00:28:44,559 --> 00:28:46,799
call to switch

763
00:28:46,799 --> 00:28:49,360
um to switch to the scheduler thread

764
00:28:49,360 --> 00:28:51,120
just like process one did when it left

765
00:28:51,120 --> 00:28:51,679
off

766
00:28:51,679 --> 00:28:53,200
that call to switch will return to

767
00:28:53,200 --> 00:28:54,880
whatever

768
00:28:54,880 --> 00:28:56,880
system call or interrupt process two was

769
00:28:56,880 --> 00:28:58,960
in when that's finished

770
00:28:58,960 --> 00:29:01,200
there will be a previously saved trap

771
00:29:01,200 --> 00:29:02,640
frame for process 2

772
00:29:02,640 --> 00:29:04,320
that will contain user registers those

773
00:29:04,320 --> 00:29:06,399
will be restored and will

774
00:29:06,399 --> 00:29:10,310
return back up into user space

775
00:29:10,320 --> 00:29:12,399
and there's a complete a separate

776
00:29:12,399 --> 00:29:14,480
scheduler

777
00:29:14,480 --> 00:29:16,880
thread for each cpu so it'll also be

778
00:29:16,880 --> 00:29:19,830
saved

779
00:29:19,840 --> 00:29:22,320
context for the scheduler thread for cpu

780
00:29:22,320 --> 00:29:24,000
one

781
00:29:24,000 --> 00:29:27,760
and a scheduler loop

782
00:29:27,760 --> 00:29:29,440
running on scheduler one and whatever

783
00:29:29,440 --> 00:29:31,039
process you know process three or

784
00:29:31,039 --> 00:29:31,679
something

785
00:29:31,679 --> 00:29:33,919
is running on cpu one when it decides to

786
00:29:33,919 --> 00:29:37,440
give up the cpu it'll switch into

787
00:29:37,440 --> 00:29:40,799
the scheduler thread for it for its

788
00:29:40,799 --> 00:29:45,120
cpu all right there's a question where

789
00:29:45,120 --> 00:29:48,159
the context stored

790
00:29:48,159 --> 00:29:51,279
it turns out that for the operations

791
00:29:51,279 --> 00:29:52,720
i've been talking about

792
00:29:52,720 --> 00:29:57,200
the saved in fact always the

793
00:29:57,200 --> 00:30:01,120
for a thread switch um

794
00:30:01,120 --> 00:30:03,600
these contexts these saved register sets

795
00:30:03,600 --> 00:30:05,200
for kernel threads are in the process

796
00:30:05,200 --> 00:30:06,159
structure

797
00:30:06,159 --> 00:30:09,039
so any given kernel thread can only have

798
00:30:09,039 --> 00:30:09,440
one

799
00:30:09,440 --> 00:30:12,559
set of saved kernel registers because

800
00:30:12,559 --> 00:30:13,840
each thread is

801
00:30:13,840 --> 00:30:15,600
only executing at sort of a single place

802
00:30:15,600 --> 00:30:16,880
and its context

803
00:30:16,880 --> 00:30:19,120
kind of reflects that place that it was

804
00:30:19,120 --> 00:30:20,960
executing when it left off because

805
00:30:20,960 --> 00:30:23,279
a thread is a single thread of control

806
00:30:23,279 --> 00:30:24,480
so a thread

807
00:30:24,480 --> 00:30:26,480
really only needs one context full of

808
00:30:26,480 --> 00:30:28,080
registers so it's in the process

809
00:30:28,080 --> 00:30:28,799
structure it's p

810
00:30:28,799 --> 00:30:31,990
arrow

811
00:30:32,000 --> 00:30:35,269
procontext

812
00:30:35,279 --> 00:30:37,039
and the scheduler each scheduler thread

813
00:30:37,039 --> 00:30:39,120
has its own context which is actually

814
00:30:39,120 --> 00:30:40,159
not in the

815
00:30:40,159 --> 00:30:42,399
there's no process associated with this

816
00:30:42,399 --> 00:30:43,600
scheduler thread

817
00:30:43,600 --> 00:30:45,679
so this is actually the scheduler's

818
00:30:45,679 --> 00:30:47,600
contacts is stored in

819
00:30:47,600 --> 00:30:51,360
the struct cpu for that core

820
00:30:51,360 --> 00:30:53,279
there's an array of these cpu structs

821
00:30:53,279 --> 00:30:55,120
one per core each one has a

822
00:30:55,120 --> 00:30:58,230
context

823
00:30:58,240 --> 00:31:00,000
uh question why can't we include the

824
00:31:00,000 --> 00:31:01,440
registers in the trap frame for the

825
00:31:01,440 --> 00:31:02,640
process

826
00:31:02,640 --> 00:31:06,799
that is uh you know actually the

827
00:31:06,799 --> 00:31:08,399
those registers could be stored in the

828
00:31:08,399 --> 00:31:10,159
trap frame which is made because there's

829
00:31:10,159 --> 00:31:12,320
only one saved set of

830
00:31:12,320 --> 00:31:15,039
kernel thread registers per process we

831
00:31:15,039 --> 00:31:17,200
could save them in any data structure

832
00:31:17,200 --> 00:31:19,440
for which there's one you know element

833
00:31:19,440 --> 00:31:21,440
of instance of that data structure per

834
00:31:21,440 --> 00:31:22,320
process

835
00:31:22,320 --> 00:31:24,320
now there's one struct proc process

836
00:31:24,320 --> 00:31:25,760
there's one struct trap frame for

837
00:31:25,760 --> 00:31:26,559
process

838
00:31:26,559 --> 00:31:28,080
we could store the registers in the trap

839
00:31:28,080 --> 00:31:31,190
frame

840
00:31:31,200 --> 00:31:34,000
but i mean just sort of for maybe

841
00:31:34,000 --> 00:31:36,080
simplicity or clarity of code the trap

842
00:31:36,080 --> 00:31:36,559
frame

843
00:31:36,559 --> 00:31:39,679
i think entirely consists of uh

844
00:31:39,679 --> 00:31:41,440
data that's needed when entering and

845
00:31:41,440 --> 00:31:43,039
leaving the kernel

846
00:31:43,039 --> 00:31:46,240
and the struct context is consists of

847
00:31:46,240 --> 00:31:47,440
the stuff that needs to be saved and

848
00:31:47,440 --> 00:31:48,240
restored when

849
00:31:48,240 --> 00:31:50,799
switching to and from between the kernel

850
00:31:50,799 --> 00:31:51,600
thread and the

851
00:31:51,600 --> 00:31:55,519
scheduler thread okay question is yield

852
00:31:55,519 --> 00:31:56,799
something that's called by the user or

853
00:31:56,799 --> 00:31:59,120
the kernel it's called by the kernel

854
00:31:59,120 --> 00:32:01,279
so the user threads there's not really a

855
00:32:01,279 --> 00:32:04,000
direct way in xv6 for user threads to

856
00:32:04,000 --> 00:32:07,360
talk about um yielding the cpu or

857
00:32:07,360 --> 00:32:08,480
switching

858
00:32:08,480 --> 00:32:11,200
uh it's done by the kernel kind of

859
00:32:11,200 --> 00:32:13,120
transparently

860
00:32:13,120 --> 00:32:14,480
you know at points in time when the

861
00:32:14,480 --> 00:32:16,720
kernel feels that it needs to happen

862
00:32:16,720 --> 00:32:18,960
if there are threads um there are some

863
00:32:18,960 --> 00:32:21,279
times when

864
00:32:21,279 --> 00:32:23,760
you can sort of guess that probably a

865
00:32:23,760 --> 00:32:25,200
certain system call

866
00:32:25,200 --> 00:32:28,640
will result in a yield like if a process

867
00:32:28,640 --> 00:32:30,799
does a read on a pipe where it knows

868
00:32:30,799 --> 00:32:32,240
that really nothing is waiting to be

869
00:32:32,240 --> 00:32:33,919
read on the pipe

870
00:32:33,919 --> 00:32:36,559
then the reed will block you can predict

871
00:32:36,559 --> 00:32:37,919
the read will block and that the

872
00:32:37,919 --> 00:32:40,799
kernel will run some other process while

873
00:32:40,799 --> 00:32:41,360
we're

874
00:32:41,360 --> 00:32:45,350
waiting for data to appear in the pipe

875
00:32:45,360 --> 00:32:47,200
and so the times when yield is called in

876
00:32:47,200 --> 00:32:48,880
the kernel there's really two main times

877
00:32:48,880 --> 00:32:49,440
one is

878
00:32:49,440 --> 00:32:52,559
if a timer interrupt goes off the kernel

879
00:32:52,559 --> 00:32:54,399
always yields

880
00:32:54,399 --> 00:32:57,600
you know just on on the theory that

881
00:32:57,600 --> 00:33:01,039
uh we should interleave the execution of

882
00:33:01,039 --> 00:33:03,440
of all the process that want to run on

883
00:33:03,440 --> 00:33:05,360
timer interrupt

884
00:33:05,360 --> 00:33:08,240
uh periods so tom renner also always

885
00:33:08,240 --> 00:33:09,360
calls yield

886
00:33:09,360 --> 00:33:11,440
and whenever a process a system calls

887
00:33:11,440 --> 00:33:12,960
waiting for i o

888
00:33:12,960 --> 00:33:14,559
like waiting for you to type the next

889
00:33:14,559 --> 00:33:16,320
keystroke you know does a read of the

890
00:33:16,320 --> 00:33:17,760
console and you haven't typed the key

891
00:33:17,760 --> 00:33:18,559
yet

892
00:33:18,559 --> 00:33:23,120
then the the machinery to wait for io

893
00:33:23,120 --> 00:33:25,360
calls yield it's called from sleep

894
00:33:25,360 --> 00:33:29,190
something we'll talk about next week

895
00:33:29,200 --> 00:33:32,480
all right um

896
00:33:32,480 --> 00:33:35,760
okay another question yes

897
00:33:35,760 --> 00:33:38,720
oh if it is asleep is it gonna do the

898
00:33:38,720 --> 00:33:39,919
same thing roughly

899
00:33:39,919 --> 00:33:42,720
so it's gonna be some um system call and

900
00:33:42,720 --> 00:33:44,799
then it's gonna save the traffic

901
00:33:44,799 --> 00:33:47,840
and um then basically the same picture

902
00:33:47,840 --> 00:33:48,559
but

903
00:33:48,559 --> 00:33:52,240
it's just um that the thing that

904
00:33:52,240 --> 00:33:54,720
made the process go into the kernel was

905
00:33:54,720 --> 00:33:56,880
not a timer interrupt but

906
00:33:56,880 --> 00:34:00,480
um the processes own decision

907
00:34:00,480 --> 00:34:02,799
yeah so if the process make does a read

908
00:34:02,799 --> 00:34:04,159
system call and that's why it's in the

909
00:34:04,159 --> 00:34:05,360
kernel

910
00:34:05,360 --> 00:34:08,079
and the read requires the process to

911
00:34:08,079 --> 00:34:09,919
wait for the disk

912
00:34:09,919 --> 00:34:11,839
to do to finish reading or to wait for

913
00:34:11,839 --> 00:34:13,280
data to appear on a pipe

914
00:34:13,280 --> 00:34:15,280
then actually the diagram's exactly the

915
00:34:15,280 --> 00:34:16,399
same as this

916
00:34:16,399 --> 00:34:19,839
the enter the kernel with a system call

917
00:34:19,839 --> 00:34:20,800
a trap frame

918
00:34:20,800 --> 00:34:22,159
hold the save user registers will

919
00:34:22,159 --> 00:34:23,520
execute the system called the simple

920
00:34:23,520 --> 00:34:24,399
realize ah

921
00:34:24,399 --> 00:34:25,679
i need to wait for the disk to finish

922
00:34:25,679 --> 00:34:27,440
reading something

923
00:34:27,440 --> 00:34:29,440
the system call code will call sleep

924
00:34:29,440 --> 00:34:32,079
which ends up calling switch

925
00:34:32,079 --> 00:34:35,760
which you know saves away the kernel

926
00:34:35,760 --> 00:34:37,520
thread registers and the processes

927
00:34:37,520 --> 00:34:40,079
contacts and switches to this

928
00:34:40,079 --> 00:34:41,760
current cpu scheduler to let some other

929
00:34:41,760 --> 00:34:44,000
thread run while this thread

930
00:34:44,000 --> 00:34:47,119
is waiting for the discrete to finish so

931
00:34:47,119 --> 00:34:48,800
everything we're going to talk about now

932
00:34:48,800 --> 00:34:50,960
except for the timer interrupt

933
00:34:50,960 --> 00:34:54,159
um is pretty much the same if what's

934
00:34:54,159 --> 00:34:55,599
going on is we're in a system call and

935
00:34:55,599 --> 00:34:56,639
the system call

936
00:34:56,639 --> 00:34:58,880
needs to wait for some for i o and give

937
00:34:58,880 --> 00:35:02,320
up the cpu

938
00:35:02,320 --> 00:35:04,160
for the purposes of today's discussion

939
00:35:04,160 --> 00:35:08,950
the two situations are almost identical

940
00:35:08,960 --> 00:35:10,960
okay so the question does each per cpu

941
00:35:10,960 --> 00:35:12,240
scheduler have its own stack

942
00:35:12,240 --> 00:35:18,470
yes there's a stack

943
00:35:18,480 --> 00:35:21,359
for this scheduler and a stack for this

944
00:35:21,359 --> 00:35:24,310
separate stack

945
00:35:24,320 --> 00:35:31,839
for the scheduler for cpu one

946
00:35:31,839 --> 00:35:34,000
yeah and indeed the stacks for the skies

947
00:35:34,000 --> 00:35:36,400
are just set up

948
00:35:36,400 --> 00:35:38,480
in fact all this stuff you know the

949
00:35:38,480 --> 00:35:41,280
context and the stacks for the scheduler

950
00:35:41,280 --> 00:35:43,440
threads are set up in a different way

951
00:35:43,440 --> 00:35:46,480
than for user processes

952
00:35:46,480 --> 00:35:48,960
they're set up at boot time if you poke

953
00:35:48,960 --> 00:35:50,320
around in

954
00:35:50,320 --> 00:35:54,160
start.s or start.c start.s probably

955
00:35:54,160 --> 00:35:55,920
you'll see some of the setup

956
00:35:55,920 --> 00:35:58,960
for each core's scheduler thread

957
00:35:58,960 --> 00:36:01,599
there's a place with a stack very early

958
00:36:01,599 --> 00:36:03,280
in the assembly code during boot

959
00:36:03,280 --> 00:36:06,160
where the stack is set up for each cpu

960
00:36:06,160 --> 00:36:07,839
and it's on that stack that this

961
00:36:07,839 --> 00:36:10,560
the cpu boots on and then runs its

962
00:36:10,560 --> 00:36:15,670
scheduler thread

963
00:36:15,680 --> 00:36:19,190
okay

964
00:36:19,200 --> 00:36:21,680
one piece of jargon when people talk

965
00:36:21,680 --> 00:36:22,800
about context

966
00:36:22,800 --> 00:36:26,480
switch they're talking about

967
00:36:26,480 --> 00:36:30,800
usually um

968
00:36:30,800 --> 00:36:33,680
this act of switching from one thread to

969
00:36:33,680 --> 00:36:35,680
another by saving one set of register

970
00:36:35,680 --> 00:36:37,200
sets for the old thread

971
00:36:37,200 --> 00:36:39,359
and restoring previously saved registers

972
00:36:39,359 --> 00:36:41,440
for the thread we're switching to

973
00:36:41,440 --> 00:36:42,560
so that's what's usually meant by

974
00:36:42,560 --> 00:36:44,240
context switch also though sometimes

975
00:36:44,240 --> 00:36:47,280
it's applied to the complete

976
00:36:47,280 --> 00:36:48,720
dance that goes on when switching from

977
00:36:48,720 --> 00:36:50,320
one user process to another and

978
00:36:50,320 --> 00:36:51,920
occasionally you'll see context switch

979
00:36:51,920 --> 00:36:53,040
apply to

980
00:36:53,040 --> 00:36:55,359
switching between user and kernel but

981
00:36:55,359 --> 00:36:57,440
for us we mostly mean it

982
00:36:57,440 --> 00:37:01,200
for switching from one kernel thread

983
00:37:01,200 --> 00:37:05,990
typically to a scheduler thread

984
00:37:06,000 --> 00:37:10,790
just some pieces of information um

985
00:37:10,800 --> 00:37:14,400
that are handy to keep in mind um

986
00:37:14,400 --> 00:37:16,560
every core just does one thing at a time

987
00:37:16,560 --> 00:37:17,760
each core

988
00:37:17,760 --> 00:37:20,079
you know is either is just running one

989
00:37:20,079 --> 00:37:21,760
thread at any given time it's either

990
00:37:21,760 --> 00:37:22,320
running

991
00:37:22,320 --> 00:37:24,960
some processes user thread some process

992
00:37:24,960 --> 00:37:26,079
kernel thread

993
00:37:26,079 --> 00:37:28,560
or that core scheduler thread so at any

994
00:37:28,560 --> 00:37:29,680
given time the core

995
00:37:29,680 --> 00:37:31,040
is not doing multiple things it's just

996
00:37:31,040 --> 00:37:33,040
doing one thing and it's this switching

997
00:37:33,040 --> 00:37:35,520
that sort of creates the illusion of

998
00:37:35,520 --> 00:37:37,760
multiple threads running at different

999
00:37:37,760 --> 00:37:40,000
times on that core

1000
00:37:40,000 --> 00:37:44,560
similarly each thread

1001
00:37:44,560 --> 00:37:48,800
is running up it's either running on

1002
00:37:48,800 --> 00:37:52,720
exactly one core or its state has been

1003
00:37:52,720 --> 00:37:54,720
state has been saved and we've switched

1004
00:37:54,720 --> 00:37:56,320
away from it

1005
00:37:56,320 --> 00:37:58,480
so so you know thread just to be clear a

1006
00:37:58,480 --> 00:38:00,240
thread never runs on more than one core

1007
00:38:00,240 --> 00:38:01,680
thread is either running on just one

1008
00:38:01,680 --> 00:38:03,920
core or it's not running at all it has

1009
00:38:03,920 --> 00:38:07,670
it has saved state somewhere

1010
00:38:07,680 --> 00:38:09,599
another interesting thing about the xv6

1011
00:38:09,599 --> 00:38:10,800
setup is that

1012
00:38:10,800 --> 00:38:14,720
these contexts that hold saved

1013
00:38:14,720 --> 00:38:18,000
kernel thread registers they're always

1014
00:38:18,000 --> 00:38:21,440
produced by a call to switch

1015
00:38:21,440 --> 00:38:23,839
and so these contacts basically always

1016
00:38:23,839 --> 00:38:24,560
refer

1017
00:38:24,560 --> 00:38:27,599
to you know the state of the thread as

1018
00:38:27,599 --> 00:38:30,880
it was executing inside a call to switch

1019
00:38:30,880 --> 00:38:34,880
um and you know the way we'll see that

1020
00:38:34,880 --> 00:38:35,359
come up

1021
00:38:35,359 --> 00:38:38,240
is that when we uh switch from one to

1022
00:38:38,240 --> 00:38:39,359
another and restore

1023
00:38:39,359 --> 00:38:41,520
the target threads context the first

1024
00:38:41,520 --> 00:38:43,359
thing it will do is return from a

1025
00:38:43,359 --> 00:38:45,200
previous call to switch

1026
00:38:45,200 --> 00:38:47,119
so these contacts sort of always save

1027
00:38:47,119 --> 00:38:48,240
state

1028
00:38:48,240 --> 00:38:52,470
in as it is in switch

1029
00:38:52,480 --> 00:38:56,000
okay um

1030
00:38:56,000 --> 00:38:58,640
any more questions about the sort of

1031
00:38:58,640 --> 00:38:59,280
diagram

1032
00:38:59,280 --> 00:39:05,190
level situation

1033
00:39:05,200 --> 00:39:07,599
um i have a question so we're using the

1034
00:39:07,599 --> 00:39:09,119
term thread all the time but

1035
00:39:09,119 --> 00:39:11,119
it seems to me like our implementation

1036
00:39:11,119 --> 00:39:13,119
for xv6

1037
00:39:13,119 --> 00:39:16,160
process is it is only one thread so like

1038
00:39:16,160 --> 00:39:17,520
could it be possible that

1039
00:39:17,520 --> 00:39:19,440
one process could have multiple threads

1040
00:39:19,440 --> 00:39:21,680
or am i wrong here

1041
00:39:21,680 --> 00:39:25,920
in xv6 right now

1042
00:39:25,920 --> 00:39:28,240
there's definitely some confusing things

1043
00:39:28,240 --> 00:39:29,839
about the way we use the words here

1044
00:39:29,839 --> 00:39:32,960
in xv6 um

1045
00:39:32,960 --> 00:39:39,510
a process

1046
00:39:39,520 --> 00:39:41,599
a process is either executing

1047
00:39:41,599 --> 00:39:43,040
instructions

1048
00:39:43,040 --> 00:39:46,320
a user level or it's executing

1049
00:39:46,320 --> 00:39:48,079
instructions

1050
00:39:48,079 --> 00:39:51,599
in the kernel or

1051
00:39:51,599 --> 00:39:53,440
it's not executing at all and its state

1052
00:39:53,440 --> 00:39:54,640
has been saved away

1053
00:39:54,640 --> 00:39:58,079
into this combination of a context

1054
00:39:58,079 --> 00:40:02,390
and a trap frame

1055
00:40:02,400 --> 00:40:04,240
so that's the actual situation now what

1056
00:40:04,240 --> 00:40:08,390
you want to call that

1057
00:40:08,400 --> 00:40:11,280
well you can call what what you like i i

1058
00:40:11,280 --> 00:40:12,880
don't know of a simple explanation for

1059
00:40:12,880 --> 00:40:13,200
this

1060
00:40:13,200 --> 00:40:15,599
structure um we've been calling it i've

1061
00:40:15,599 --> 00:40:16,720
been calling it

1062
00:40:16,720 --> 00:40:19,440
i've been saying that each process has

1063
00:40:19,440 --> 00:40:21,040
two threads

1064
00:40:21,040 --> 00:40:24,319
a user level thread and a kernel level

1065
00:40:24,319 --> 00:40:25,040
thread and

1066
00:40:25,040 --> 00:40:26,480
and that's a process there's this

1067
00:40:26,480 --> 00:40:28,640
restriction that a process is only

1068
00:40:28,640 --> 00:40:30,720
is either executing in the kernel in the

1069
00:40:30,720 --> 00:40:32,160
user space

1070
00:40:32,160 --> 00:40:33,839
or executing in the kernel in an

1071
00:40:33,839 --> 00:40:35,280
interrupt your system call

1072
00:40:35,280 --> 00:40:38,560
but never both yeah

1073
00:40:38,560 --> 00:40:42,000
okay that makes sense yeah i apologize

1074
00:40:42,000 --> 00:40:47,510
for the kind of complexity of this

1075
00:40:47,520 --> 00:40:50,560
okay okay so let me switch to code

1076
00:40:50,560 --> 00:41:02,390
looking at the xv6 code

1077
00:41:02,400 --> 00:41:10,069
right so first of all

1078
00:41:10,079 --> 00:41:13,440
um i just want to just to

1079
00:41:13,440 --> 00:41:14,720
show some of the stuff we've been

1080
00:41:14,720 --> 00:41:19,270
talking about i'm going to look at the

1081
00:41:19,280 --> 00:41:21,760
process structure and we can see in the

1082
00:41:21,760 --> 00:41:22,960
process structure a lot of the things

1083
00:41:22,960 --> 00:41:24,400
we've been talking about

1084
00:41:24,400 --> 00:41:30,710
um just for review there's the

1085
00:41:30,720 --> 00:41:34,000
wrap frame that saves the user level

1086
00:41:34,000 --> 00:41:37,599
registers there's

1087
00:41:37,599 --> 00:41:42,480
a context here that saves the

1088
00:41:42,480 --> 00:41:44,400
kernel thread registers when we switch

1089
00:41:44,400 --> 00:41:46,560
to the scheduler thread

1090
00:41:46,560 --> 00:41:50,400
there's a pointer to this process is

1091
00:41:50,400 --> 00:41:52,480
kernel stack which is where you know

1092
00:41:52,480 --> 00:41:53,920
function calls are saved

1093
00:41:53,920 --> 00:41:57,200
while we're executing in the kernel

1094
00:41:57,200 --> 00:41:59,599
there's this state variable which

1095
00:41:59,599 --> 00:42:02,319
records whether this process is

1096
00:42:02,319 --> 00:42:04,640
running or runnable or sleeping or not

1097
00:42:04,640 --> 00:42:07,200
allocated at all

1098
00:42:07,200 --> 00:42:10,720
and then finally there's a lock that

1099
00:42:10,720 --> 00:42:15,589
protects various things as we'll see

1100
00:42:15,599 --> 00:42:18,720
for now it we can observe that it at

1101
00:42:18,720 --> 00:42:20,319
least protects

1102
00:42:20,319 --> 00:42:23,440
uh changes to the state variable um

1103
00:42:23,440 --> 00:42:26,000
so that for example uh two scheduler

1104
00:42:26,000 --> 00:42:27,440
threats don't try to grab

1105
00:42:27,440 --> 00:42:29,119
a runnable process and run it at the

1106
00:42:29,119 --> 00:42:31,359
same time one of the many things this

1107
00:42:31,359 --> 00:42:35,270
lock does is prevent that from happening

1108
00:42:35,280 --> 00:42:38,560
i'm going to run a simple demo program

1109
00:42:38,560 --> 00:42:39,280
for you

1110
00:42:39,280 --> 00:42:43,119
this spin program i'm using it mostly

1111
00:42:43,119 --> 00:42:43,440
just

1112
00:42:43,440 --> 00:42:45,760
to drive the to sort of create this

1113
00:42:45,760 --> 00:42:48,160
issue predictable situation in which

1114
00:42:48,160 --> 00:42:50,240
we switch from one thread to another but

1115
00:42:50,240 --> 00:42:51,520
this is

1116
00:42:51,520 --> 00:42:54,240
this program spin program creates two

1117
00:42:54,240 --> 00:42:56,240
processes and the processes both compute

1118
00:42:56,240 --> 00:42:57,200
forever

1119
00:42:57,200 --> 00:43:00,240
you know call fork here

1120
00:43:00,240 --> 00:43:03,359
i make a child um and then

1121
00:43:03,359 --> 00:43:06,480
forever both children both children just

1122
00:43:06,480 --> 00:43:07,839
sit in this loop and every once in a

1123
00:43:07,839 --> 00:43:09,200
while they'll print a character just so

1124
00:43:09,200 --> 00:43:11,680
we can see they're making progress

1125
00:43:11,680 --> 00:43:12,880
but they don't print characters very

1126
00:43:12,880 --> 00:43:15,359
often and they never

1127
00:43:15,359 --> 00:43:17,760
sort of intentionally give up the cpu so

1128
00:43:17,760 --> 00:43:19,680
what we have here is two

1129
00:43:19,680 --> 00:43:21,839
essentially two compute bound processes

1130
00:43:21,839 --> 00:43:23,440
and in order for both of them to run i'm

1131
00:43:23,440 --> 00:43:24,480
going to run them on a

1132
00:43:24,480 --> 00:43:27,599
single cpu

1133
00:43:27,599 --> 00:43:30,079
xv6 that is only one core and so in

1134
00:43:30,079 --> 00:43:31,680
order for both of them to execute

1135
00:43:31,680 --> 00:43:35,040
you know it's going to be necessary to

1136
00:43:35,040 --> 00:43:40,400
do switching between the two processes

1137
00:43:40,400 --> 00:43:44,560
let me fire up the spin program

1138
00:43:44,560 --> 00:43:51,030
under gdb

1139
00:43:51,040 --> 00:43:52,319
run the spin program and you can see

1140
00:43:52,319 --> 00:43:54,480
it's printing one of the

1141
00:43:54,480 --> 00:43:56,720
two processes prints forward slash and

1142
00:43:56,720 --> 00:43:57,520
the other prints

1143
00:43:57,520 --> 00:43:59,359
backwards slash and you can see that

1144
00:43:59,359 --> 00:44:01,359
every once in a while

1145
00:44:01,359 --> 00:44:03,359
xp6 is switching between them it only

1146
00:44:03,359 --> 00:44:05,599
has one core the way i've configured it

1147
00:44:05,599 --> 00:44:08,480
so it we see a bunch of forward slashes

1148
00:44:08,480 --> 00:44:09,680
printing and then

1149
00:44:09,680 --> 00:44:12,079
apparently a timer in or up most go off

1150
00:44:12,079 --> 00:44:13,119
um

1151
00:44:13,119 --> 00:44:15,359
switch the one cpu to the other process

1152
00:44:15,359 --> 00:44:17,040
and then uh prints the other kind of

1153
00:44:17,040 --> 00:44:17,920
slash for a while

1154
00:44:17,920 --> 00:44:19,760
so what i want to observe is the timer

1155
00:44:19,760 --> 00:44:21,280
interrupt going off so i'm going to put

1156
00:44:21,280 --> 00:44:23,599
a break point

1157
00:44:23,599 --> 00:44:26,870
in trap

1158
00:44:26,880 --> 00:44:35,190
and in particular at line 207 and trap

1159
00:44:35,200 --> 00:44:39,040
uh which is um the code

1160
00:44:39,040 --> 00:44:42,800
in um

1161
00:44:42,800 --> 00:44:46,560
trap in dev inter that um

1162
00:44:46,560 --> 00:44:48,640
recognizes that ah we are in an

1163
00:44:48,640 --> 00:44:50,400
interrupt and the interrupt was

1164
00:44:50,400 --> 00:44:54,309
caused by a timer interrupt

1165
00:44:54,319 --> 00:44:55,920
so i'm going to put a break point here

1166
00:44:55,920 --> 00:44:57,520
at

1167
00:44:57,520 --> 00:45:00,560
trap.c 0.27 and

1168
00:45:00,560 --> 00:45:03,599
continue um boom the height triggers

1169
00:45:03,599 --> 00:45:04,079
right away

1170
00:45:04,079 --> 00:45:06,160
because timer amps are pretty frequent

1171
00:45:06,160 --> 00:45:07,760
and we can tell from where that indeed

1172
00:45:07,760 --> 00:45:09,440
we're in user trap and user trap

1173
00:45:09,440 --> 00:45:12,319
has called dev enter to handle this

1174
00:45:12,319 --> 00:45:13,359
interrupt

1175
00:45:13,359 --> 00:45:16,480
um i want to type finish to get out of

1176
00:45:16,480 --> 00:45:18,960
dev internet back into user trap

1177
00:45:18,960 --> 00:45:22,640
um because in fact we don't that

1178
00:45:22,640 --> 00:45:24,480
the code and different of our timer is

1179
00:45:24,480 --> 00:45:27,349
almost nothing

1180
00:45:27,359 --> 00:45:30,839
however once we're back at um

1181
00:45:30,839 --> 00:45:36,400
in uh user trap

1182
00:45:36,400 --> 00:45:40,160
we can see that from this line here that

1183
00:45:40,160 --> 00:45:47,839
we just returned from dev enter um

1184
00:45:47,839 --> 00:45:51,599
and the interesting thing about this

1185
00:45:51,599 --> 00:45:56,400
is that um what we're about to do

1186
00:45:56,400 --> 00:45:58,000
i mean looking forward we're currently

1187
00:45:58,000 --> 00:45:59,760
at this line here and

1188
00:45:59,760 --> 00:46:02,240
we're looking forward to this called the

1189
00:46:02,240 --> 00:46:03,680
yield

1190
00:46:03,680 --> 00:46:06,560
when dividend to return to you can see

1191
00:46:06,560 --> 00:46:07,520
from this

1192
00:46:07,520 --> 00:46:09,839
value return is two that two is

1193
00:46:09,839 --> 00:46:11,280
basically the device number

1194
00:46:11,280 --> 00:46:14,240
and we're going to see that by and by

1195
00:46:14,240 --> 00:46:17,280
because which devices too

1196
00:46:17,280 --> 00:46:19,040
user perhaps going to call yield which

1197
00:46:19,040 --> 00:46:20,640
will give up the cpu

1198
00:46:20,640 --> 00:46:22,720
and allow switch another process so

1199
00:46:22,720 --> 00:46:23,920
we'll see that in a moment

1200
00:46:23,920 --> 00:46:25,680
meantime let's look at what was

1201
00:46:25,680 --> 00:46:27,520
currently executing when the interrupt

1202
00:46:27,520 --> 00:46:28,400
happened

1203
00:46:28,400 --> 00:46:32,000
so i'm going to print p the variable

1204
00:46:32,000 --> 00:46:34,640
p holds a pointer to the current process

1205
00:46:34,640 --> 00:46:35,359
is struct

1206
00:46:35,359 --> 00:46:38,800
proc um

1207
00:46:38,800 --> 00:46:40,240
okay we have a question what makes each

1208
00:46:40,240 --> 00:46:42,160
process's kernel thread

1209
00:46:42,160 --> 00:46:45,599
different every process has a

1210
00:46:45,599 --> 00:46:48,640
separate kernel thread so there's really

1211
00:46:48,640 --> 00:46:50,000
two things that

1212
00:46:50,000 --> 00:46:52,160
differentiate different processes kernel

1213
00:46:52,160 --> 00:46:53,280
thread because more than one could be

1214
00:46:53,280 --> 00:46:54,720
executing on

1215
00:46:54,720 --> 00:46:58,960
different cores one is indeed that every

1216
00:46:58,960 --> 00:46:59,760
process

1217
00:46:59,760 --> 00:47:02,160
has a separate kernel stack and that's

1218
00:47:02,160 --> 00:47:03,839
what's pointed to by that

1219
00:47:03,839 --> 00:47:06,800
k stack element of struck proc and the

1220
00:47:06,800 --> 00:47:07,119
other

1221
00:47:07,119 --> 00:47:13,430
is that

1222
00:47:13,440 --> 00:47:16,480
uh early in

1223
00:47:16,480 --> 00:47:18,160
we're in user trap which is you know the

1224
00:47:18,160 --> 00:47:20,559
c code that's called

1225
00:47:20,559 --> 00:47:25,589
by trampoline when an interrupt occurs

1226
00:47:25,599 --> 00:47:28,640
we can tell by this call that my any

1227
00:47:28,640 --> 00:47:30,800
any kernel code can tell by calling

1228
00:47:30,800 --> 00:47:31,920
myproc

1229
00:47:31,920 --> 00:47:34,640
what the process is that's running on

1230
00:47:34,640 --> 00:47:36,559
the current cpu

1231
00:47:36,559 --> 00:47:37,520
and that's another thing that

1232
00:47:37,520 --> 00:47:40,400
differentiates um

1233
00:47:40,400 --> 00:47:42,960
that allows each cr that allows kernel

1234
00:47:42,960 --> 00:47:45,200
code to tell what process it's part of

1235
00:47:45,200 --> 00:47:45,760
that is

1236
00:47:45,760 --> 00:47:47,280
which processes kernel thread is

1237
00:47:47,280 --> 00:47:49,040
executing and what myproc does is

1238
00:47:49,040 --> 00:47:50,880
basically use the tp register which you

1239
00:47:50,880 --> 00:47:53,040
may recall

1240
00:47:53,040 --> 00:47:56,640
is set up to contain the current core's

1241
00:47:56,640 --> 00:47:59,119
heart id or core number it uses that to

1242
00:47:59,119 --> 00:48:01,599
index into an array of structures that

1243
00:48:01,599 --> 00:48:03,680
say for each core that the scheduler

1244
00:48:03,680 --> 00:48:06,480
sets whenever it switches processes to

1245
00:48:06,480 --> 00:48:08,319
indicate for each core which process is

1246
00:48:08,319 --> 00:48:11,119
running on that core

1247
00:48:11,119 --> 00:48:12,640
and so that's how different kernel

1248
00:48:12,640 --> 00:48:15,680
threads are differentiated

1249
00:48:15,680 --> 00:48:18,880
okay so i was going to use that p value

1250
00:48:18,880 --> 00:48:20,559
the name and that p-value to figure out

1251
00:48:20,559 --> 00:48:21,920
what process is running

1252
00:48:21,920 --> 00:48:23,920
xv6 remembers the name it's that spin

1253
00:48:23,920 --> 00:48:25,200
process just

1254
00:48:25,200 --> 00:48:28,160
exactly as expected there were two of

1255
00:48:28,160 --> 00:48:29,040
them

1256
00:48:29,040 --> 00:48:32,839
i think with process ids three and four

1257
00:48:32,839 --> 00:48:36,000
oops we're currently executing again

1258
00:48:36,000 --> 00:48:37,359
process id3

1259
00:48:37,359 --> 00:48:40,079
so after the switch we'd expect to be in

1260
00:48:40,079 --> 00:48:41,359
process id for

1261
00:48:41,359 --> 00:48:44,960
the other spin process how can we can

1262
00:48:44,960 --> 00:48:46,240
look at the saved

1263
00:48:46,240 --> 00:48:54,549
user registers in the trap frame

1264
00:48:54,559 --> 00:48:57,680
and these are just the 32 registers that

1265
00:48:57,680 --> 00:49:01,040
the trampoline code saves away to save

1266
00:49:01,040 --> 00:49:02,160
the user state

1267
00:49:02,160 --> 00:49:05,440
there's the user ra return address

1268
00:49:05,440 --> 00:49:08,480
register user stack pointer

1269
00:49:08,480 --> 00:49:12,240
user program counter at hex 62.

1270
00:49:12,240 --> 00:49:14,160
these are all familiar things from when

1271
00:49:14,160 --> 00:49:15,760
we looked at

1272
00:49:15,760 --> 00:49:18,480
traps and you know maybe of the most

1273
00:49:18,480 --> 00:49:19,040
interest

1274
00:49:19,040 --> 00:49:23,190
is that

1275
00:49:23,200 --> 00:49:25,359
the trap frame saves the user program

1276
00:49:25,359 --> 00:49:28,000
counter it's at value 62

1277
00:49:28,000 --> 00:49:31,599
if we cared

1278
00:49:31,599 --> 00:49:34,400
we can look in the assembly code for

1279
00:49:34,400 --> 00:49:37,829
spin.c

1280
00:49:37,839 --> 00:49:41,119
just spin.asm and look for 62.

1281
00:49:41,119 --> 00:49:43,839
now we can see that oh the interrupt

1282
00:49:43,839 --> 00:49:45,280
time or an interrupt occurred during

1283
00:49:45,280 --> 00:49:45,920
this ad

1284
00:49:45,920 --> 00:49:49,119
instruction in that infinite loop

1285
00:49:49,119 --> 00:49:55,510
in spin so it's not too surprising

1286
00:49:55,520 --> 00:49:58,960
okay so back to

1287
00:49:58,960 --> 00:50:00,880
the trap code they've entered just

1288
00:50:00,880 --> 00:50:03,119
returned i'm going to type

1289
00:50:03,119 --> 00:50:09,030
step a few times to get us to the

1290
00:50:09,040 --> 00:50:11,040
to just being about to execute this

1291
00:50:11,040 --> 00:50:12,319
yield

1292
00:50:12,319 --> 00:50:13,760
and yield is sort of the first step in

1293
00:50:13,760 --> 00:50:16,000
the process of giving up the cpu

1294
00:50:16,000 --> 00:50:17,440
switching to the scheduler letting the

1295
00:50:17,440 --> 00:50:18,000
scheduler

1296
00:50:18,000 --> 00:50:20,240
choose another kernel thread and process

1297
00:50:20,240 --> 00:50:24,309
to run

1298
00:50:24,319 --> 00:50:26,240
all right so let's actually step into

1299
00:50:26,240 --> 00:50:27,359
yield

1300
00:50:27,359 --> 00:50:30,319
now we're in yield yields um if you have

1301
00:50:30,319 --> 00:50:31,520
a question

1302
00:50:31,520 --> 00:50:38,829
no

1303
00:50:38,839 --> 00:50:43,589
oops

1304
00:50:43,599 --> 00:50:47,520
okay we're in yield um

1305
00:50:47,520 --> 00:50:51,119
yield does just a couple of things it uh

1306
00:50:51,119 --> 00:50:54,400
acquires the lock for this process

1307
00:50:54,400 --> 00:50:55,680
because it's about to make a bunch of

1308
00:50:55,680 --> 00:50:57,440
changes to this process and

1309
00:50:57,440 --> 00:51:00,400
it doesn't want any other and in fact

1310
00:51:00,400 --> 00:51:01,920
until it gives up the lock the state of

1311
00:51:01,920 --> 00:51:03,520
this process will be sort of

1312
00:51:03,520 --> 00:51:06,160
inconsistent um like for example it's

1313
00:51:06,160 --> 00:51:08,319
about yield is about to change the state

1314
00:51:08,319 --> 00:51:10,480
of the process to runnable which

1315
00:51:10,480 --> 00:51:12,480
would you know indicates that the

1316
00:51:12,480 --> 00:51:15,040
process is not running but would like to

1317
00:51:15,040 --> 00:51:17,839
but this process is running right i mean

1318
00:51:17,839 --> 00:51:19,119
we're running the process right now

1319
00:51:19,119 --> 00:51:20,559
that's what's executing is the kernel

1320
00:51:20,559 --> 00:51:22,000
threat for this process

1321
00:51:22,000 --> 00:51:23,839
and so the one of the many things that

1322
00:51:23,839 --> 00:51:25,119
acquire in this lock does

1323
00:51:25,119 --> 00:51:27,680
is uh makes it so that even though we

1324
00:51:27,680 --> 00:51:29,280
just changed the state to runnable

1325
00:51:29,280 --> 00:51:32,079
no other core scheduling thread will

1326
00:51:32,079 --> 00:51:33,680
look at this process

1327
00:51:33,680 --> 00:51:36,640
and because of the lock and see that

1328
00:51:36,640 --> 00:51:38,319
it's runnable and try to run it

1329
00:51:38,319 --> 00:51:39,680
while we're still running it on this

1330
00:51:39,680 --> 00:51:41,119
core which would be a disaster right

1331
00:51:41,119 --> 00:51:43,119
running the same process

1332
00:51:43,119 --> 00:51:45,040
um on two different cores and that

1333
00:51:45,040 --> 00:51:47,119
process has only one stack

1334
00:51:47,119 --> 00:51:48,880
so that means like two different cores

1335
00:51:48,880 --> 00:51:50,400
are you know calling subroutines on the

1336
00:51:50,400 --> 00:51:51,599
same stack which is

1337
00:51:51,599 --> 00:51:55,520
um just a recipe for disaster

1338
00:51:55,520 --> 00:51:59,430
so we take the lock out

1339
00:51:59,440 --> 00:52:02,839
we yield changes the state to runable

1340
00:52:02,839 --> 00:52:06,240
um and what this means is that you know

1341
00:52:06,240 --> 00:52:09,440
when we finally given up the

1342
00:52:09,440 --> 00:52:12,960
um when we finally yield

1343
00:52:12,960 --> 00:52:14,559
the cpu and give it up and switch to the

1344
00:52:14,559 --> 00:52:16,079
scheduler process this state will be

1345
00:52:16,079 --> 00:52:17,680
left in this runnable state so that it

1346
00:52:17,680 --> 00:52:18,880
will run again

1347
00:52:18,880 --> 00:52:20,319
because after all this was a timer

1348
00:52:20,319 --> 00:52:22,000
interrupt that interrupted a running

1349
00:52:22,000 --> 00:52:23,520
user level process that would like to

1350
00:52:23,520 --> 00:52:24,720
continue computing

1351
00:52:24,720 --> 00:52:26,720
so we're going to leave it in state

1352
00:52:26,720 --> 00:52:28,079
runnable so that it will run again

1353
00:52:28,079 --> 00:52:34,549
as soon as the scheduler decides to

1354
00:52:34,559 --> 00:52:44,390
and then the only other thing the

1355
00:52:44,400 --> 00:52:46,079
yield does is call this scheduler

1356
00:52:46,079 --> 00:52:48,400
function

1357
00:52:48,400 --> 00:52:49,760
so i'm going to step into the scheduler

1358
00:52:49,760 --> 00:52:52,160
function i'll show this

1359
00:52:52,160 --> 00:53:01,440
whole thing here

1360
00:53:01,440 --> 00:53:04,240
um this scheduler something does almost

1361
00:53:04,240 --> 00:53:06,079
nothing it does a bunch of checks

1362
00:53:06,079 --> 00:53:08,480
it does a whole bunch of sanity checks

1363
00:53:08,480 --> 00:53:09,280
and panics

1364
00:53:09,280 --> 00:53:13,589
and the reason for that is actually that

1365
00:53:13,599 --> 00:53:16,880
this code in xv6 over its many year

1366
00:53:16,880 --> 00:53:18,480
lifetime has had a

1367
00:53:18,480 --> 00:53:21,280
been among the most bug prone and had

1368
00:53:21,280 --> 00:53:22,960
most surprises

1369
00:53:22,960 --> 00:53:25,440
um on unhappy surprises so there's a lot

1370
00:53:25,440 --> 00:53:27,040
of

1371
00:53:27,040 --> 00:53:30,640
sanity checks and panics here because um

1372
00:53:30,640 --> 00:53:32,000
because there's often been bugs

1373
00:53:32,000 --> 00:53:35,510
associated with this code

1374
00:53:35,520 --> 00:53:38,400
all right i'm gonna um skip over these

1375
00:53:38,400 --> 00:53:39,680
um

1376
00:53:39,680 --> 00:53:46,790
sanity checks and proceed to the

1377
00:53:46,800 --> 00:53:48,559
call to switch this call to switch is

1378
00:53:48,559 --> 00:53:49,760
where the real action happens this is

1379
00:53:49,760 --> 00:53:51,200
called the switch it's going to

1380
00:53:51,200 --> 00:53:54,720
save away the current kernel threads

1381
00:53:54,720 --> 00:53:55,680
registers

1382
00:53:55,680 --> 00:53:58,079
in pro context which is the current

1383
00:53:58,079 --> 00:53:59,119
processes

1384
00:53:59,119 --> 00:54:01,200
saved kernel thread context save set of

1385
00:54:01,200 --> 00:54:03,040
registers

1386
00:54:03,040 --> 00:54:06,559
c arrow context c is the pointer to this

1387
00:54:06,559 --> 00:54:09,680
core's struct cpu

1388
00:54:09,680 --> 00:54:12,800
and struct cpu has the context the save

1389
00:54:12,800 --> 00:54:13,599
registers

1390
00:54:13,599 --> 00:54:16,480
of this core's scheduler thread so we're

1391
00:54:16,480 --> 00:54:18,400
going to be switching from this thread

1392
00:54:18,400 --> 00:54:20,559
saving this red state restoring the

1393
00:54:20,559 --> 00:54:22,559
threat state of

1394
00:54:22,559 --> 00:54:24,800
discord's scheduler and sort of

1395
00:54:24,800 --> 00:54:27,200
continuing the execution of this score's

1396
00:54:27,200 --> 00:54:32,790
course scheduler thread

1397
00:54:32,800 --> 00:54:36,470
okay so let's see what uh

1398
00:54:36,480 --> 00:54:39,760
let's take a quick preview at

1399
00:54:39,760 --> 00:54:41,440
the context that we're going to be

1400
00:54:41,440 --> 00:54:43,200
switching to

1401
00:54:43,200 --> 00:54:45,839
and i can get that turns out that i

1402
00:54:45,839 --> 00:54:47,920
can't actually print cro context but i

1403
00:54:47,920 --> 00:54:49,760
happen to know that c

1404
00:54:49,760 --> 00:54:52,799
prints to cpus zero just

1405
00:54:52,799 --> 00:54:54,400
because we're on the zero with core

1406
00:54:54,400 --> 00:54:55,920
there's only one core

1407
00:54:55,920 --> 00:55:00,950
um and i can print its context

1408
00:55:00,960 --> 00:55:04,480
and so this is the saved

1409
00:55:04,480 --> 00:55:08,000
registers from this course scheduler

1410
00:55:08,000 --> 00:55:10,470
thread

1411
00:55:10,480 --> 00:55:12,480
and of particular interest is the ra

1412
00:55:12,480 --> 00:55:15,040
because the ra register

1413
00:55:15,040 --> 00:55:18,240
is where the current function call

1414
00:55:18,240 --> 00:55:19,599
is going to return to so we're going to

1415
00:55:19,599 --> 00:55:20,880
switch to the scheduler thread and it's

1416
00:55:20,880 --> 00:55:22,640
going to do a return

1417
00:55:22,640 --> 00:55:27,200
and return to that ra and

1418
00:55:27,200 --> 00:55:29,119
we can find out where that are where

1419
00:55:29,119 --> 00:55:32,829
that return address is by looking in

1420
00:55:32,839 --> 00:55:35,119
kernel.asm

1421
00:55:35,119 --> 00:55:44,069
actually that's uh

1422
00:55:44,079 --> 00:55:46,000
and as you can see this x slash i you

1423
00:55:46,000 --> 00:55:47,920
know prints the instructions that are at

1424
00:55:47,920 --> 00:55:49,599
a certain address but it also prints the

1425
00:55:49,599 --> 00:55:50,720
label

1426
00:55:50,720 --> 00:55:53,520
um of the the name of the function that

1427
00:55:53,520 --> 00:55:54,960
those instructions are in it so we're

1428
00:55:54,960 --> 00:55:56,559
going to be returning

1429
00:55:56,559 --> 00:55:59,040
uh to schedule or buy and buy that's

1430
00:55:59,040 --> 00:55:59,760
just

1431
00:55:59,760 --> 00:56:05,030
you know as you might expect

1432
00:56:05,040 --> 00:56:11,030
okay um

1433
00:56:11,040 --> 00:56:13,200
i want to look at what switch actually

1434
00:56:13,200 --> 00:56:18,630
does we're about to call switch

1435
00:56:18,640 --> 00:56:20,160
so i put a breakpoint on switch i'm

1436
00:56:20,160 --> 00:56:21,200
putting a breakpoint because there's a

1437
00:56:21,200 --> 00:56:22,559
bunch of setup code

1438
00:56:22,559 --> 00:56:25,520
like that pulls the values of context

1439
00:56:25,520 --> 00:56:26,559
out of those structures

1440
00:56:26,559 --> 00:56:30,160
i'm going to skip over it okay so now

1441
00:56:30,160 --> 00:56:33,280
um we're to break point and switch

1442
00:56:33,280 --> 00:56:36,160
the gdb won't show us the instructions

1443
00:56:36,160 --> 00:56:38,480
but we can look in switch.s

1444
00:56:38,480 --> 00:56:39,760
um to look at the instructions we're

1445
00:56:39,760 --> 00:56:42,079
about to execute so as you can see we're

1446
00:56:42,079 --> 00:56:43,760
on the very first instruction the store

1447
00:56:43,760 --> 00:56:44,000
of

1448
00:56:44,000 --> 00:56:47,599
ra to the address pointed to by

1449
00:56:47,599 --> 00:56:50,240
a0 you may remember in the call to

1450
00:56:50,240 --> 00:56:50,799
switch

1451
00:56:50,799 --> 00:56:53,359
that the first argument was the current

1452
00:56:53,359 --> 00:56:55,280
thread's context and the second argument

1453
00:56:55,280 --> 00:56:56,480
was the

1454
00:56:56,480 --> 00:56:58,319
context of the thread we're switching to

1455
00:56:58,319 --> 00:57:01,200
the two arguments go in a0 and a1

1456
00:57:01,200 --> 00:57:02,480
and so the reason why we see all these

1457
00:57:02,480 --> 00:57:04,240
stores through

1458
00:57:04,240 --> 00:57:06,000
register a0 is because we're storing

1459
00:57:06,000 --> 00:57:07,920
away a bunch of registers

1460
00:57:07,920 --> 00:57:10,000
in the memory that a0 points to that is

1461
00:57:10,000 --> 00:57:12,319
in the context of the thread we're

1462
00:57:12,319 --> 00:57:13,200
switching from

1463
00:57:13,200 --> 00:57:15,839
and the loads load from address a1

1464
00:57:15,839 --> 00:57:16,480
because

1465
00:57:16,480 --> 00:57:18,480
that's a pointer to the context of the

1466
00:57:18,480 --> 00:57:26,720
thread we're switching to

1467
00:57:26,720 --> 00:57:30,000
okay um and so thread

1468
00:57:30,000 --> 00:57:33,680
you know uh switch saves registers

1469
00:57:33,680 --> 00:57:35,680
loads registers from the target threads

1470
00:57:35,680 --> 00:57:37,760
context and then return

1471
00:57:37,760 --> 00:57:39,440
that's that's why the ra was interesting

1472
00:57:39,440 --> 00:57:40,720
because it's going to return to the

1473
00:57:40,720 --> 00:57:42,480
place that ra pointed to namely into

1474
00:57:42,480 --> 00:57:44,160
scheduler

1475
00:57:44,160 --> 00:57:46,559
all right so one question is you may

1476
00:57:46,559 --> 00:57:47,599
notice here that

1477
00:57:47,599 --> 00:57:50,319
while switch saves rasp and a bunch of s

1478
00:57:50,319 --> 00:57:51,200
registers

1479
00:57:51,200 --> 00:57:53,040
one thing it does not save is the

1480
00:57:53,040 --> 00:57:54,400
program counter

1481
00:57:54,400 --> 00:57:55,520
there's no mention of the program

1482
00:57:55,520 --> 00:57:57,680
counter here

1483
00:57:57,680 --> 00:58:04,950
so why is that

1484
00:58:04,960 --> 00:58:07,839
is it because the program cutter is uh

1485
00:58:07,839 --> 00:58:09,760
updated with like the function calls

1486
00:58:09,760 --> 00:58:11,280
anyway

1487
00:58:11,280 --> 00:58:13,280
yeah it's it's the program counter

1488
00:58:13,280 --> 00:58:15,040
there's no actual information value in

1489
00:58:15,040 --> 00:58:16,480
the program counter we know

1490
00:58:16,480 --> 00:58:18,400
that where we're executing right now is

1491
00:58:18,400 --> 00:58:19,920
in switch

1492
00:58:19,920 --> 00:58:22,000
right so there'll be no point in saving

1493
00:58:22,000 --> 00:58:23,520
the program counter because it has an

1494
00:58:23,520 --> 00:58:26,000
extremely predictable value namely

1495
00:58:26,000 --> 00:58:27,599
this instruction the address of this

1496
00:58:27,599 --> 00:58:30,000
instruction and switch

1497
00:58:30,000 --> 00:58:31,839
what we really care about is where we

1498
00:58:31,839 --> 00:58:33,280
were called from

1499
00:58:33,280 --> 00:58:34,839
because when we switch back to this

1500
00:58:34,839 --> 00:58:37,680
thread we want to continue executing out

1501
00:58:37,680 --> 00:58:39,680
whatever point switch was called from

1502
00:58:39,680 --> 00:58:42,640
and it's ra that holds the address of

1503
00:58:42,640 --> 00:58:44,319
the instruction that switch was called

1504
00:58:44,319 --> 00:58:45,520
from

1505
00:58:45,520 --> 00:58:49,359
so it's ra that's being saved away here

1506
00:58:49,359 --> 00:58:53,910
and our a is the point at which we'll be

1507
00:58:53,920 --> 00:58:56,400
executing out again let's switch returns

1508
00:58:56,400 --> 00:58:59,119
so we can even print that we can print

1509
00:58:59,119 --> 00:59:03,589
our a

1510
00:59:03,599 --> 00:59:05,040
we can print our a and you know we

1511
00:59:05,040 --> 00:59:06,720
haven't actually switched threads yet

1512
00:59:06,720 --> 00:59:08,640
you remember we came here from the sched

1513
00:59:08,640 --> 00:59:09,920
function um

1514
00:59:09,920 --> 00:59:12,160
so our a is as you might expect a

1515
00:59:12,160 --> 00:59:14,799
pointer back into the sched function

1516
00:59:14,799 --> 00:59:17,119
another question is how come switch only

1517
00:59:17,119 --> 00:59:19,440
saves 14 registers i counted them

1518
00:59:19,440 --> 00:59:22,640
and only saves and restores 14 registers

1519
00:59:22,640 --> 00:59:25,440
even though the risk 5 has 32 registers

1520
00:59:25,440 --> 00:59:27,359
available for the

1521
00:59:27,359 --> 00:59:31,680
for use for code to use why

1522
00:59:31,680 --> 00:59:33,760
why only half the registers are saved

1523
00:59:33,760 --> 00:59:35,520
well when switch was called it was

1524
00:59:35,520 --> 00:59:37,520
called as a normal function so whoever

1525
00:59:37,520 --> 00:59:39,119
called switch already assumed well

1526
00:59:39,119 --> 00:59:41,040
switch might modify those so that

1527
00:59:41,040 --> 00:59:43,839
that function already saved that on its

1528
00:59:43,839 --> 00:59:44,480
stack

1529
00:59:44,480 --> 00:59:47,440
meaning that like when we jump from one

1530
00:59:47,440 --> 00:59:48,720
to the other

1531
00:59:48,720 --> 00:59:52,000
uh that one's going to self-restore its

1532
00:59:52,000 --> 00:59:54,960
caller saved registers that's exactly

1533
00:59:54,960 --> 00:59:56,640
right the switch is a

1534
00:59:56,640 --> 00:59:59,839
called from c code we know that the c

1535
00:59:59,839 --> 01:00:04,160
compiler um saves on the current stack

1536
01:00:04,160 --> 01:00:07,760
uh any caller saved registers that have

1537
01:00:07,760 --> 01:00:09,119
values in them that the compiler is

1538
01:00:09,119 --> 01:00:11,200
going to need later

1539
01:00:11,200 --> 01:00:13,119
and those caller saved registers

1540
01:00:13,119 --> 01:00:16,000
actually include

1541
01:00:16,000 --> 01:00:19,040
i think there's 18

1542
01:00:19,040 --> 01:00:20,319
depending on how you count them they're

1543
01:00:20,319 --> 01:00:22,160
somewhere between 15 and 18

1544
01:00:22,160 --> 01:00:25,359
caller saved registers and so the

1545
01:00:25,359 --> 01:00:27,119
registers we see here

1546
01:00:27,119 --> 01:00:29,200
are all the registers that aren't caller

1547
01:00:29,200 --> 01:00:30,799
saved and that the compiler

1548
01:00:30,799 --> 01:00:32,960
doesn't promise to save but nevertheless

1549
01:00:32,960 --> 01:00:35,359
may hold

1550
01:00:35,359 --> 01:00:37,200
values that are needed by the calling

1551
01:00:37,200 --> 01:00:38,799
function so we only have to save the

1552
01:00:38,799 --> 01:00:41,040
callee saved registers

1553
01:00:41,040 --> 01:00:45,910
when we're switching threads

1554
01:00:45,920 --> 01:00:49,760
okay um final thing i want to print

1555
01:00:49,760 --> 01:00:52,240
is the we do we do save and restore the

1556
01:00:52,240 --> 01:00:53,040
stack pointer

1557
01:00:53,040 --> 01:00:54,720
the current stack pointer it's like hard

1558
01:00:54,720 --> 01:00:56,160
to tell from this value what that means

1559
01:00:56,160 --> 01:00:56,720
but it's

1560
01:00:56,720 --> 01:00:59,280
the kernel stack of the current process

1561
01:00:59,280 --> 01:01:00,640
which

1562
01:01:00,640 --> 01:01:01,920
i don't know if you recall but is

1563
01:01:01,920 --> 01:01:04,000
allocated is mapped by the virtual

1564
01:01:04,000 --> 01:01:07,280
memory system at high memory

1565
01:01:07,280 --> 01:01:12,230
okay so um

1566
01:01:12,240 --> 01:01:14,160
okay so so the we're going to save away

1567
01:01:14,160 --> 01:01:15,760
the current registers and restore

1568
01:01:15,760 --> 01:01:17,520
registers from

1569
01:01:17,520 --> 01:01:20,240
uh the scheduler threads context um i

1570
01:01:20,240 --> 01:01:21,280
don't want to like

1571
01:01:21,280 --> 01:01:23,200
execute every single one of these lows

1572
01:01:23,200 --> 01:01:24,559
in store so i'm going to step

1573
01:01:24,559 --> 01:01:28,559
over all the 14 loads the 14 stores and

1574
01:01:28,559 --> 01:01:30,000
the 14 loads gonna

1575
01:01:30,000 --> 01:01:32,240
proceed directly to the return

1576
01:01:32,240 --> 01:01:33,920
instructions okay so we executed

1577
01:01:33,920 --> 01:01:37,280
everything in switch except the return

1578
01:01:37,280 --> 01:01:39,599
before we do the return we'll just print

1579
01:01:39,599 --> 01:01:41,200
the interesting registers again to see

1580
01:01:41,200 --> 01:01:42,319
where we are so

1581
01:01:42,319 --> 01:01:45,440
stack pointer um now is

1582
01:01:45,440 --> 01:01:47,440
has a different value the stack pointer

1583
01:01:47,440 --> 01:01:48,799
now points into this stack

1584
01:01:48,799 --> 01:01:52,000
zero area in memory and this is actually

1585
01:01:52,000 --> 01:01:54,799
the place very very early in the boot

1586
01:01:54,799 --> 01:01:55,839
sequence

1587
01:01:55,839 --> 01:01:58,880
where start.s puts the stack so it can

1588
01:01:58,880 --> 01:01:59,359
make

1589
01:01:59,359 --> 01:02:01,760
call the ferry for c function so

1590
01:02:01,760 --> 01:02:03,599
actually back on the original bootstack

1591
01:02:03,599 --> 01:02:06,240
for this cpu which just happens to be

1592
01:02:06,240 --> 01:02:10,470
where the scheduler runs

1593
01:02:10,480 --> 01:02:13,599
okay the program counter

1594
01:02:13,599 --> 01:02:14,960
not very interesting or in switch

1595
01:02:14,960 --> 01:02:18,079
because we haven't returned yet and

1596
01:02:18,079 --> 01:02:21,200
the ra register now points to scheduler

1597
01:02:21,200 --> 01:02:22,559
because we've loaded

1598
01:02:22,559 --> 01:02:25,599
uh we've restored the register set

1599
01:02:25,599 --> 01:02:28,880
previously saved by the scheduler thread

1600
01:02:28,880 --> 01:02:30,799
and indeed we're really now in the

1601
01:02:30,799 --> 01:02:33,440
schedulers right if i were on where

1602
01:02:33,440 --> 01:02:34,799
where now looks totally different from

1603
01:02:34,799 --> 01:02:36,720
the last time we ran it we're now

1604
01:02:36,720 --> 01:02:39,280
indeed in a call to switch but now we're

1605
01:02:39,280 --> 01:02:41,119
in a call from swift to switch that the

1606
01:02:41,119 --> 01:02:42,480
scheduler made

1607
01:02:42,480 --> 01:02:44,640
at some point in the past and the

1608
01:02:44,640 --> 01:02:47,200
schedule was run long ago during boot

1609
01:02:47,200 --> 01:02:48,880
was called as the last thing that maine

1610
01:02:48,880 --> 01:02:53,190
did during the boot process

1611
01:02:53,200 --> 01:02:54,880
so i'm going to execute one instruction

1612
01:02:54,880 --> 01:02:56,559
to return from

1613
01:02:56,559 --> 01:02:59,839
switch now into scheduler

1614
01:02:59,839 --> 01:03:01,760
so now we're in this course scheduler

1615
01:03:01,760 --> 01:03:08,390
let's look at the full code

1616
01:03:08,400 --> 01:03:11,599
so this is the scheduler code this

1617
01:03:11,599 --> 01:03:12,960
function called scheduler and now we're

1618
01:03:12,960 --> 01:03:14,960
executing in the scheduler thread

1619
01:03:14,960 --> 01:03:18,079
for the cpu and we're just at the point

1620
01:03:18,079 --> 01:03:21,520
we just returned from the from a

1621
01:03:21,520 --> 01:03:23,280
previous call to switch that the

1622
01:03:23,280 --> 01:03:25,119
scheduler made a while ago

1623
01:03:25,119 --> 01:03:26,880
uh when it decided it was going to start

1624
01:03:26,880 --> 01:03:29,520
running that process you know pid3 which

1625
01:03:29,520 --> 01:03:30,240
was the spin

1626
01:03:30,240 --> 01:03:33,920
process that was interrupted so

1627
01:03:33,920 --> 01:03:36,960
it's this switch you know process id3

1628
01:03:36,960 --> 01:03:38,880
that spin called switch but it's not

1629
01:03:38,880 --> 01:03:40,559
swift that switch that's returning that

1630
01:03:40,559 --> 01:03:42,079
switch hasn't returned yet

1631
01:03:42,079 --> 01:03:45,440
it's still saved away in uh

1632
01:03:45,440 --> 01:03:48,319
process ids threes stack and contacts we

1633
01:03:48,319 --> 01:03:50,160
just returned from this earlier call to

1634
01:03:50,160 --> 01:03:53,109
switch

1635
01:03:53,119 --> 01:03:54,559
all right so the stuff that happens here

1636
01:03:54,559 --> 01:03:57,359
in the scheduler um

1637
01:03:57,359 --> 01:03:59,920
we're stopped running this process and

1638
01:03:59,920 --> 01:04:00,559
so

1639
01:04:00,559 --> 01:04:03,200
uh we want to forget about the various

1640
01:04:03,200 --> 01:04:04,880
things we did

1641
01:04:04,880 --> 01:04:06,480
in the process of running this process

1642
01:04:06,480 --> 01:04:09,039
we want to forget the cro proc equals

1643
01:04:09,039 --> 01:04:09,680
zero

1644
01:04:09,680 --> 01:04:11,039
basically means that we're forgetting

1645
01:04:11,039 --> 01:04:13,200
that you know we're no longer running

1646
01:04:13,200 --> 01:04:14,640
this process in this course so we don't

1647
01:04:14,640 --> 01:04:15,760
want to

1648
01:04:15,760 --> 01:04:17,760
get have anybody be confused about that

1649
01:04:17,760 --> 01:04:19,039
we set this

1650
01:04:19,039 --> 01:04:22,559
per core proc pointer to zero instead of

1651
01:04:22,559 --> 01:04:24,799
this process the next thing that happens

1652
01:04:24,799 --> 01:04:26,240
is that um

1653
01:04:26,240 --> 01:04:29,119
you remember yield acquired the lock for

1654
01:04:29,119 --> 01:04:30,240
this process

1655
01:04:30,240 --> 01:04:31,760
because it didn't want any other core

1656
01:04:31,760 --> 01:04:33,280
scheduler to look at this process and

1657
01:04:33,280 --> 01:04:34,720
maybe run it

1658
01:04:34,720 --> 01:04:37,039
until the process was completely put to

1659
01:04:37,039 --> 01:04:37,839
sleep

1660
01:04:37,839 --> 01:04:40,559
um we've we've now completed the switch

1661
01:04:40,559 --> 01:04:42,000
away from this process

1662
01:04:42,000 --> 01:04:44,319
and so we can release the lock on the

1663
01:04:44,319 --> 01:04:46,880
process that just yielded

1664
01:04:46,880 --> 01:04:50,480
that's the release at this point

1665
01:04:50,480 --> 01:04:53,119
um we're still in the scheduler if there

1666
01:04:53,119 --> 01:04:54,319
was another core

1667
01:04:54,319 --> 01:04:56,960
at this point some other core scheduler

1668
01:04:56,960 --> 01:04:57,359
could

1669
01:04:57,359 --> 01:04:59,520
find that process because it's runnable

1670
01:04:59,520 --> 01:05:00,559
and run it

1671
01:05:00,559 --> 01:05:02,720
but that's okay because we've completely

1672
01:05:02,720 --> 01:05:04,240
saved its registers

1673
01:05:04,240 --> 01:05:06,480
we're no longer executing on its that

1674
01:05:06,480 --> 01:05:08,160
process is stacked because we're now

1675
01:05:08,160 --> 01:05:10,000
executing on the

1676
01:05:10,000 --> 01:05:12,559
um discord scheduler stack so it's

1677
01:05:12,559 --> 01:05:14,240
actually fine if some other core

1678
01:05:14,240 --> 01:05:17,200
decides to run that process okay but

1679
01:05:17,200 --> 01:05:18,960
there is no other core so

1680
01:05:18,960 --> 01:05:20,839
that doesn't actually happen in this

1681
01:05:20,839 --> 01:05:24,950
demonstration

1682
01:05:24,960 --> 01:05:28,480
um i actually want to spend a moment

1683
01:05:28,480 --> 01:05:30,799
talking about the pr lock a little bit

1684
01:05:30,799 --> 01:05:33,039
more

1685
01:05:33,039 --> 01:05:39,109
pr lock actually does a couple of things

1686
01:05:39,119 --> 01:05:41,520
it does really two things from the point

1687
01:05:41,520 --> 01:05:42,720
of view of scheduling

1688
01:05:42,720 --> 01:05:45,839
one is that yielding the cpu

1689
01:05:45,839 --> 01:05:47,920
involves multiple steps we have to set

1690
01:05:47,920 --> 01:05:49,280
the state to run up

1691
01:05:49,280 --> 01:05:50,319
change the state from running to

1692
01:05:50,319 --> 01:05:53,359
runnable we save the registers

1693
01:05:53,359 --> 01:05:55,920
in the yielding processes context and we

1694
01:05:55,920 --> 01:05:57,599
have to stop using the yielding

1695
01:05:57,599 --> 01:05:58,720
processes stack

1696
01:05:58,720 --> 01:06:00,640
there's at least three steps you know

1697
01:06:00,640 --> 01:06:03,440
which take time

1698
01:06:03,440 --> 01:06:06,640
in order to do all the steps required to

1699
01:06:06,640 --> 01:06:08,880
yield the cpu and so one of the things

1700
01:06:08,880 --> 01:06:10,000
that lock does

1701
01:06:10,000 --> 01:06:11,920
as i mentioned is prevent any other core

1702
01:06:11,920 --> 01:06:13,839
scheduler from looking at our process

1703
01:06:13,839 --> 01:06:15,280
until all three steps

1704
01:06:15,280 --> 01:06:17,119
have completed so the lock is basically

1705
01:06:17,119 --> 01:06:19,680
making those steps atomic

1706
01:06:19,680 --> 01:06:21,200
that either all happen from the point of

1707
01:06:21,200 --> 01:06:22,960
view of other cores

1708
01:06:22,960 --> 01:06:26,720
um or none of them have happened

1709
01:06:26,720 --> 01:06:28,559
it's going to turn out also when we

1710
01:06:28,559 --> 01:06:30,319
start running a process

1711
01:06:30,319 --> 01:06:33,200
that the pr lock is going to have a

1712
01:06:33,200 --> 01:06:34,000
similar

1713
01:06:34,000 --> 01:06:37,680
protective function we're going to set

1714
01:06:37,680 --> 01:06:39,280
the state of a process to running when

1715
01:06:39,280 --> 01:06:40,960
we start executing a process and we're

1716
01:06:40,960 --> 01:06:42,079
going to move its

1717
01:06:42,079 --> 01:06:44,640
registers from its process context into

1718
01:06:44,640 --> 01:06:47,599
the risk 5 registers

1719
01:06:47,599 --> 01:06:50,559
but if an interrupt should happen in the

1720
01:06:50,559 --> 01:06:52,400
middle of that process the interrupt is

1721
01:06:52,400 --> 01:06:53,760
going to see the process in a weird

1722
01:06:53,760 --> 01:06:54,559
state like

1723
01:06:54,559 --> 01:06:56,960
maybe in the state of mark running but

1724
01:06:56,960 --> 01:06:57,680
hasn't yet

1725
01:06:57,680 --> 01:06:59,839
finished moving its registers from the

1726
01:06:59,839 --> 01:07:02,079
context into the risk five registers

1727
01:07:02,079 --> 01:07:03,599
and so that would be a disaster if a

1728
01:07:03,599 --> 01:07:05,200
timer interrupt happened then because we

1729
01:07:05,200 --> 01:07:07,359
might switch away from that process

1730
01:07:07,359 --> 01:07:11,119
before it had restored its registers

1731
01:07:11,119 --> 01:07:12,640
and switching away from that process

1732
01:07:12,640 --> 01:07:15,280
would save now uninitialized risk five

1733
01:07:15,280 --> 01:07:16,160
registers

1734
01:07:16,160 --> 01:07:18,799
into the context processes context

1735
01:07:18,799 --> 01:07:20,960
overwriting its real registers

1736
01:07:20,960 --> 01:07:23,440
so indeed we want starting a process to

1737
01:07:23,440 --> 01:07:24,400
also be

1738
01:07:24,400 --> 01:07:27,599
effectively atomic and in this case

1739
01:07:27,599 --> 01:07:28,559
holding the lock

1740
01:07:28,559 --> 01:07:31,280
holding pro lock across switching to a

1741
01:07:31,280 --> 01:07:32,720
process

1742
01:07:32,720 --> 01:07:34,240
as well as preventing other cores from

1743
01:07:34,240 --> 01:07:35,920
looking at that process also turns off

1744
01:07:35,920 --> 01:07:37,280
interrupts

1745
01:07:37,280 --> 01:07:39,839
for the duration of firing up of

1746
01:07:39,839 --> 01:07:41,280
switching to that thread

1747
01:07:41,280 --> 01:07:43,200
which prevents a timer interrupt from

1748
01:07:43,200 --> 01:07:44,640
ever seeing a process

1749
01:07:44,640 --> 01:07:47,359
that's only midway through being

1750
01:07:47,359 --> 01:07:51,270
switched to

1751
01:07:51,280 --> 01:07:56,000
okay so we're in the scheduler we're

1752
01:07:56,000 --> 01:07:57,359
executing this loop in the schedule

1753
01:07:57,359 --> 01:07:58,720
there's this loop in the scheduler that

1754
01:07:58,720 --> 01:08:00,480
looks at all the processes in turn to

1755
01:08:00,480 --> 01:08:02,640
find one to run

1756
01:08:02,640 --> 01:08:04,079
and in this case we know there's another

1757
01:08:04,079 --> 01:08:06,559
process because

1758
01:08:06,559 --> 01:08:08,319
there's that other spin process that we

1759
01:08:08,319 --> 01:08:10,319
forked

1760
01:08:10,319 --> 01:08:12,400
the but there's a lot of process lots to

1761
01:08:12,400 --> 01:08:13,440
examine

1762
01:08:13,440 --> 01:08:15,440
um so i want to skip over the actual

1763
01:08:15,440 --> 01:08:17,279
proc the scanning of the

1764
01:08:17,279 --> 01:08:19,679
process table and go direct to the point

1765
01:08:19,679 --> 01:08:20,799
at which

1766
01:08:20,799 --> 01:08:22,400
the scheduler finds the next process so

1767
01:08:22,400 --> 01:08:26,470
i'm going to put a break point

1768
01:08:26,480 --> 01:08:29,199
at line 474 where it's actually found a

1769
01:08:29,199 --> 01:08:32,080
new process to run

1770
01:08:32,080 --> 01:08:35,440
here we are on the schedulers scan the

1771
01:08:35,440 --> 01:08:35,839
process

1772
01:08:35,839 --> 01:08:39,199
table and found another process to run

1773
01:08:39,199 --> 01:08:41,679
and it's going to cause that process to

1774
01:08:41,679 --> 01:08:42,239
run you

1775
01:08:42,239 --> 01:08:44,719
can see it line 468 it acquired that

1776
01:08:44,719 --> 01:08:45,759
process is locked

1777
01:08:45,759 --> 01:08:48,080
so now it's entitled to do the various

1778
01:08:48,080 --> 01:08:49,520
steps that are required to switch to

1779
01:08:49,520 --> 01:08:51,679
that process

1780
01:08:51,679 --> 01:08:54,080
in line 473 it set the processes state

1781
01:08:54,080 --> 01:08:55,279
to running

1782
01:08:55,279 --> 01:08:57,759
it's now at 474 going to record in the

1783
01:08:57,759 --> 01:08:59,359
cpu structure

1784
01:08:59,359 --> 01:09:03,279
which process the cpu is executing

1785
01:09:03,279 --> 01:09:05,839
and then call switch to save the

1786
01:09:05,839 --> 01:09:08,719
scheduler's registers and restore

1787
01:09:08,719 --> 01:09:10,640
the target processors registers so we

1788
01:09:10,640 --> 01:09:12,400
can see what process it found by looking

1789
01:09:12,400 --> 01:09:13,679
at

1790
01:09:13,679 --> 01:09:15,920
the new process's name surprisingly it's

1791
01:09:15,920 --> 01:09:17,679
spin

1792
01:09:17,679 --> 01:09:19,759
this process id is now four we used to

1793
01:09:19,759 --> 01:09:23,829
be running three we're now running four

1794
01:09:23,839 --> 01:09:25,120
and we've already set the state to

1795
01:09:25,120 --> 01:09:29,430
running so

1796
01:09:29,440 --> 01:09:32,640
so the state's running

1797
01:09:32,640 --> 01:09:34,480
we can see where this thread is going to

1798
01:09:34,480 --> 01:09:36,520
switch to in the call to switch at line

1799
01:09:36,520 --> 01:09:39,910
475.

1800
01:09:39,920 --> 01:09:42,960
print this context he saved registers

1801
01:09:42,960 --> 01:09:45,359
so where is the ra the course we're

1802
01:09:45,359 --> 01:09:46,799
going to call switch but

1803
01:09:46,799 --> 01:09:49,839
switch as we know returns when it

1804
01:09:49,839 --> 01:09:52,480
returns it returns to the restored ra

1805
01:09:52,480 --> 01:09:55,199
so we really care about is where is it

1806
01:09:55,199 --> 01:09:56,080
that

1807
01:09:56,080 --> 01:10:00,830
ra points to we can find that out by

1808
01:10:00,840 --> 01:10:07,270
oops um using x slash i

1809
01:10:07,280 --> 01:10:09,360
uh it's going to return r a points to

1810
01:10:09,360 --> 01:10:11,040
some point in sked so that's not too

1811
01:10:11,040 --> 01:10:12,159
surprising since

1812
01:10:12,159 --> 01:10:16,320
presumably that other spin process

1813
01:10:16,320 --> 01:10:18,159
was suspended due to a timer interrupt

1814
01:10:18,159 --> 01:10:20,960
which as we know called sked

1815
01:10:20,960 --> 01:10:26,390
which calls switch

1816
01:10:26,400 --> 01:10:29,600
all right so we're about to call switch

1817
01:10:29,600 --> 01:10:31,120
let me just bring up the switch code

1818
01:10:31,120 --> 01:10:34,070
again

1819
01:10:34,080 --> 01:10:38,709
um

1820
01:10:38,719 --> 01:10:41,679
actually enter switch we're still uh

1821
01:10:41,679 --> 01:10:42,960
where shows that we're still in the

1822
01:10:42,960 --> 01:10:46,950
scheduler's context

1823
01:10:46,960 --> 01:10:49,360
i want to again execute all of the

1824
01:10:49,360 --> 01:10:50,480
instructions of the switch this time

1825
01:10:50,480 --> 01:10:52,560
switching from the scheduler to the

1826
01:10:52,560 --> 01:10:55,840
new process so we skip over the 28

1827
01:10:55,840 --> 01:11:00,709
stores and loads

1828
01:11:00,719 --> 01:11:02,800
just convince ourselves that we are

1829
01:11:02,800 --> 01:11:04,640
actually about to return to sched so now

1830
01:11:04,640 --> 01:11:06,320
since we're about to return to sched and

1831
01:11:06,320 --> 01:11:07,360
not scheduler

1832
01:11:07,360 --> 01:11:09,840
we must now be in a process kernel

1833
01:11:09,840 --> 01:11:12,800
thread and no longer

1834
01:11:12,800 --> 01:11:14,719
in the scheduler threat and indeed if we

1835
01:11:14,719 --> 01:11:16,239
look at the back trace

1836
01:11:16,239 --> 01:11:18,480
we had a user trap call there must have

1837
01:11:18,480 --> 01:11:19,840
been a timer interrupt

1838
01:11:19,840 --> 01:11:22,560
from long you know sometime in the past

1839
01:11:22,560 --> 01:11:23,360
that

1840
01:11:23,360 --> 01:11:24,960
as we've seen called yield and sched but

1841
01:11:24,960 --> 01:11:26,480
it was the timer interrupt in the other

1842
01:11:26,480 --> 01:11:27,040
process

1843
01:11:27,040 --> 01:11:29,199
now not in the process that we

1844
01:11:29,199 --> 01:11:35,590
originally looked at

1845
01:11:35,600 --> 01:11:39,280
okay any questions about

1846
01:11:39,280 --> 01:11:41,600
i think i'm going to leave off stepping

1847
01:11:41,600 --> 01:11:43,920
through the code at this point and

1848
01:11:43,920 --> 01:11:46,400
any questions about the any of the

1849
01:11:46,400 --> 01:11:49,750
material we've seen

1850
01:11:49,760 --> 01:11:52,840
oh sorry if if it was for example this

1851
01:11:52,840 --> 01:11:54,000
kyle

1852
01:11:54,000 --> 01:11:57,280
then we would see that our

1853
01:11:57,280 --> 01:12:00,480
array would point somewhere to like

1854
01:12:00,480 --> 01:12:04,320
sleeve or something like that right um

1855
01:12:04,320 --> 01:12:08,400
uh yes well

1856
01:12:08,400 --> 01:12:10,320
we see that the where at this point

1857
01:12:10,320 --> 01:12:11,520
would include

1858
01:12:11,520 --> 01:12:13,280
some system call implementation

1859
01:12:13,280 --> 01:12:14,880
functions and a call to sleep

1860
01:12:14,880 --> 01:12:17,440
as it happens i think i mean this is

1861
01:12:17,440 --> 01:12:18,400
this is

1862
01:12:18,400 --> 01:12:19,280
you're basically the answer your

1863
01:12:19,280 --> 01:12:22,640
question is yes if we had just

1864
01:12:22,640 --> 01:12:25,120
left off executing this process for some

1865
01:12:25,120 --> 01:12:27,040
reason other than timer interrupt

1866
01:12:27,040 --> 01:12:29,199
um switch would be basically returning

1867
01:12:29,199 --> 01:12:32,239
to some system call code instead of to

1868
01:12:32,239 --> 01:12:34,000
scan as it happens i think sleep make

1869
01:12:34,000 --> 01:12:37,750
calls get so

1870
01:12:37,760 --> 01:12:38,960
the back trace would look different it

1871
01:12:38,960 --> 01:12:41,120
would just happen includes get but yes

1872
01:12:41,120 --> 01:12:43,920
so i've chosen just one way of you know

1873
01:12:43,920 --> 01:12:44,880
just

1874
01:12:44,880 --> 01:12:46,640
one way of switching between processes

1875
01:12:46,640 --> 01:12:49,520
due to timer interrupts

1876
01:12:49,520 --> 01:12:51,600
but you also get switches to wait for

1877
01:12:51,600 --> 01:12:53,360
user i o or to wait for other processes

1878
01:12:53,360 --> 01:12:58,470
to do things like write to a pipe

1879
01:12:58,480 --> 01:13:00,719
okay one thing to you probably noticed

1880
01:13:00,719 --> 01:13:02,640
is that um

1881
01:13:02,640 --> 01:13:05,679
scheduler called switch and we're about

1882
01:13:05,679 --> 01:13:07,840
to return from switch here

1883
01:13:07,840 --> 01:13:10,159
but we're returning really from a

1884
01:13:10,159 --> 01:13:11,280
different call to switch

1885
01:13:11,280 --> 01:13:13,120
than the one the scheduler made we're

1886
01:13:13,120 --> 01:13:14,480
returning from a call to switch that

1887
01:13:14,480 --> 01:13:18,080
this process made a long time ago um

1888
01:13:18,080 --> 01:13:20,560
so you know this is potentially a little

1889
01:13:20,560 --> 01:13:22,400
bit confusing but

1890
01:13:22,400 --> 01:13:23,600
you know this is how the guts of a

1891
01:13:23,600 --> 01:13:25,840
thread switch work

1892
01:13:25,840 --> 01:13:28,320
another thing to notice is that the code

1893
01:13:28,320 --> 01:13:29,840
we're looking at the switch code this is

1894
01:13:29,840 --> 01:13:30,880
really the heart

1895
01:13:30,880 --> 01:13:34,080
of thread switching and

1896
01:13:34,080 --> 01:13:35,760
really all you have to do to switch

1897
01:13:35,760 --> 01:13:37,920
switch threads is

1898
01:13:37,920 --> 01:13:40,800
save registers and restore registers now

1899
01:13:40,800 --> 01:13:42,320
threads have a lot more state than just

1900
01:13:42,320 --> 01:13:44,480
registers they have variables and

1901
01:13:44,480 --> 01:13:47,920
stuff in the heap and who knows what but

1902
01:13:47,920 --> 01:13:51,040
all that other state is in memory and

1903
01:13:51,040 --> 01:13:52,719
isn't going to be disturbed we've done

1904
01:13:52,719 --> 01:13:54,800
nothing to disturb

1905
01:13:54,800 --> 01:13:57,360
any of these threads stacks for example

1906
01:13:57,360 --> 01:13:58,000
or

1907
01:13:58,000 --> 01:14:01,840
heap values so the registers in the

1908
01:14:01,840 --> 01:14:03,520
microprocessor are really the only kind

1909
01:14:03,520 --> 01:14:04,640
of volatile state

1910
01:14:04,640 --> 01:14:06,320
that actually needs to be saved and

1911
01:14:06,320 --> 01:14:07,920
restored to do a thread switch

1912
01:14:07,920 --> 01:14:09,840
all the stuffs in memory stack for

1913
01:14:09,840 --> 01:14:11,840
example will still be in memory on

1914
01:14:11,840 --> 01:14:13,760
undisturbed and so it doesn't have to be

1915
01:14:13,760 --> 01:14:16,239
explicitly saved and restored

1916
01:14:16,239 --> 01:14:17,840
now we're only saving and restoring this

1917
01:14:17,840 --> 01:14:19,920
microprocessor the cpu registers

1918
01:14:19,920 --> 01:14:22,080
because we want to reuse those very

1919
01:14:22,080 --> 01:14:24,239
registers in the cpu for the new thread

1920
01:14:24,239 --> 01:14:24,480
and

1921
01:14:24,480 --> 01:14:27,440
overwrite whatever values they have so

1922
01:14:27,440 --> 01:14:28,640
the

1923
01:14:28,640 --> 01:14:30,000
register that that's where we have to

1924
01:14:30,000 --> 01:14:32,880
see the old threads registers

1925
01:14:32,880 --> 01:14:35,840
uh what about um other processor state

1926
01:14:35,840 --> 01:14:37,520
so i don't know if the risk 5 processor

1927
01:14:37,520 --> 01:14:39,360
that we're using has other

1928
01:14:39,360 --> 01:14:41,840
flags but i know like some x86 intel

1929
01:14:41,840 --> 01:14:43,040
chips have like

1930
01:14:43,040 --> 01:14:45,040
like the floating point unit state and

1931
01:14:45,040 --> 01:14:46,560
like things like that

1932
01:14:46,560 --> 01:14:48,480
do we do we just not have that in

1933
01:14:48,480 --> 01:14:50,719
response absolutely

1934
01:14:50,719 --> 01:14:52,400
your points very well taken on other

1935
01:14:52,400 --> 01:14:54,560
microprocessors like x86

1936
01:14:54,560 --> 01:14:58,080
the details of switching

1937
01:14:58,080 --> 01:14:59,920
are a bit different because you know

1938
01:14:59,920 --> 01:15:00,880
they had different registers in

1939
01:15:00,880 --> 01:15:01,679
different state

1940
01:15:01,679 --> 01:15:04,480
um and so the code you know this is very

1941
01:15:04,480 --> 01:15:07,120
very risk five dependent code and

1942
01:15:07,120 --> 01:15:09,760
the switch routine for some other

1943
01:15:09,760 --> 01:15:11,120
processor

1944
01:15:11,120 --> 01:15:12,480
might look quite different like indeed

1945
01:15:12,480 --> 01:15:14,320
might have to save floating point

1946
01:15:14,320 --> 01:15:15,199
registers

1947
01:15:15,199 --> 01:15:17,840
now risk five actually uses the general

1948
01:15:17,840 --> 01:15:20,239
purpose registers or

1949
01:15:20,239 --> 01:15:22,239
actually i'm not sure what it does for

1950
01:15:22,239 --> 01:15:23,600
floating point

1951
01:15:23,600 --> 01:15:24,719
but the kernel doesn't use floating

1952
01:15:24,719 --> 01:15:25,920
point so it doesn't have to worry about

1953
01:15:25,920 --> 01:15:27,360
it

1954
01:15:27,360 --> 01:15:29,040
but yeah this is totally microprocessor

1955
01:15:29,040 --> 01:15:31,669
dependent

1956
01:15:31,679 --> 01:15:34,880
a question about the timer interrupts so

1957
01:15:34,880 --> 01:15:36,560
it sounds like the the core

1958
01:15:36,560 --> 01:15:39,760
of all of this scheduling working is

1959
01:15:39,760 --> 01:15:41,679
that there will be a timer interrupt

1960
01:15:41,679 --> 01:15:44,000
uh what happens in cases where that

1961
01:15:44,000 --> 01:15:45,600
malfunctions

1962
01:15:45,600 --> 01:15:49,040
there is going to be a timer interrupt

1963
01:15:49,040 --> 01:15:52,640
um so the the

1964
01:15:52,640 --> 01:15:55,920
i mean uh so so um okay so the the

1965
01:15:55,920 --> 01:15:57,760
reasoning for how come

1966
01:15:57,760 --> 01:16:00,080
preemptive scheduling of user processes

1967
01:16:00,080 --> 01:16:01,440
works

1968
01:16:01,440 --> 01:16:05,679
um is that user processes execute with

1969
01:16:05,679 --> 01:16:09,040
interrupts turned on always xv6 just

1970
01:16:09,040 --> 01:16:11,360
ensures that interrupts are enabled

1971
01:16:11,360 --> 01:16:13,120
before returning to user space and that

1972
01:16:13,120 --> 01:16:14,400
means that a timer interrupt

1973
01:16:14,400 --> 01:16:16,320
can happen if you're executing in user

1974
01:16:16,320 --> 01:16:17,920
space so

1975
01:16:17,920 --> 01:16:20,000
there's nothing a user process you know

1976
01:16:20,000 --> 01:16:21,280
if we're in user space the timer and

1977
01:16:21,280 --> 01:16:21,520
error

1978
01:16:21,520 --> 01:16:24,080
just will happen uh when the time comes

1979
01:16:24,080 --> 01:16:25,520
it's a little trickier in the kernel

1980
01:16:25,520 --> 01:16:26,800
the kernel sometimes turns off

1981
01:16:26,800 --> 01:16:28,800
interrupts like when you acquire a lock

1982
01:16:28,800 --> 01:16:30,080
the interops are going to be turned off

1983
01:16:30,080 --> 01:16:32,840
until you release it so

1984
01:16:32,840 --> 01:16:35,920
um uh so

1985
01:16:35,920 --> 01:16:38,719
if there were some bug in the kernel the

1986
01:16:38,719 --> 01:16:39,920
you know if the kernel turned off

1987
01:16:39,920 --> 01:16:40,800
interrupts and

1988
01:16:40,800 --> 01:16:43,760
never turned them back on and the code

1989
01:16:43,760 --> 01:16:45,840
in the kernel never gave up the cpu

1990
01:16:45,840 --> 01:16:47,520
you know never called sleep or gave up

1991
01:16:47,520 --> 01:16:49,679
the cpu for any other reason

1992
01:16:49,679 --> 01:16:52,400
then indeed a timer interrupt wouldn't

1993
01:16:52,400 --> 01:16:52,960
occur

1994
01:16:52,960 --> 01:16:55,920
and that would mean that this kernel

1995
01:16:55,920 --> 01:16:56,719
code may

1996
01:16:56,719 --> 01:17:00,400
you know we would never give the cpu but

1997
01:17:00,400 --> 01:17:03,280
in fact uh as far as we know

1998
01:17:03,280 --> 01:17:06,400
xp6 is we wrote xp6 so that

1999
01:17:06,400 --> 01:17:09,840
it always turns interrupts back on or

2000
01:17:09,840 --> 01:17:11,360
you know if there's code in xv6 that

2001
01:17:11,360 --> 01:17:13,280
turns off interrupts it either turns

2002
01:17:13,280 --> 01:17:14,239
them back on

2003
01:17:14,239 --> 01:17:17,440
and so uh timer interrupt can then occur

2004
01:17:17,440 --> 01:17:18,960
in the kernel and we can switch away

2005
01:17:18,960 --> 01:17:20,320
from this kernel thread

2006
01:17:20,320 --> 01:17:23,280
or the code returns back to user space

2007
01:17:23,280 --> 01:17:25,120
kernel code returns back to user space

2008
01:17:25,120 --> 01:17:27,280
we believe there's never a situation in

2009
01:17:27,280 --> 01:17:28,640
which kernel code

2010
01:17:28,640 --> 01:17:30,719
will simply like loop with interrupts

2011
01:17:30,719 --> 01:17:33,910
turned off forever

2012
01:17:33,920 --> 01:17:36,080
i got it my question was more about like

2013
01:17:36,080 --> 01:17:37,600
so i assume the interrupts are actually

2014
01:17:37,600 --> 01:17:39,120
coming from some piece of hardware

2015
01:17:39,120 --> 01:17:40,640
like what if that piece of hardware

2016
01:17:40,640 --> 01:17:44,719
malfunctions no

2017
01:17:44,719 --> 01:17:46,719
now it's all right then your computer is

2018
01:17:46,719 --> 01:17:50,470
broken and you should buy a new one

2019
01:17:50,480 --> 01:17:52,719
okay i mean i mean that's a valid

2020
01:17:52,719 --> 01:17:54,080
question for me there's

2021
01:17:54,080 --> 01:17:56,239
you know 10 billion transistors in your

2022
01:17:56,239 --> 01:17:57,199
computer and

2023
01:17:57,199 --> 01:18:00,400
indeed sometimes the hardware just like

2024
01:18:00,400 --> 01:18:02,560
has bugs in it but that's

2025
01:18:02,560 --> 01:18:05,920
beyond our reach for i mean

2026
01:18:05,920 --> 01:18:07,679
if you add one on one and the computer

2027
01:18:07,679 --> 01:18:10,790
says three then

2028
01:18:10,800 --> 01:18:13,199
you just have deep problems that xv6

2029
01:18:13,199 --> 01:18:16,709
can't help you with

2030
01:18:16,719 --> 01:18:18,239
so we're assuming that the computer

2031
01:18:18,239 --> 01:18:20,480
works

2032
01:18:20,480 --> 01:18:25,199
the only time when that when software

2033
01:18:25,199 --> 01:18:26,480
there are times when software tries to

2034
01:18:26,480 --> 01:18:28,000
compensate for hardware level errors

2035
01:18:28,000 --> 01:18:28,560
like

2036
01:18:28,560 --> 01:18:30,000
if you're sending packets across a

2037
01:18:30,000 --> 01:18:33,199
network you always send a checksum

2038
01:18:33,199 --> 01:18:35,679
so that if the network hardware you know

2039
01:18:35,679 --> 01:18:36,800
flips a bit

2040
01:18:36,800 --> 01:18:38,640
malfunctions and flips a bit then you

2041
01:18:38,640 --> 01:18:40,000
can correct that but

2042
01:18:40,000 --> 01:18:41,920
for stuff inside the computer that

2043
01:18:41,920 --> 01:18:43,280
people tend not to

2044
01:18:43,280 --> 01:18:47,920
it's just people basically don't

2045
01:18:47,920 --> 01:18:49,360
try to make the software compensate for

2046
01:18:49,360 --> 01:18:54,080
hardware errors

2047
01:18:54,080 --> 01:18:57,120
oh i have a question why uh so like

2048
01:18:57,120 --> 01:19:00,480
in trampoline dot sending switch we

2049
01:19:00,480 --> 01:19:03,440
write the code in assembly is that why

2050
01:19:03,440 --> 01:19:04,400
is that because

2051
01:19:04,400 --> 01:19:07,440
we want to make sure that exactly this

2052
01:19:07,440 --> 01:19:10,239
thing's happening so we cannot you

2053
01:19:10,239 --> 01:19:12,000
cannot write in and see because

2054
01:19:12,000 --> 01:19:15,120
we just need those like those exact

2055
01:19:15,120 --> 01:19:17,520
things to happen basically

2056
01:19:17,520 --> 01:19:20,880
um yeah yeah

2057
01:19:20,880 --> 01:19:24,000
uh yes certainly we want this exact

2058
01:19:24,000 --> 01:19:25,360
sequence to happen

2059
01:19:25,360 --> 01:19:28,800
um in nnc it

2060
01:19:28,800 --> 01:19:30,320
it's very hard to talk about things like

2061
01:19:30,320 --> 01:19:32,320
r a and c

2062
01:19:32,320 --> 01:19:35,679
or sp um certainly there's no way within

2063
01:19:35,679 --> 01:19:36,000
the c

2064
01:19:36,000 --> 01:19:38,719
language to talk about changing the

2065
01:19:38,719 --> 01:19:40,719
stack pointer

2066
01:19:40,719 --> 01:19:43,520
or the ra register so these are things

2067
01:19:43,520 --> 01:19:45,280
that

2068
01:19:45,280 --> 01:19:47,760
just can't be you can't say it in

2069
01:19:47,760 --> 01:19:49,600
ordinary c

2070
01:19:49,600 --> 01:19:51,760
the only way you can say it in c is

2071
01:19:51,760 --> 01:19:53,840
there is it's possible in c to sort of

2072
01:19:53,840 --> 01:19:54,640
embed

2073
01:19:54,640 --> 01:19:57,280
assembly instructions in the c code and

2074
01:19:57,280 --> 01:19:58,719
so we could have just embedded these

2075
01:19:58,719 --> 01:20:00,719
assembly structures in the c function

2076
01:20:00,719 --> 01:20:03,679
but it would amount to the same thing

2077
01:20:03,679 --> 01:20:05,040
we're basically we're operating at a

2078
01:20:05,040 --> 01:20:07,040
level below

2079
01:20:07,040 --> 01:20:09,920
below c so we can't really we can't

2080
01:20:09,920 --> 01:20:13,910
really use c here

2081
01:20:13,920 --> 01:20:16,159
i have a question about when a thread

2082
01:20:16,159 --> 01:20:18,000
finishes executing i'm assuming that

2083
01:20:18,000 --> 01:20:19,840
happens in the user space when we call

2084
01:20:19,840 --> 01:20:22,080
the exec i'm sorry

2085
01:20:22,080 --> 01:20:25,199
exit system call and

2086
01:20:25,199 --> 01:20:28,000
um that also ends the process in the

2087
01:20:28,000 --> 01:20:30,159
thread i'm assuming in the kernel space

2088
01:20:30,159 --> 01:20:33,199
but if the thread ends

2089
01:20:33,199 --> 01:20:35,360
within before a new time interrupt

2090
01:20:35,360 --> 01:20:37,120
happens does it still

2091
01:20:37,120 --> 01:20:40,480
like um is this

2092
01:20:40,480 --> 01:20:42,320
like the cpu still acquired by that

2093
01:20:42,320 --> 01:20:44,320
thread or do we end that thread and

2094
01:20:44,320 --> 01:20:45,840
start a new one before the new time

2095
01:20:45,840 --> 01:20:46,560
interrupt

2096
01:20:46,560 --> 01:20:49,750
oh yeah the

2097
01:20:49,760 --> 01:20:54,719
the thread um the thread yields the cpu

2098
01:20:54,719 --> 01:20:57,280
does the exit exit yields the cpu so

2099
01:20:57,280 --> 01:20:59,520
there's actually many points that

2100
01:20:59,520 --> 01:21:00,560
even though i've been driving this

2101
01:21:00,560 --> 01:21:02,159
discussion with the timer interrupt in

2102
01:21:02,159 --> 01:21:04,159
fact

2103
01:21:04,159 --> 01:21:07,040
in almost almost all cases where xv6

2104
01:21:07,040 --> 01:21:08,719
switches between threads it's not due to

2105
01:21:08,719 --> 01:21:09,920
timer interrupts

2106
01:21:09,920 --> 01:21:12,960
it's because um some system call is

2107
01:21:12,960 --> 01:21:15,360
waiting for something or decides

2108
01:21:15,360 --> 01:21:17,679
that that it needs to give up the cpu

2109
01:21:17,679 --> 01:21:19,120
and so for example

2110
01:21:19,120 --> 01:21:21,679
exit does various things and then calls

2111
01:21:21,679 --> 01:21:22,480
yield

2112
01:21:22,480 --> 01:21:25,520
to give up the cpu and it does that

2113
01:21:25,520 --> 01:21:26,880
there's really nothing it does that

2114
01:21:26,880 --> 01:21:28,400
independently whether there's a timer

2115
01:21:28,400 --> 01:21:31,669
interrupt

2116
01:21:31,679 --> 01:21:37,750
yes

2117
01:21:37,760 --> 01:21:40,480
all right the time is up for this

2118
01:21:40,480 --> 01:21:41,520
lecture i think i'll

2119
01:21:41,520 --> 01:21:43,920
continue some of this discussion next

2120
01:21:43,920 --> 01:21:45,520
week but i'm happy to take more

2121
01:21:45,520 --> 01:21:46,800
questions

2122
01:21:46,800 --> 01:21:52,229
right now if people have them

2123
01:21:52,239 --> 01:21:54,159
so let's say the operating system

2124
01:21:54,159 --> 01:21:56,400
actually um

2125
01:21:56,400 --> 01:21:59,520
takes on the thread implementation so

2126
01:21:59,520 --> 01:22:02,320
so for example you want to run multiple

2127
01:22:02,320 --> 01:22:03,760
threads of a process

2128
01:22:03,760 --> 01:22:06,239
on multiple cpus like that has to be

2129
01:22:06,239 --> 01:22:08,320
handled by the os that cannot just be

2130
01:22:08,320 --> 01:22:10,320
handled in user space right

2131
01:22:10,320 --> 01:22:12,080
how does that kind of switching work is

2132
01:22:12,080 --> 01:22:14,239
each each thread mouse becomes the same

2133
01:22:14,239 --> 01:22:15,840
as the process it's like is it always

2134
01:22:15,840 --> 01:22:16,960
going to loop through all

2135
01:22:16,960 --> 01:22:20,400
existing threads or you know because

2136
01:22:20,400 --> 01:22:22,560
like each cpu will still switch between

2137
01:22:22,560 --> 01:22:24,159
even if one process has give me eight

2138
01:22:24,159 --> 01:22:25,760
cores like it's still going to switch

2139
01:22:25,760 --> 01:22:27,840
switch each of the cpus between

2140
01:22:27,840 --> 01:22:30,480
those and then couple of other processes

2141
01:22:30,480 --> 01:22:32,080
and then also we don't want to really

2142
01:22:32,080 --> 01:22:32,800
switch

2143
01:22:32,800 --> 01:22:34,639
like between one and the other thread on

2144
01:22:34,639 --> 01:22:36,080
the same cpu

2145
01:22:36,080 --> 01:22:39,360
or do we i don't know wait

2146
01:22:39,360 --> 01:22:42,960
can uh um i'm not sure what the question

2147
01:22:42,960 --> 01:22:43,840
is

2148
01:22:43,840 --> 01:22:45,360
yeah i guess i guess can you just

2149
01:22:45,360 --> 01:22:48,560
explain more like how does that happen

2150
01:22:48,560 --> 01:22:51,600
sorry how does what happen um

2151
01:22:51,600 --> 01:22:53,199
let's say we have multiple threads per

2152
01:22:53,199 --> 01:22:54,880
process so that they can and they can

2153
01:22:54,880 --> 01:22:55,360
run on

2154
01:22:55,360 --> 01:22:57,920
different gpus like how do what do we go

2155
01:22:57,920 --> 01:22:59,199
how to go about there

2156
01:22:59,199 --> 01:23:01,679
yeah so linux for example supports uh

2157
01:23:01,679 --> 01:23:02,880
multiple threads

2158
01:23:02,880 --> 01:23:05,040
per process and in linux the

2159
01:23:05,040 --> 01:23:07,600
implementation

2160
01:23:07,600 --> 01:23:09,360
is a complex implementation but maybe

2161
01:23:09,360 --> 01:23:11,920
the simplest way to explain it is that

2162
01:23:11,920 --> 01:23:15,679
um each uh it's almost as if each thread

2163
01:23:15,679 --> 01:23:16,560
in linux

2164
01:23:16,560 --> 01:23:20,080
is a complete process and

2165
01:23:20,080 --> 01:23:22,880
the the threads of a given what we would

2166
01:23:22,880 --> 01:23:25,199
call the threads of a particular process

2167
01:23:25,199 --> 01:23:27,679
are essentially separate processes that

2168
01:23:27,679 --> 01:23:29,920
share the same memory

2169
01:23:29,920 --> 01:23:31,760
so linux has sort of separated out the

2170
01:23:31,760 --> 01:23:33,040
notion of

2171
01:23:33,040 --> 01:23:34,960
the thread of execution from address

2172
01:23:34,960 --> 01:23:36,320
space and

2173
01:23:36,320 --> 01:23:39,120
you know you can have them separately

2174
01:23:39,120 --> 01:23:39,520
and

2175
01:23:39,520 --> 01:23:41,040
if you make two threads in one process

2176
01:23:41,040 --> 01:23:42,719
it basically makes two processes

2177
01:23:42,719 --> 01:23:44,880
that share one address space and then

2178
01:23:44,880 --> 01:23:46,719
from then on the scheduling is

2179
01:23:46,719 --> 01:23:49,760
not unlike what xv6 does

2180
01:23:49,760 --> 01:23:53,360
for individual processes i see and then

2181
01:23:53,360 --> 01:23:55,600
is there anything like does the user

2182
01:23:55,600 --> 01:23:57,199
have to specify like

2183
01:23:57,199 --> 01:24:00,480
okay pin each thread to a cpu uh

2184
01:24:00,480 --> 01:24:03,120
or or how does the os make sure that

2185
01:24:03,120 --> 01:24:04,480
different threads of the same process

2186
01:24:04,480 --> 01:24:05,679
don't run on the same core because

2187
01:24:05,679 --> 01:24:06,320
that's kind of

2188
01:24:06,320 --> 01:24:08,320
defeating the purpose or not i guess i

2189
01:24:08,320 --> 01:24:09,679
don't know the

2190
01:24:09,679 --> 01:24:12,320
the uh it's actually just like it's much

2191
01:24:12,320 --> 01:24:14,560
like xp6 namely the

2192
01:24:14,560 --> 01:24:18,719
um you know there's four cores and

2193
01:24:18,719 --> 01:24:20,320
linux will just find four things to run

2194
01:24:20,320 --> 01:24:22,159
on those four cores

2195
01:24:22,159 --> 01:24:25,360
they may be you know if there's not much

2196
01:24:25,360 --> 01:24:26,639
going on then maybe there'll be four

2197
01:24:26,639 --> 01:24:27,840
threads of the same

2198
01:24:27,840 --> 01:24:31,199
process or if there's 100 users logged

2199
01:24:31,199 --> 01:24:33,600
in on an athena machine maybe it's

2200
01:24:33,600 --> 01:24:36,000
one thread each from multiple different

2201
01:24:36,000 --> 01:24:36,800
processes

2202
01:24:36,800 --> 01:24:40,880
you know there's not any one answer

2203
01:24:40,880 --> 01:24:42,960
or the kernel basically finds something

2204
01:24:42,960 --> 01:24:44,320
for each core to do and then

2205
01:24:44,320 --> 01:24:46,880
that core does that thing okay that

2206
01:24:46,880 --> 01:24:48,560
makes sense

2207
01:24:48,560 --> 01:24:51,120
you can you know if you're if you want

2208
01:24:51,120 --> 01:24:52,400
to do careful measurements there is a

2209
01:24:52,400 --> 01:24:53,920
way to pin threads to cores

2210
01:24:53,920 --> 01:24:56,400
but people only do it when they're up to

2211
01:24:56,400 --> 01:25:00,550
something strange

2212
01:25:00,560 --> 01:25:03,920
so they share this little virtual table

2213
01:25:03,920 --> 01:25:06,480
you say it again memory so they say they

2214
01:25:06,480 --> 01:25:07,360
have the same

2215
01:25:07,360 --> 01:25:10,400
page table those threads

2216
01:25:10,400 --> 01:25:13,520
yeah yeah yeah yeah if you're on linux

2217
01:25:13,520 --> 01:25:15,440
if you create two threads in one process

2218
01:25:15,440 --> 01:25:17,280
then you have these two threads that

2219
01:25:17,280 --> 01:25:20,639
uh i don't know if they like literally

2220
01:25:20,639 --> 01:25:23,840
share the exact same page table or

2221
01:25:23,840 --> 01:25:25,199
whether their page tables

2222
01:25:25,199 --> 01:25:28,639
are identical one or the other

2223
01:25:28,639 --> 01:25:30,320
is there a reason why they would have to

2224
01:25:30,320 --> 01:25:32,800
be separate ever

2225
01:25:32,800 --> 01:25:36,550
if you manually map memory or

2226
01:25:36,560 --> 01:25:39,679
i i don't know enough to know whether

2227
01:25:39,679 --> 01:25:44,229
which which linux does

2228
01:25:44,239 --> 01:25:46,000
okay i have another question about like

2229
01:25:46,000 --> 01:25:48,239
a small detail um

2230
01:25:48,239 --> 01:25:50,639
so basically like from my understanding

2231
01:25:50,639 --> 01:25:52,960
when you call switch

2232
01:25:52,960 --> 01:25:55,600
you switch from one call to switch to

2233
01:25:55,600 --> 01:25:57,120
another so the first time you call

2234
01:25:57,120 --> 01:25:57,600
switch

2235
01:25:57,600 --> 01:25:59,840
you have to like kind of artificially

2236
01:25:59,840 --> 01:26:00,880
create that

2237
01:26:00,880 --> 01:26:04,400
other endpoint to come back to right yes

2238
01:26:04,400 --> 01:26:05,840
because you can't just randomly jump

2239
01:26:05,840 --> 01:26:07,440
into write any code

2240
01:26:07,440 --> 01:26:11,840
yes you want to know where that

2241
01:26:11,840 --> 01:26:14,880
where that fake

2242
01:26:14,880 --> 01:26:17,920
where that context is cooked up probably

2243
01:26:17,920 --> 01:26:19,440
somewhere where the process is

2244
01:26:19,440 --> 01:26:21,520
uh created now i guess i don't know yeah

2245
01:26:21,520 --> 01:26:24,719
yeah yeah yeah maybe user internet

2246
01:26:24,719 --> 01:26:33,110
or let's see not using it lock proc

2247
01:26:33,120 --> 01:26:35,520
there's something called fork trap or

2248
01:26:35,520 --> 01:26:36,239
something yeah

2249
01:26:36,239 --> 01:26:38,880
look at this yeah yeah well yeah fork

2250
01:26:38,880 --> 01:26:40,800
red okay so an alloc proc which is

2251
01:26:40,800 --> 01:26:42,159
called both

2252
01:26:42,159 --> 01:26:44,560
for the very first process of blue time

2253
01:26:44,560 --> 01:26:45,199
and by

2254
01:26:45,199 --> 01:26:49,120
fork um alloc proc sets up the critical

2255
01:26:49,120 --> 01:26:50,719
elements of the context

2256
01:26:50,719 --> 01:26:54,550
for the new processes

2257
01:26:54,560 --> 01:26:56,480
it sets up the new processes context it

2258
01:26:56,480 --> 01:26:57,520
actually doesn't matter what most of the

2259
01:26:57,520 --> 01:26:59,120
registers are

2260
01:26:59,120 --> 01:27:00,960
but it doesn't matter what ra is because

2261
01:27:00,960 --> 01:27:02,159
that's where the switch

2262
01:27:02,159 --> 01:27:04,080
the very first switch and that process

2263
01:27:04,080 --> 01:27:07,430
is going to return to ra

2264
01:27:07,440 --> 01:27:09,199
and that process is going to need to use

2265
01:27:09,199 --> 01:27:12,480
its own stack so ra and sp are set up

2266
01:27:12,480 --> 01:27:15,280
are faked essentially so that the very

2267
01:27:15,280 --> 01:27:16,080
first switch

2268
01:27:16,080 --> 01:27:19,120
into a process works so so if i

2269
01:27:19,120 --> 01:27:20,639
understand this correctly like when the

2270
01:27:20,639 --> 01:27:22,400
switch will happen then

2271
01:27:22,400 --> 01:27:24,880
it'll basically just start executing the

2272
01:27:24,880 --> 01:27:25,920
first instruction

2273
01:27:25,920 --> 01:27:28,960
inside of the four cred as if red just

2274
01:27:28,960 --> 01:27:30,960
called switch and returned from

2275
01:27:30,960 --> 01:27:33,280
yeah yeah yeah yeah the return from

2276
01:27:33,280 --> 01:27:34,159
switch is

2277
01:27:34,159 --> 01:27:35,760
going to be a jump to the beginning of

2278
01:27:35,760 --> 01:27:37,360
fork red

2279
01:27:37,360 --> 01:27:41,199
right interesting do we ever

2280
01:27:41,199 --> 01:27:44,400
call fork red or is it always hap

2281
01:27:44,400 --> 01:27:46,159
it always happens like this i don't

2282
01:27:46,159 --> 01:27:48,159
think anything ever calls for credit for

2283
01:27:48,159 --> 01:27:50,320
real i think it's

2284
01:27:50,320 --> 01:27:52,960
just yeah it's only executed in this

2285
01:27:52,960 --> 01:27:55,840
weird way from

2286
01:27:55,840 --> 01:27:59,120
first time a process is run it is really

2287
01:27:59,120 --> 01:28:00,639
its job is to

2288
01:28:00,639 --> 01:28:04,400
release the lock that the scheduler took

2289
01:28:04,400 --> 01:28:06,800
and then return and then this user trap

2290
01:28:06,800 --> 01:28:08,159
red of course is also

2291
01:28:08,159 --> 01:28:11,280
fake in that it's

2292
01:28:11,280 --> 01:28:13,440
it's you know yeah it's like it's as if

2293
01:28:13,440 --> 01:28:15,440
returning from a trap except the trap

2294
01:28:15,440 --> 01:28:16,320
frame is

2295
01:28:16,320 --> 01:28:19,679
faked also to have to like jump to the

2296
01:28:19,679 --> 01:28:21,600
first instruction in the

2297
01:28:21,600 --> 01:28:24,639
user right code

2298
01:28:24,639 --> 01:28:26,880
oh but the trap frame it's again the

2299
01:28:26,880 --> 01:28:28,320
same like you don't need to initialize

2300
01:28:28,320 --> 01:28:30,080
any registers because it's like well

2301
01:28:30,080 --> 01:28:32,719
we're going to the beginning so you

2302
01:28:32,719 --> 01:28:34,320
don't need to assume anything

2303
01:28:34,320 --> 01:28:37,360
yeah the program counter i think is uh

2304
01:28:37,360 --> 01:28:40,159
yeah it needs to be initialized to zero

2305
01:28:40,159 --> 01:28:42,960
i don't know what else

2306
01:28:42,960 --> 01:28:46,229
maybe it

2307
01:28:46,239 --> 01:28:48,080
i mean probably if we call them it

2308
01:28:48,080 --> 01:28:49,840
doesn't right because if we already do

2309
01:28:49,840 --> 01:28:50,320
the

2310
01:28:50,320 --> 01:28:52,080
call then that's going to set the broken

2311
01:28:52,080 --> 01:28:54,159
counter yeah yeah so here's the this

2312
01:28:54,159 --> 01:28:56,480
only happens oh because four copies

2313
01:28:56,480 --> 01:28:59,199
fork copies the program counter the user

2314
01:28:59,199 --> 01:29:00,480
program counter

2315
01:29:00,480 --> 01:29:01,840
and so the only time when we're not

2316
01:29:01,840 --> 01:29:03,199
doing a fork is for the very first

2317
01:29:03,199 --> 01:29:04,239
process where

2318
01:29:04,239 --> 01:29:07,360
it's like explicitly deceptive oh a

2319
01:29:07,360 --> 01:29:09,040
stack pointer oh yeah also needs to be

2320
01:29:09,040 --> 01:29:11,199
set up

2321
01:29:11,199 --> 01:29:13,360
oh yeah because it's that's epc that's

2322
01:29:13,360 --> 01:29:14,960
not pc that's the one that's going to

2323
01:29:14,960 --> 01:29:16,639
get swapped by the trap

2324
01:29:16,639 --> 01:29:21,760
trampoline yes oh i see

2325
01:29:21,760 --> 01:29:23,280
because the real pc is actually going to

2326
01:29:23,280 --> 01:29:25,920
be in trap like inside traveling

2327
01:29:25,920 --> 01:29:27,280
but then we're going to switch it to

2328
01:29:27,280 --> 01:29:29,280
jump to there

2329
01:29:29,280 --> 01:29:32,790
yeah interesting

2330
01:29:32,800 --> 01:29:34,800
uh can i just ask like can you go back

2331
01:29:34,800 --> 01:29:42,870
to the alloc proc

2332
01:29:42,880 --> 01:29:46,800
um i think there's a oh no sorry

2333
01:29:46,800 --> 01:29:49,520
uh four cred uh there's something there

2334
01:29:49,520 --> 01:29:51,920
that happens i think for the first

2335
01:29:51,920 --> 01:29:53,920
process only

2336
01:29:53,920 --> 01:29:57,199
uh what's this for a first call

2337
01:29:57,199 --> 01:29:59,120
i wasn't really sure what happened let's

2338
01:29:59,120 --> 01:30:01,440
see the file system

2339
01:30:01,440 --> 01:30:03,520
the file system needs to be initialized

2340
01:30:03,520 --> 01:30:05,600
and in particular some stuff needs to be

2341
01:30:05,600 --> 01:30:07,120
read off the disk

2342
01:30:07,120 --> 01:30:10,639
in order to get the file system going um

2343
01:30:10,639 --> 01:30:12,239
like the there's this thing called the

2344
01:30:12,239 --> 01:30:14,159
super block which describes like how big

2345
01:30:14,159 --> 01:30:15,840
the file system are is and where the

2346
01:30:15,840 --> 01:30:17,520
various things are in the file system

2347
01:30:17,520 --> 01:30:18,639
and there's also a

2348
01:30:18,639 --> 01:30:21,199
crash recovery log that needs to be

2349
01:30:21,199 --> 01:30:22,159
replayed

2350
01:30:22,159 --> 01:30:25,520
in order to recover from a previous

2351
01:30:25,520 --> 01:30:28,159
crash if there was one

2352
01:30:28,159 --> 01:30:30,639
but in order to do anything in the file

2353
01:30:30,639 --> 01:30:32,400
system you need to be able to wait

2354
01:30:32,400 --> 01:30:35,280
for disk operations to complete but the

2355
01:30:35,280 --> 01:30:37,120
way xv6 works

2356
01:30:37,120 --> 01:30:38,800
you really can only execute the file

2357
01:30:38,800 --> 01:30:42,960
system code in the context of a process

2358
01:30:42,960 --> 01:30:45,679
in order to like wait for i o and so

2359
01:30:45,679 --> 01:30:46,719
therefore the

2360
01:30:46,719 --> 01:30:48,480
initialization of the file system has to

2361
01:30:48,480 --> 01:30:50,480
be deferred until we the first time we

2362
01:30:50,480 --> 01:30:53,120
have a process running

2363
01:30:53,120 --> 01:30:55,600
and that occurs in this very first

2364
01:30:55,600 --> 01:31:00,390
process in fork read

2365
01:31:00,400 --> 01:31:02,719
i see and i'm guessing we'll learn more

2366
01:31:02,719 --> 01:31:04,000
about this uh

2367
01:31:04,000 --> 01:31:07,360
later yeah not about this horrible

2368
01:31:07,360 --> 01:31:10,960
but about uh how file systems work

2369
01:31:10,960 --> 01:31:13,360
all right okay well thank you i'm sorry

2370
01:31:13,360 --> 01:31:21,830
for holding off the oh

2371
01:31:21,840 --> 01:31:23,360
sorry is that going to be the in that

2372
01:31:23,360 --> 01:31:25,280
process when this thing

2373
01:31:25,280 --> 01:31:28,320
is executed

