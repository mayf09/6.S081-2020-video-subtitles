1
00:00:00,030 --> 00:00:00,930
Anyone hear me?


2
00:00:02,860 --> 00:00:03,430
I can hear you.


3
00:00:03,850 --> 00:00:05,170
Thank you, alright .


4
00:00:06,500 --> 00:00:10,520
I'd like to spend today's lecture talking about threads


5
00:00:10,520 --> 00:00:12,680
and how xv6 does thread switching,


6
00:00:12,980 --> 00:00:17,870
this is a sort of one of the under the hood lectures about [xv6],


7
00:00:17,870 --> 00:00:23,120
we've had lectures before about how the system calls interrupts, page tables and locks work


8
00:00:23,510 --> 00:00:29,810
and today we're going to talk about how does the xv6 switches among different processes now.


9
00:00:32,040 --> 00:00:34,110
The reason for this the highest level reason for this,


10
00:00:34,110 --> 00:00:35,700
is that people like their computer


11
00:00:35,700 --> 00:00:39,090
to be able to do more than one task at the same time,


12
00:00:39,540 --> 00:00:42,330
so the reason might be that you're supporting time sharing,


13
00:00:42,330 --> 00:00:45,540
like Athena allows many users to login at the same time


14
00:00:45,540 --> 00:00:47,310
and they know they can all run processes


15
00:00:47,610 --> 00:00:50,670
or even a single user machine or even your iPhone,


16
00:00:51,040 --> 00:00:53,440
you may run many different processes


17
00:00:53,440 --> 00:00:56,680
and expect the computer to do all the things you ask of it,


18
00:00:56,680 --> 00:00:57,490
not just one thing.


19
00:01:00,260 --> 00:01:03,860
Another reason that people like to support multiple tasks is


20
00:01:03,860 --> 00:01:06,350
because it can ease program structure,


21
00:01:06,470 --> 00:01:08,690
threads in particular today's topic,


22
00:01:08,810 --> 00:01:11,060
are sometimes used as a way


23
00:01:11,060 --> 00:01:16,970
to help people, to help programmers put together a program in a sort of simple elegant way,


24
00:01:17,000 --> 00:01:18,140
to reduce complexity,


25
00:01:18,290 --> 00:01:22,010
and you actually saw an example of this in the first lab with the prime number sieve,


26
00:01:22,010 --> 00:01:26,060
which didn't use threads exactly but use multiple processes,


27
00:01:26,060 --> 00:01:30,590
in order to help structure this your prime number sieve software


28
00:01:30,590 --> 00:01:34,850
and arguably it's sort of a more convenient or elegant or simpler way


29
00:01:34,850 --> 00:01:36,850
to to write that software.


30
00:01:37,620 --> 00:01:40,920
And the final reason why people use threads is,


31
00:01:40,920 --> 00:01:44,460
to get parallel speedup from multi core machines,


32
00:01:44,700 --> 00:01:48,570
so it's common to break up your program in a way,


33
00:01:48,570 --> 00:01:55,110
that using threads to allow different parts of the same program to run on different cores,


34
00:01:55,110 --> 00:01:57,600
and if you can maybe if you're lucky,


35
00:01:57,600 --> 00:02:01,710
if you can split your program up to run on four threads on four cores,


36
00:02:01,740 --> 00:02:05,370
you might be able to get a factor of four speed up how fast it runs.


37
00:02:06,780 --> 00:02:11,850
And indeed, you can view the xv6 kernel as a multi core parallel program.


38
00:02:12,700 --> 00:02:18,760
So what threads are, is an abstraction to simplify programming,


39
00:02:18,940 --> 00:02:20,650
when you have many tasks,


40
00:02:20,650 --> 00:02:22,600
when you want to juggle many tasks,


41
00:02:22,630 --> 00:02:24,430
so what a thread is is ,


42
00:02:24,670 --> 00:02:28,960
you can think of a thread as just being a single serial execution,


43
00:02:28,960 --> 00:02:31,390
if you just write a program that does one thing after another,


44
00:02:31,390 --> 00:02:32,560
and you run that program,


45
00:02:32,830 --> 00:02:37,570
that you know you can view the program as a sort of single thread of control.


46
00:02:38,290 --> 00:02:41,830
So, this is a loose definition.


47
00:02:44,160 --> 00:02:47,790
Because there's many different sort of flavors of what people mean by threads,


48
00:02:47,790 --> 00:02:52,920
but we'll say it's one serial execution,


49
00:02:53,250 --> 00:02:59,440
so it's what you get if you fire up one CPU and have it,


50
00:02:59,500 --> 00:03:02,560
you know just execute one instruction after another in the ordinary way.


51
00:03:05,580 --> 00:03:07,770
We often talk about thread having state,


52
00:03:07,800 --> 00:03:08,820
because it's going to turn out,


53
00:03:08,820 --> 00:03:10,830
we're going to want to save away thread state


54
00:03:10,830 --> 00:03:11,760
and restore it later,


55
00:03:12,780 --> 00:03:15,630
and so the right way to think about thread state,


56
00:03:15,630 --> 00:03:18,720
for the most part the most important part,


57
00:03:18,720 --> 00:03:21,120
perhaps of the thread state is its program counter,


58
00:03:21,420 --> 00:03:25,530
because it's an execution, we care a lot about where is it in its execution


59
00:03:25,710 --> 00:03:27,960
and what address is it executing instructions,


60
00:03:29,010 --> 00:03:32,910
but also we care about the rest of the microprocessor state,


61
00:03:32,910 --> 00:03:34,860
that's required to support this execution


62
00:03:35,100 --> 00:03:36,300
and so that means it's.


63
00:03:38,520 --> 00:03:42,960
The state of a thread includes the registers that the compiler uses to hold variables


64
00:03:43,200 --> 00:03:47,070
and also because the just the way the compiler generates code,


65
00:03:47,130 --> 00:03:49,200
thread state includes a stack,


66
00:03:49,970 --> 00:03:53,150
so typically if each thread has its own stack


67
00:03:53,150 --> 00:03:56,090
dedicated to executing that thread and the stack records,


68
00:03:56,270 --> 00:03:58,130
the record of function calls,


69
00:03:59,960 --> 00:04:04,490
that the reflect the current point in the execution of that thread.


70
00:04:05,630 --> 00:04:10,130
And so what a threading system xv6 includes a threading system inside it,


71
00:04:10,160 --> 00:04:11,510
what a threading system does


72
00:04:11,510 --> 00:04:18,200
is manage this interleave, automate the interleaving of multiple threads,


73
00:04:18,410 --> 00:04:22,820
you would like to be able to fire up two or 400 or a thousand threads


74
00:04:22,820 --> 00:04:28,380
and the threading system figure out how to juggle all those threads


75
00:04:28,380 --> 00:04:30,780
and cause them all to make progress and all to execute.


76
00:04:32,030 --> 00:04:36,920
There's really two main strategy,


77
00:04:36,920 --> 00:04:38,390
so we want to interleave,


78
00:04:39,210 --> 00:04:43,190
this is gonna be big topic,


79
00:04:43,190 --> 00:04:46,790
here's how to interleave threads, many threads,


80
00:04:47,240 --> 00:04:50,000
one way to interleave the execution of many threads


81
00:04:50,000 --> 00:04:51,920
is to have multiple CPUs.


82
00:04:55,070 --> 00:04:57,890
Maybe as on a multi core processor


83
00:04:58,490 --> 00:05:00,770
and then each CPU can run its own thread,


84
00:05:00,770 --> 00:05:01,970
so if you have four CPUs,


85
00:05:01,970 --> 00:05:05,540
there's an obvious way to run four threads,


86
00:05:05,540 --> 00:05:07,070
is to run one thread per CPU,


87
00:05:07,070 --> 00:05:11,030
and then each thread automatically gets its own program counter registers,


88
00:05:11,030 --> 00:05:15,320
that is the program counter register is associated with the CPU is running on,


89
00:05:15,680 --> 00:05:18,440
but if you have four CPUs and you have a thousand threads,


90
00:05:18,590 --> 00:05:23,830
then you know how using one corporate thread,


91
00:05:23,830 --> 00:05:26,320
is not going to be enough of an answer.


92
00:05:26,530 --> 00:05:29,740
And so the other main strategy that we'll see,


93
00:05:30,620 --> 00:05:33,620
indeed the topic of most of this lecture


94
00:05:33,830 --> 00:05:41,360
is how each CPU is going to switch among different threads,


95
00:05:41,390 --> 00:05:43,580
so if I have one CPU and a thousand threads,


96
00:05:43,670 --> 00:05:47,810
we're going to see how xv6 builds switching system,


97
00:05:48,050 --> 00:05:50,300
that allows xv6 to run one thread for a while


98
00:05:50,300 --> 00:05:52,340
and then switch and set aside


99
00:05:52,340 --> 00:05:53,900
and save the state of that one thread


100
00:05:53,900 --> 00:05:56,540
and switch to executing a second thread for a while


101
00:05:56,540 --> 00:05:57,950
and then a third thread and so forth,


102
00:05:57,950 --> 00:06:00,080
until it's executed a little bit of each thread,


103
00:06:00,080 --> 00:06:04,340
and then go back and execute more of the of the first thread and so on.


104
00:06:06,170 --> 00:06:08,690
And indeed, xv6 like most operating system,


105
00:06:08,690 --> 00:06:13,640
combines the xv6 will run threads on all the cores that are available


106
00:06:13,640 --> 00:06:16,820
and each core will switch among threads,


107
00:06:17,030 --> 00:06:18,020
because there's typical ,


108
00:06:18,050 --> 00:06:23,030
typically although not always there's typically many more threads than there are CPUs.


109
00:06:24,780 --> 00:06:31,290
One of the many ways in which a different threading systems


110
00:06:31,320 --> 00:06:35,580
or instances of threading systems differ is in whether or not they share memory.


111
00:06:36,960 --> 00:06:44,380
So this is important point .


112
00:06:45,040 --> 00:06:50,920
One possibility is that you could have a single address space with many threads executing in that address space


113
00:06:50,920 --> 00:06:52,810
and then they see each other's changes,


114
00:06:52,990 --> 00:06:56,950
if one of the threads sharing some memory modifies variable,


115
00:06:56,950 --> 00:06:59,770
then the other thread sharing that memory will see the modification.


116
00:07:01,430 --> 00:07:03,200
And so it's in the context of threads running


117
00:07:03,200 --> 00:07:06,080
and sharing memory that we need things like the locks


118
00:07:06,080 --> 00:07:08,390
that you saw in the last lecture.


119
00:07:10,330 --> 00:07:13,510
Xv6 kernel is shared memory,


120
00:07:13,750 --> 00:07:19,810
so xv6 there's a , supports the notion of kernel threads,


121
00:07:19,810 --> 00:07:24,670
there's one kernel thread per process that executes system calls for that process,


122
00:07:24,820 --> 00:07:27,520
all those kernel threads share kernel memory,


123
00:07:28,240 --> 00:07:30,730
so xv6 kernel threads do share memory.


124
00:07:34,400 --> 00:07:38,400
And, on the other hand xv6,


125
00:07:38,400 --> 00:07:43,020
that's another kind of threads, each user process essentially has a single thread of control,


126
00:07:43,020 --> 00:07:45,870
that executes the user instructions for that process,


127
00:07:46,080 --> 00:07:51,060
and indeed a lot of the xv6 kernel threading machinery


128
00:07:51,060 --> 00:07:56,940
is ultimately in support of being able to support and switch them on many user processes,


129
00:07:56,970 --> 00:08:01,080
each user process has a memory and a single thread that runs in that memory,


130
00:08:01,290 --> 00:08:05,750
so xv6 user processes,


131
00:08:11,160 --> 00:08:13,110
each process has one thread


132
00:08:13,110 --> 00:08:17,400
and so there's no sharing of memory among threads


133
00:08:18,030 --> 00:08:20,520
within a single xv6 user process,


134
00:08:20,520 --> 00:08:21,840
because you get multiple processes,


135
00:08:21,840 --> 00:08:27,870
but each of those processes is an address space with a single thread processes,


136
00:08:27,900 --> 00:08:29,310
in xv6, don't share memory.


137
00:08:29,790 --> 00:08:32,400
In other more sophisticated operating systems,


138
00:08:32,400 --> 00:08:33,510
for example Linux,


139
00:08:35,110 --> 00:08:42,310
Linux user level does allow multiple threads in a process


140
00:08:42,310 --> 00:08:46,750
and the processes and those threads share the memory of that single process


141
00:08:47,380 --> 00:08:49,780
and that's super cool if you want to write user level programs


142
00:08:49,780 --> 00:08:54,940
that use level parallel programs that get speed up from multiple cores


143
00:08:55,060 --> 00:08:56,650
better requires sort of another,


144
00:08:56,770 --> 00:08:59,290
it uses a lot of the same basic techniques,


145
00:08:59,290 --> 00:09:00,580
we're going to talk about today,


146
00:09:00,580 --> 00:09:03,790
but there's a certain amount more sophistication in Linux


147
00:09:03,790 --> 00:09:08,600
to get it keep track of multiple threads per process instead of just one.


148
00:09:12,250 --> 00:09:15,970
Okay, at a sort of high level, I just want to mention,


149
00:09:15,970 --> 00:09:22,150
that there's other ways to support the interleaving of multiple tasks on a single computer


150
00:09:22,510 --> 00:09:24,940
and we're not going to talk about them,


151
00:09:25,030 --> 00:09:29,020
if you're curious you can look up things like event-driven programming


152
00:09:29,020 --> 00:09:30,910
or state machines


153
00:09:31,060 --> 00:09:37,360
and these are non-thread techniques to share one computer among many different tasks,


154
00:09:37,480 --> 00:09:42,730
it turns out you know sort of on the [spectrum] of different schemes


155
00:09:42,730 --> 00:09:44,770
for supporting multiple tasks on a computer,


156
00:09:44,860 --> 00:09:46,570
threads are not very efficient,


157
00:09:46,600 --> 00:09:48,130
there's a more efficient schemes,


158
00:09:48,700 --> 00:09:51,670
but threads are usually the most convenient way,


159
00:09:51,700 --> 00:09:53,500
most programmer friendly way,


160
00:09:53,680 --> 00:09:57,220
to support lots of different tasks.


161
00:09:59,770 --> 00:10:04,610
Okay , there's a couple of challenges,


162
00:10:04,610 --> 00:10:08,420
that we're gonna have to bite off if we want to implement a threading system.


163
00:10:12,590 --> 00:10:15,800
The, so this is just high level challenges.


164
00:10:22,880 --> 00:10:24,350
One is as I mentioned before,


165
00:10:24,350 --> 00:10:37,030
how to actually implement the switching for interleave , the switching that allows us to interleave the execution of multiple threads,


166
00:10:37,570 --> 00:10:43,680
and this sort of [broad] name for this process of switching deciding,


167
00:10:43,680 --> 00:10:45,570
I'm going to leave off one thread


168
00:10:45,570 --> 00:10:47,190
and start executing another thread,


169
00:10:47,340 --> 00:10:48,600
it's often called scheduling.


170
00:10:51,520 --> 00:10:55,810
And we'll see that xv6 [detects] as an actual piece of code,


171
00:10:55,810 --> 00:10:58,750
that's the scheduler indeed has multiple schedulers one per core,


172
00:10:59,530 --> 00:11:04,660
but the general idea of how do you drive the decision to switch from one to another,


173
00:11:04,660 --> 00:11:07,780
how to pick the next thread to run, its called scheduling.


174
00:11:10,120 --> 00:11:14,800
Another question is, if you want to actually implement the switch from one thread to another,


175
00:11:14,800 --> 00:11:16,750
you need to save and restore,


176
00:11:16,930 --> 00:11:21,970
so we need to decide what needs to be saved and we are to save it,


177
00:11:22,900 --> 00:11:24,700
when it needs to be saved,


178
00:11:24,700 --> 00:11:26,260
when we leave off executing one thread


179
00:11:26,260 --> 00:11:30,820
and restored when we want to resume executing that thread at some later time,


180
00:11:31,180 --> 00:11:34,870
and final question is what to do about compute bound threads.


181
00:11:37,960 --> 00:11:42,280
The, many of the options ,


182
00:11:42,370 --> 00:11:44,500
many of the most straightforward options for thread switching


183
00:11:44,770 --> 00:11:46,930
involve the threads are voluntarily saying


184
00:11:46,930 --> 00:11:48,370
well I'm going to save away my state


185
00:11:48,370 --> 00:11:51,400
and sort of run another, let another thread be run,


186
00:11:51,460 --> 00:11:56,050
but what do we have a user program that's doing some long running calculation,


187
00:11:56,050 --> 00:11:57,280
that might take hours,


188
00:11:57,430 --> 00:12:00,190
it's not going to be particularly thinking about,


189
00:12:00,190 --> 00:12:02,560
now be time to good time to let something else run,


190
00:12:02,770 --> 00:12:08,620
so it's most convenient to have some way of sort of automatically revoking control


191
00:12:08,620 --> 00:12:11,260
from some long running compute bound process,


192
00:12:11,440 --> 00:12:14,260
setting it aside and maybe running it later.


193
00:12:16,780 --> 00:12:18,850
Alright, so I'm going to talk about these,


194
00:12:18,850 --> 00:12:24,850
I'm actually going to talk about the machinery for dealing with compute bound threads first.


195
00:12:27,220 --> 00:12:31,860
And, scheme for that is something you've come up before,


196
00:12:32,520 --> 00:12:34,110
that's timer interrupts.


197
00:12:38,340 --> 00:12:46,040
And the idea here, is that there's a , a piece of hardware on each CPU on each core,


198
00:12:46,280 --> 00:12:49,640
that generates periodic interrupts ,


199
00:12:49,850 --> 00:12:54,920
and the xv6 or any operating system really arranges to have those interrupts delivered to the kernel,


200
00:12:54,920 --> 00:12:57,410
so even if we're running at user level


201
00:12:57,410 --> 00:13:01,640
and some loop that's you know computing the first billion digits of pi,


202
00:13:01,730 --> 00:13:07,430
nevertheless the timer interrupts go off at some point maybe every ten milliseconds


203
00:13:07,640 --> 00:13:12,770
and transfer control from that user level code into the interrupt handler in the kernel,


204
00:13:12,860 --> 00:13:16,520
and so that's the first step in the kernel,


205
00:13:16,520 --> 00:13:21,110
being able to gain control to switch among different user level processes,


206
00:13:21,110 --> 00:13:22,370
user level threads,


207
00:13:22,580 --> 00:13:25,820
even if those user level threads aren't cooperative.


208
00:13:26,900 --> 00:13:32,720
And the basic scheme is that in the interrupt handler,


209
00:13:32,720 --> 00:13:36,920
so we're gonna have you know kernel handler for these interrupts.


210
00:13:38,170 --> 00:13:39,640
And we'll see,


211
00:13:40,500 --> 00:13:44,370
that a kernel handler yields,


212
00:13:44,430 --> 00:13:47,010
this is the sort of name for this it yields,


213
00:13:49,290 --> 00:13:53,730
the kernel handler sort of voluntarily yields the CPU back to the schedule


214
00:13:53,730 --> 00:13:56,400
until the schedule look, you can let something else run now.


215
00:13:58,240 --> 00:14:03,060
And this yielding is really a form of thread switch,


216
00:14:03,060 --> 00:14:06,330
that saves away the state of the current thread,


217
00:14:06,480 --> 00:14:07,920
so it can be restored later,


218
00:14:10,320 --> 00:14:12,390
as we'll see the full story here,


219
00:14:12,390 --> 00:14:13,920
actually you've seen a lot of the full story here,


220
00:14:13,920 --> 00:14:15,360
because it involves an interrupt,


221
00:14:16,050 --> 00:14:18,330
what you already know about the full story some complex,


222
00:14:18,330 --> 00:14:20,640
but the basic idea is that a timer interrupt


223
00:14:21,000 --> 00:14:22,500
gives control to the kernel


224
00:14:22,500 --> 00:14:25,560
and the kernel voluntarily yields the CPU,


225
00:14:26,820 --> 00:14:30,900
this is called as a piece of terminology preemptive scheduling.


226
00:14:38,980 --> 00:14:42,460
And what that means is that the preemptive means


227
00:14:42,460 --> 00:14:45,730
is that, even if the code that's running doesn't,


228
00:14:46,360 --> 00:14:51,010
it doesn't want to you know doesn't explicitly yield the CPU,


229
00:14:51,160 --> 00:14:53,500
the timer interrupt is going to take control away


230
00:14:53,710 --> 00:14:55,570
and we're going to yield for it


231
00:14:55,570 --> 00:15:02,150
and the opposite of preemptive scheduling might be called, maybe voluntary scheduling.


232
00:15:05,040 --> 00:15:06,330
And the interesting thing is that


233
00:15:06,330 --> 00:15:11,220
the the implementation in xv6 and other operating systems of preemptive scheduling


234
00:15:11,220 --> 00:15:14,100
is this time interrupt forcibly takes away the CPU


235
00:15:14,400 --> 00:15:21,300
and then the kernel basically does a voluntary yield thread thread switch on behalf of that process.


236
00:15:24,100 --> 00:15:30,730
Now, another just piece of terminology that comes up here


237
00:15:30,730 --> 00:15:33,610
is that while the threads running ,


238
00:15:33,640 --> 00:15:37,350
there's need to distinguish ,


239
00:15:37,410 --> 00:15:42,510
systems that distinguish between threads that are currently actually running on some CPU


240
00:15:42,630 --> 00:15:48,330
versus threads that would like to run, but aren't currently running on any CPU,


241
00:15:48,330 --> 00:15:51,150
but you know could run if a CPU became free


242
00:15:51,390 --> 00:15:54,060
versus threads that actually don't want to run,


243
00:15:54,060 --> 00:15:55,590
because they're waiting for IO


244
00:15:55,590 --> 00:15:58,020
or waiting for some event .


245
00:15:58,600 --> 00:16:01,780
And unfortunately, this distinction is often called state,


246
00:16:02,980 --> 00:16:07,540
even though the full state of the thread is actually much more complicated than that.


247
00:16:09,830 --> 00:16:15,300
Since this is going to come up, I just want a list out a couple of states that will be seeing,


248
00:16:16,370 --> 00:16:19,310
and these are states that xv6 maintains,


249
00:16:19,310 --> 00:16:20,930
there's a state called running,


250
00:16:20,930 --> 00:16:25,160
which means it's actually executing on some core, some CPU right now,


251
00:16:25,370 --> 00:16:31,650
there's runnable, which means not currently executing anywhere,


252
00:16:31,650 --> 00:16:33,450
but just a saved state,


253
00:16:33,930 --> 00:16:36,030
but would like to run as soon as possible


254
00:16:36,360 --> 00:16:38,490
and then it turns out there's a state,


255
00:16:38,490 --> 00:16:40,200
which won't come out much today,


256
00:16:40,200 --> 00:16:42,300
but will come up next week called sleeping,


257
00:16:42,420 --> 00:16:45,270
just means the threads waiting for some IO event


258
00:16:45,480 --> 00:16:48,120
and only wants to run after the IO event occurs,


259
00:16:48,300 --> 00:16:51,420
so today we're mostly concerned with running and runnable threads


260
00:16:51,420 --> 00:16:55,560
and what this preemptive switch does what this timer interrupt does and the yield


261
00:16:55,560 --> 00:17:01,800
is basically convert a running thread whatever thread was interrupted by the timer into a vulnerable thread.


262
00:17:02,160 --> 00:17:04,230
That is a thread that's by yielding


263
00:17:04,230 --> 00:17:07,140
or converting that thread into a thread, that's not running right now,


264
00:17:07,140 --> 00:17:08,910
but would actually like to clearly,


265
00:17:08,910 --> 00:17:12,060
because it was running at the time of a timer interrupt.


266
00:17:14,220 --> 00:17:21,180
Okay so, running thread its program counter registers are actually in the CPU,


267
00:17:21,360 --> 00:17:24,420
you know in the hardware registers of the CPU that's executing it,


268
00:17:24,690 --> 00:17:27,870
a runnable thread though has no,


269
00:17:28,470 --> 00:17:30,840
it's not doesn't have a CPU associated with it


270
00:17:31,620 --> 00:17:34,680
and therefore we need to save for every runnable state,


271
00:17:34,860 --> 00:17:45,540
we need to save whatever CPU state, whatever state the CPU was keeping when that thread was running,


272
00:17:45,720 --> 00:17:47,730
so we need to copy the CPU contents,


273
00:17:47,730 --> 00:17:49,890
you know which is not RAM, but just registers really,


274
00:17:50,040 --> 00:17:54,180
from the CPU into memory somewhere to save them,


275
00:17:54,240 --> 00:17:56,550
when we turn to thread from running to runnable


276
00:17:56,640 --> 00:18:02,370
and again this is the basically the state we have to explicitly save yours just the state.


277
00:18:03,030 --> 00:18:10,070
The executing state of the CPU, which is the program counter and registers


278
00:18:10,250 --> 00:18:12,170
and CPU, so these need to be saved.


279
00:18:13,000 --> 00:18:14,980
Only convert a thread runnable,


280
00:18:15,520 --> 00:18:19,480
when some scheduler finally decides to run a runnable thread,


281
00:18:19,690 --> 00:18:23,890
then as part of the many steps in getting that thread going again and resuming it,


282
00:18:24,100 --> 00:18:27,010
we're going to see that the program counter,


283
00:18:27,010 --> 00:18:34,300
the saved program counter registers are copied back into the CPU's actual register on the CPU


284
00:18:34,300 --> 00:18:35,830
that the scheduler decides to run it on.


285
00:18:38,910 --> 00:18:42,960
Alright. Any questions about these terminology.


286
00:18:49,880 --> 00:18:56,210
Alright, I'm gonna, now sort of talk about a sort of more xv6 [oriented] view of things.


287
00:18:57,760 --> 00:19:00,340
I'm gonna draw two pictures, really,


288
00:19:00,340 --> 00:19:05,320
of threads and xv6 are kind of simplified picture and a more detailed picture,


289
00:19:05,470 --> 00:19:09,870
so as usually the user stuff up here


290
00:19:09,870 --> 00:19:11,460
and the kernel down here.


291
00:19:14,040 --> 00:19:17,940
We might be running multiple processes at user level,


292
00:19:17,940 --> 00:19:23,160
maybe you know the C compiler and ls and a shell,


293
00:19:23,520 --> 00:19:27,420
they may or may not be all wanting to run at the same time,


294
00:19:28,260 --> 00:19:37,180
at user level, each of these processes has ,


295
00:19:38,400 --> 00:19:39,570
you know it has memory,


296
00:19:39,690 --> 00:19:42,840
and of particular interest to us,


297
00:19:42,870 --> 00:19:45,510
each of these processes has a user stack,


298
00:19:46,840 --> 00:19:52,660
and while it's running it has registers in the RISC-V hardware,


299
00:19:52,660 --> 00:19:54,610
so PC plus registers.


300
00:19:55,590 --> 00:20:01,920
Alright, so while programs running you know there's essentially a thread of control that's running at user level


301
00:20:02,760 --> 00:20:04,710
and the way I'm going to talk about it is,


302
00:20:04,710 --> 00:20:13,290
as if there's a user thread that consists of the user stack user memory user program counter user registers,


303
00:20:13,530 --> 00:20:16,080
if the program makes a system callers interrupted


304
00:20:16,080 --> 00:20:18,240
and goes into the kernel,


305
00:20:19,020 --> 00:20:23,010
then this stuff saved away in this program's trapframe


306
00:20:23,640 --> 00:20:30,570
and a kernel, the kernel thread for this program is activated


307
00:20:30,660 --> 00:20:35,760
and so now this is the trapframe hold saved user stuff


308
00:20:35,760 --> 00:20:38,700
after we saved a way the user peace program counter registers,


309
00:20:38,850 --> 00:20:42,510
then we switch the CPU to using the kernel stack.


310
00:20:45,520 --> 00:20:47,590
And you know we don't need to restore registers,


311
00:20:47,590 --> 00:20:53,900
because, through the the kernel thread for process isn't really running,


312
00:20:53,900 --> 00:20:58,960
it has no real save state when the user thread is running


313
00:20:58,990 --> 00:21:02,950
instead it's sort of the kernel thread is kind of activated on its stack


314
00:21:02,980 --> 00:21:08,800
for the first time in in the trampoline and a user trap code.


315
00:21:11,940 --> 00:21:13,710
And then the kernel runs for a while,


316
00:21:13,710 --> 00:21:17,250
maybe running a system call or interrupt handler whatever it maybe


317
00:21:18,150 --> 00:21:25,140
and sometimes it is a system called particular will just simply return from this point back to the same process


318
00:21:25,140 --> 00:21:30,690
and return to user space will restore this program's program counter registers,


319
00:21:30,720 --> 00:21:36,180
but it could also be that instead of simply returning for one reason or another,


320
00:21:36,180 --> 00:21:37,590
maybe because it was a timer interrupt,


321
00:21:37,710 --> 00:21:39,900
we're actually going to switch to another process


322
00:21:39,900 --> 00:21:42,270
and the very high-level view of that is that


323
00:21:43,110 --> 00:21:48,930
if the xv6 scheduler decides switch from this process to a different process,


324
00:21:49,590 --> 00:21:51,330
what the first thing that really happens


325
00:21:51,330 --> 00:21:54,540
is that we're going to switch kernel threads


326
00:21:54,570 --> 00:21:58,260
from this processe's kernel thread to the other process's kernel thread,


327
00:21:58,350 --> 00:22:01,530
and then the other processes kernel thread will turn back to user space,


328
00:22:01,530 --> 00:22:04,950
so supposing that the C compiler [C] needs to read the disk


329
00:22:05,070 --> 00:22:07,380
and so it's going to yield the CPU


330
00:22:07,860 --> 00:22:10,230
while sleeping to wait for the disk to complete,


331
00:22:10,410 --> 00:22:17,250
maybe ls wants to execute and is in runnable state, what the xv6 scheduler, maybe they do is that,


332
00:22:17,400 --> 00:22:20,280
well, if ls is in runnable state,


333
00:22:20,280 --> 00:22:22,080
that means it left off somewhere


334
00:22:22,080 --> 00:22:25,740
and its state was saved away possibly by a timer interrupt


335
00:22:25,920 --> 00:22:32,880
and so ls will actually have a saved trapframe with user registers and its own kernel stack,


336
00:22:33,150 --> 00:22:40,910
and as it turns out a [seed] set of kernel registers associated with the kernel thread


337
00:22:40,940 --> 00:22:43,280
which is going to be called the context,


338
00:22:43,520 --> 00:22:48,980
so if xv6 switches from the compiler kernel thread to ls's kernel thread,


339
00:22:49,500 --> 00:22:54,720
xv6 will save away the kernel registers in a context


340
00:22:54,720 --> 00:23:01,500
with a [seat] the compilers kernel thread switch to the ls thread,


341
00:23:02,730 --> 00:23:06,450
complex scheme which I'll describe a little bit later,


342
00:23:06,750 --> 00:23:15,670
we'll restore ls is, kernel thread registers from the previously save context from one ls last left off,


343
00:23:16,000 --> 00:23:19,510
maybe ls will finish whatever system called was executing,


344
00:23:19,510 --> 00:23:23,800
you know on the ls's kernel thread stack,


345
00:23:24,160 --> 00:23:26,620
and then return back to ls system called,


346
00:23:26,620 --> 00:23:28,480
on the way to return to user space


347
00:23:28,480 --> 00:23:32,810
that will restore these previously saved user registers for ls


348
00:23:33,440 --> 00:23:35,660
and then resume executing ls.


349
00:23:35,690 --> 00:23:39,980
So, there's bunch of details here which we'll talk about,


350
00:23:39,980 --> 00:23:42,800
but maybe the main point here is that


351
00:23:42,830 --> 00:23:47,930
whenever in xv6 see direct user to user context switches,


352
00:23:47,930 --> 00:23:50,330
when we're switching from one process to another,


353
00:23:50,330 --> 00:23:58,480
always the sort of strategy by which xv6 switches from executing one process to another process


354
00:23:58,690 --> 00:24:03,910
is you jump in the kernel saves the process state run this process's kernel thread


355
00:24:03,910 --> 00:24:07,720
switch to the kernel thread another process that suspended itself


356
00:24:07,720 --> 00:24:10,000
and then return and restore user register,


357
00:24:10,000 --> 00:24:12,100
so it's always this indirect strategy,


358
00:24:12,430 --> 00:24:15,790
actually even more indirect than this to threads swhich,


359
00:24:15,910 --> 00:24:20,380
where the net effect is to switch from one user process to another user process.


360
00:24:23,310 --> 00:24:26,400
Question about this diagram or anything?


361
00:24:29,260 --> 00:24:32,890
Switched the scheduler, that happens in between those two, right.


362
00:24:33,220 --> 00:24:36,700
Yep, all right, let me talk about the scheduler,


363
00:24:36,700 --> 00:24:42,400
so the real picture is actually a significantly more complex than that.


364
00:24:43,070 --> 00:24:44,390
This is a [medium] or,


365
00:24:46,690 --> 00:24:47,890
gonna be more full diagram,


366
00:24:47,890 --> 00:24:49,330
let's say we have process one,


367
00:24:51,060 --> 00:24:57,150
which is executing process two which is runnable, but not currently running,


368
00:24:58,020 --> 00:24:59,550
now the additional layer of details,


369
00:24:59,550 --> 00:25:01,770
we actually have multiple cores ,


370
00:25:01,980 --> 00:25:04,050
xv6 let's say we have two cores,


371
00:25:04,050 --> 00:25:06,960
so that means that at the hardware level,


372
00:25:07,680 --> 00:25:12,360
we have CPU zero which is one of the cores


373
00:25:12,660 --> 00:25:14,850
and let's say CPU one.


374
00:25:20,030 --> 00:25:27,340
And the more full story about how we get from executing user space to,


375
00:25:27,370 --> 00:25:33,460
in one process executing in user space in another runnable but not yet running process ,


376
00:25:33,880 --> 00:25:36,910
the first part is about the same as I talked about


377
00:25:37,060 --> 00:25:43,540
and may say a timer interrupt forces transfer control from the user process into the kernel,


378
00:25:43,600 --> 00:25:49,480
the trampoline code saves the user registers the trapframe for process one,


379
00:25:51,530 --> 00:25:54,900
and then executes user trap


380
00:25:54,900 --> 00:25:56,820
which figures out what to do with this trap


381
00:25:56,820 --> 00:25:58,710
or interrupt you know system call,


382
00:25:58,740 --> 00:25:59,940
let's say as a,


383
00:26:00,090 --> 00:26:07,830
so for a little while we're executing ordinary kernel C code on the kernel stack of process one.


384
00:26:10,240 --> 00:26:15,250
Let's say process one the kernel code process once decides it wants to yield the CPU,


385
00:26:16,120 --> 00:26:18,580
it has a bunch of things which will see the details of,


386
00:26:18,610 --> 00:26:26,060
that end up in a call to this routine switch, just sort of one of the central routines in this story,


387
00:26:26,150 --> 00:26:33,740
switch saves away this context that registers for the kernel thread that's running in context one,


388
00:26:33,740 --> 00:26:36,620
so there's two sets of registers the user registers,


389
00:26:36,620 --> 00:26:40,450
the trapframe, the kernel thread registers in the context.


390
00:26:41,510 --> 00:26:47,240
Switch doesn't actually switch switches from one content from one thread to another,


391
00:26:47,240 --> 00:26:50,600
but in fact the way xv6 is designed,


392
00:26:50,750 --> 00:26:57,560
the only place that a user thread, sorry the kernel thread running on a CPU can switch to


393
00:26:57,680 --> 00:27:00,710
is what's called the scheduler thread for that CPU.


394
00:27:02,910 --> 00:27:05,550
So we can't even switch directly to another process,


395
00:27:05,790 --> 00:27:07,680
can only switch to the scheduler thread,


396
00:27:07,680 --> 00:27:09,150
so there's a ,


397
00:27:10,360 --> 00:27:16,510
the complete thread [apparatus] dedicated to the scheduler for CPU zero,


398
00:27:16,540 --> 00:27:17,830
since we're running on CPUs,


399
00:27:17,830 --> 00:27:25,390
this switch is going to switch to the previously saved registers for the scheduler thread,


400
00:27:25,420 --> 00:27:27,340
so let's schedule zero.


401
00:27:29,650 --> 00:27:34,210
And in the scheduler for CPU zero switch will,


402
00:27:34,390 --> 00:27:36,100
by restoring these registers,


403
00:27:36,100 --> 00:27:38,440
since registers include the stack pointer ,


404
00:27:38,440 --> 00:27:40,450
the return from switch as we'll see,


405
00:27:40,630 --> 00:27:53,650
well now actually return up to the scheduler function on CPU zero,


406
00:27:53,650 --> 00:27:58,630
and schedule function will do some cleanup to finish putting process one to sleep,


407
00:27:58,720 --> 00:28:02,020
then it'll look in the process table for another process,


408
00:28:02,020 --> 00:28:06,130
run a runnable process, and if it finds one.


409
00:28:07,160 --> 00:28:10,550
And so we've sort of gone down here and up into the scheduler,


410
00:28:10,670 --> 00:28:12,620
if the scheduler finds another process to run


411
00:28:12,620 --> 00:28:17,360
or even finds process one is runnable and still wants to run a find process one


412
00:28:17,360 --> 00:28:19,010
nothing else nothing else wants to run,


413
00:28:19,850 --> 00:28:20,420
but in any case,


414
00:28:20,420 --> 00:28:22,940
the scheduler will call switch again,


415
00:28:22,940 --> 00:28:27,020
to switch contexts to say process two.


416
00:28:27,960 --> 00:28:32,490
in the process swhich will save its own registers again in its own context,


417
00:28:33,020 --> 00:28:36,680
they'll be a previously saved context too,


418
00:28:36,680 --> 00:28:38,720
from whenever process two left off


419
00:28:38,900 --> 00:28:45,800
that those this set of registers will be restored, process two will have made a previous call to switch,


420
00:28:48,100 --> 00:28:50,560
to switch to the scheduler thread just like process one did


421
00:28:50,560 --> 00:28:51,550
when it left off,


422
00:28:51,730 --> 00:28:57,380
that called a switch return to whatever system call or interrupt process two is end


423
00:28:57,560 --> 00:29:02,510
when that's finished, there will be a previously saved trapframe for process two,


424
00:29:02,720 --> 00:29:04,220
that will contain user registers,


425
00:29:04,220 --> 00:29:08,360
also be restored or a return back up into user space.


426
00:29:09,050 --> 00:29:15,920
And there's a complete, a separate scheduler thread for each CPU,


427
00:29:15,920 --> 00:29:22,940
so, they'll also be saved , context for the scheduler thread for CPU one,


428
00:29:23,210 --> 00:29:28,960
and a scheduler loop running on scheduler one


429
00:29:28,960 --> 00:29:32,950
and whatever process you know process three or something is running on CPU one


430
00:29:32,950 --> 00:29:34,750
when it decides to give up the CPU,


431
00:29:34,750 --> 00:29:41,410
it'll switch into a scheduler thread for it for its CPU.


432
00:29:43,800 --> 00:29:46,980
Alright, there's a question where the context stored,


433
00:29:47,310 --> 00:29:51,320
it turns out that for the operations,


434
00:29:51,320 --> 00:29:55,730
I've been talking about the saved in fact always the,


435
00:29:57,250 --> 00:30:00,190
for a thread switch ,


436
00:30:01,030 --> 00:30:04,450
these contexts, these saved register sets for kernel threads


437
00:30:04,450 --> 00:30:05,860
are in the process structure,


438
00:30:06,190 --> 00:30:11,890
so any given kernel thread can only have one set of saved kernel registers.


439
00:30:12,380 --> 00:30:15,680
Because each thread is only executing a single place


440
00:30:15,680 --> 00:30:20,690
and its context kind of reflects that place that it was executing on it left off


441
00:30:20,690 --> 00:30:23,420
a thread is a single thread of control,


442
00:30:23,420 --> 00:30:27,350
so a thread really only needs one context full of registers,


443
00:30:27,380 --> 00:30:33,190
so it's in the process structures p arrow, p->context.


444
00:30:35,310 --> 00:30:38,670
And the scheduler, each scheduler thread has its own context


445
00:30:38,670 --> 00:30:39,900
which actually not in the,


446
00:30:40,230 --> 00:30:43,380
there's no process associated with this scheduler thread,


447
00:30:43,620 --> 00:30:50,710
this is actually scheduler's context is stored in the struct CPU for that core.


448
00:30:51,480 --> 00:30:54,120
There's an array of these CPU's struct one per core,


449
00:30:54,120 --> 00:30:55,860
each one as a context.


450
00:30:58,210 --> 00:31:02,200
Question why can't we include the registers in the trapframe for the process,


451
00:31:02,230 --> 00:31:08,890
that is, you know actually the those registers could be stored in the trapframe


452
00:31:08,890 --> 00:31:14,050
which made because there's only one save set of kernel thread registers,


453
00:31:14,260 --> 00:31:17,050
for process, we could save them in any data structure


454
00:31:17,230 --> 00:31:22,030
for which there's one element of instance of that data structure process,


455
00:31:22,270 --> 00:31:24,400
there's one struct process,


456
00:31:24,400 --> 00:31:26,380
there's one struct trapframe for process,


457
00:31:26,500 --> 00:31:28,450
we could store the registers in the trapframe.


458
00:31:31,730 --> 00:31:33,140
I mean just sort of for,


459
00:31:33,720 --> 00:31:37,050
maybe simplicity or clarity of code, the trapframe I think.


460
00:31:37,640 --> 00:31:42,500
Entirely consists of data, that's needed when entering and leaving the kernel.


461
00:31:43,060 --> 00:31:47,920
And the struct context is consists of the stuff that needs to be saved and restored,


462
00:31:47,920 --> 00:31:52,540
when switching to and from between the kernel thread and the scheduler thread.


463
00:31:54,640 --> 00:31:58,210
Okay question is, yield something that's called by the user of the kernel, called by the kernel,


464
00:31:59,060 --> 00:32:02,630
So the user threads, there's not really a direct way in xv6


465
00:32:02,630 --> 00:32:10,010
for user threads to talk about , yielding the CPU or switching that it's done


466
00:32:10,010 --> 00:32:12,320
by the kernel kind of transparently,


467
00:32:13,210 --> 00:32:16,510
you know it points in time when the kernel feels that it needs to happen,


468
00:32:16,810 --> 00:32:19,690
if their threads there are sometimes when,


469
00:32:21,190 --> 00:32:26,500
you can sort of guess that probably a certain system call will result in yield,


470
00:32:26,500 --> 00:32:30,130
like if a process does a read on a pipe,


471
00:32:30,130 --> 00:32:33,190
where it knows that really nothing is waiting to be read in that pipe,


472
00:32:33,400 --> 00:32:35,290
then the read will block,


473
00:32:35,870 --> 00:32:37,370
you can predict the read block


474
00:32:37,370 --> 00:32:40,620
and that the kernel will run some other process,


475
00:32:40,620 --> 00:32:42,930
while we're waiting for data to appear in the pipe,


476
00:32:44,800 --> 00:32:47,800
so the times when yield is called in the kernel,


477
00:32:47,800 --> 00:32:48,940
there's really two main times,


478
00:32:48,940 --> 00:32:53,650
one is if a timer interrupt goes off , kernel always yields,


479
00:32:53,860 --> 00:32:56,760
you know just on the theory that,


480
00:32:57,640 --> 00:33:06,500
we should interleave the execution of of all the process that want to run on timer interrupt periods,


481
00:33:06,770 --> 00:33:09,020
so timer interrupt also always calls yield


482
00:33:09,410 --> 00:33:13,280
and whenever a process system calls waiting for IO,


483
00:33:13,370 --> 00:33:15,200
waiting for you to type the next keystroke,


484
00:33:15,200 --> 00:33:18,110
does a read of the console and you haven't typed key yet,


485
00:33:18,410 --> 00:33:25,180
then the the machinery to wait for IO calls yields, called from sleep,


486
00:33:25,420 --> 00:33:26,650
something, we'll talk about next week.


487
00:33:29,200 --> 00:33:32,950
Alright. Okay so.


488
00:33:32,950 --> 00:33:34,390
I have another question.


489
00:33:34,420 --> 00:33:35,020
Yes.


490
00:33:35,700 --> 00:33:37,530
Oh, if it is a sleep,


491
00:33:37,560 --> 00:33:42,660
is it gonna do the same thing roughly so it's gonna be some system call,


492
00:33:42,690 --> 00:33:44,520
there's gonna save the trapframe


493
00:33:44,790 --> 00:33:47,850
and then basically the same [picture],


494
00:33:47,850 --> 00:33:50,070
but it's just a,


495
00:33:50,100 --> 00:33:55,800
then the thing that made the process go into the kernel without a timer interrupt,


496
00:33:55,800 --> 00:33:57,350
but, the process's own decision.


497
00:33:57,900 --> 00:34:03,510
Yeah, so the process make there's a read system call


498
00:34:03,510 --> 00:34:04,710
and that's why it's in the kernel,


499
00:34:05,330 --> 00:34:09,500
and the read requires the process to wait for the disk,


500
00:34:10,010 --> 00:34:11,210
to do to finish reading


501
00:34:11,210 --> 00:34:13,220
or to wait for data to appear on a pipe,


502
00:34:13,310 --> 00:34:17,630
then actually the diagram is exactly the same as this, the ,


503
00:34:18,510 --> 00:34:20,910
and the kernel with a system called trapframe,


504
00:34:20,940 --> 00:34:22,200
all the saved user register,


505
00:34:22,200 --> 00:34:23,970
execute the system called the system whole rely,


506
00:34:24,000 --> 00:34:26,550
need to wait for the disk to finish reading something,


507
00:34:27,480 --> 00:34:31,770
the system call code will call sleep which ends up coding switch,


508
00:34:32,180 --> 00:34:38,150
which saves away the kernel thread registers in the process context


509
00:34:38,450 --> 00:34:41,090
switches to this current CPU scheduler,


510
00:34:41,090 --> 00:34:42,680
to let some other thread run,


511
00:34:42,710 --> 00:34:45,980
while this thread is waiting for the discrete to finish.


512
00:34:46,590 --> 00:34:48,690
So everything we're going to talk about now,


513
00:34:48,720 --> 00:34:53,100
except for the timer interrupt, is pretty much the same


514
00:34:53,100 --> 00:34:55,680
if what's going on is we're in a system called,


515
00:34:55,680 --> 00:34:58,470
the system call needs to wait for some for IO


516
00:34:58,680 --> 00:34:59,700
and give up the CPU,


517
00:35:02,250 --> 00:35:04,230
for the purposes of today's discussion,


518
00:35:04,230 --> 00:35:06,090
the two situations are almost identical.


519
00:35:08,970 --> 00:35:12,090
Okay, so the question does each per CPU scheduler has its own stack.


520
00:35:12,090 --> 00:35:15,030
Yes there's a stack,


521
00:35:18,600 --> 00:35:22,350
this scheduler stack for this separate stack,


522
00:35:23,730 --> 00:35:26,520
for scheduler for CPU one.


523
00:35:31,920 --> 00:35:35,010
Yeah, and indeed the stacks for this guy's also setup,


524
00:35:36,300 --> 00:35:41,910
in fact, all this stuff you know the context and the stacks with a scheduler threads


525
00:35:41,910 --> 00:35:45,060
are set up in a different way than for user process's.


526
00:35:46,450 --> 00:35:48,130
They're set up at boot time,


527
00:35:48,280 --> 00:35:52,660
if you poke around and start.S or start.c,


528
00:35:53,380 --> 00:35:59,020
start.S probably you'll see some of the setup for each core's scheduler thread,


529
00:35:59,020 --> 00:36:03,070
there's a place with stack very early in the assembly code during boot,


530
00:36:03,100 --> 00:36:06,220
where the stack is set up for each CPU


531
00:36:06,220 --> 00:36:09,460
and it's on that stack, that's a CPU boots on


532
00:36:09,640 --> 00:36:11,320
and then runs its scheduler thread.


533
00:36:15,590 --> 00:36:16,880
Okay, um.


534
00:36:18,930 --> 00:36:23,700
One piece of jargon a when people talk about context switch,


535
00:36:24,840 --> 00:36:32,690
they're talking about usually this act of switching


536
00:36:32,690 --> 00:36:37,040
from one thread to another by saving one set of register sets for the old thread


537
00:36:37,220 --> 00:36:40,730
and restoring previously saved registers for the threads were switching to,


538
00:36:41,600 --> 00:36:43,070
that's what's usually meant by context,


539
00:36:43,070 --> 00:36:47,700
which also sometimes it's applied to the complete [dance]


540
00:36:47,700 --> 00:36:52,770
that goes on when switching from one user process to another and occasionally you'll see context which apply to,


541
00:36:52,890 --> 00:36:54,810
switching between user and kernel,


542
00:36:54,930 --> 00:36:57,060
but for us we mostly mean it ,


543
00:36:58,260 --> 00:37:03,670
switching from one kernel thread typically to scheduler thread.


544
00:37:04,980 --> 00:37:08,130
Just some pieces of information,


545
00:37:10,100 --> 00:37:13,430
the [] [] to keep in mind,


546
00:37:13,610 --> 00:37:16,580
every core just does one thing at a time,


547
00:37:16,610 --> 00:37:21,320
each core you know is either is just running one thread at any given time,


548
00:37:21,320 --> 00:37:25,700
it's either running some processes user thread some process kernel thread


549
00:37:25,730 --> 00:37:27,890
or that core's scheduler thread,


550
00:37:28,190 --> 00:37:30,830
so at any given time the core is not doing multiple things,


551
00:37:30,830 --> 00:37:31,850
it's just doing one thing


552
00:37:31,850 --> 00:37:39,200
and it's this switching that sort of creates the illusion of multiple threads running at different times on that core,


553
00:37:39,620 --> 00:37:46,360
similarly, each thread is running on,


554
00:37:46,870 --> 00:37:50,320
is either running on exactly one core


555
00:37:50,740 --> 00:37:54,130
or its state has been state has been saved


556
00:37:54,130 --> 00:37:55,570
and we switched away from it.


557
00:37:56,280 --> 00:38:00,270
So, so a thread just to be clear thread never runs on more than one core,


558
00:38:00,270 --> 00:38:03,510
thread is either running on just one core or it's not running at all,


559
00:38:03,540 --> 00:38:05,310
as a save state somewhere.


560
00:38:06,610 --> 00:38:10,690
Another interesting thing about the xv6 setup is that


561
00:38:10,780 --> 00:38:17,190
these contexts that hold saved Kernel thread registers,


562
00:38:17,430 --> 00:38:20,280
they're always produced by a call to switch


563
00:38:20,670 --> 00:38:27,210
and so these contexts basically always refer to the state of the thread


564
00:38:27,240 --> 00:38:30,420
as it was executing inside a call to switch.


565
00:38:33,240 --> 00:38:35,820
And you know the way we'll see that come up is that


566
00:38:35,910 --> 00:38:40,950
when we switch from one to another and restore the target thread's context,


567
00:38:41,100 --> 00:38:44,340
the first thing it will do is return from a previous call


568
00:38:44,340 --> 00:38:47,700
to switch these contexts are always save state,


569
00:38:48,270 --> 00:38:50,160
in as it is in switch.


570
00:38:52,490 --> 00:38:53,180
Okay.


571
00:38:56,060 --> 00:39:00,320
Any more questions about the diagram level situation?


572
00:39:05,240 --> 00:39:08,900
I have a question, you are using the term thread all the time,


573
00:39:08,900 --> 00:39:12,500
but it seems to me like our implementation for xv6,


574
00:39:12,860 --> 00:39:15,710
process is if there's only one thread,


575
00:39:15,860 --> 00:39:19,460
so it could it be possible that one process could have multiple threads


576
00:39:19,460 --> 00:39:21,050
or am I wrong here.


577
00:39:21,740 --> 00:39:24,630
In xv6, right now.


578
00:39:26,030 --> 00:39:29,870
There's definitely some confusing things about the way we use the words here,


579
00:39:29,900 --> 00:39:34,150
in xv6, a process,


580
00:39:38,470 --> 00:39:43,930
a process is either executing instructions user level


581
00:39:44,080 --> 00:39:49,000
or it's executing instructions in the kernel


582
00:39:49,780 --> 00:39:52,900
or it's not executed at all


583
00:39:52,900 --> 00:39:59,440
and its state has been saved away into this combination of context and trapframe.


584
00:40:02,300 --> 00:40:03,920
So that's the actual situation,


585
00:40:03,980 --> 00:40:05,570
now what you want to call that,


586
00:40:09,060 --> 00:40:10,740
you call it what you like,


587
00:40:10,800 --> 00:40:13,950
I I don't know of a simple explanation for this structure,


588
00:40:14,160 --> 00:40:16,320
we've been calling it I've been calling it,


589
00:40:16,590 --> 00:40:20,640
I've been saying that each process has two threads,


590
00:40:21,100 --> 00:40:24,930
a user level thread, and a kernel level thread


591
00:40:25,080 --> 00:40:27,090
and that's a process there's this restriction


592
00:40:27,090 --> 00:40:28,860
that a process is only x is


593
00:40:28,860 --> 00:40:35,070
either executing in the kernel in the user space or executing in the kernel, inter- interrupt system call,


594
00:40:35,430 --> 00:40:36,240
but never both.


595
00:40:38,680 --> 00:40:39,820
That makes sense.


596
00:40:39,850 --> 00:40:43,780
Yeah, I apologize for the complexity of this.


597
00:40:47,540 --> 00:40:48,110
Okay.


598
00:40:48,710 --> 00:40:50,690
Okay, so let me switch to code,


599
00:40:50,690 --> 00:40:52,220
looking at the xv6 code.


600
00:41:01,130 --> 00:41:04,250
Right, so first of all,


601
00:41:10,090 --> 00:41:15,520
I just wanna just show some of the stuff we've been talking about,


602
00:41:15,520 --> 00:41:20,300
I'm going to look at the process structure.


603
00:41:20,970 --> 00:41:23,880
And we can see in the process structure, a lot of the things we've been talking about,


604
00:41:24,390 --> 00:41:27,360
just for review, there's the ,


605
00:41:30,760 --> 00:41:34,980
trapframe that saves the user level registers.


606
00:41:36,440 --> 00:41:43,810
There's a context here, that saves the kernel thread registers,


607
00:41:43,810 --> 00:41:46,000
we switch to the scheduler thread.


608
00:41:46,580 --> 00:41:51,320
There's a pointer to this process is kernel stack,


609
00:41:51,320 --> 00:41:55,490
which is where function calls are saved while we're executing in the kernel.


610
00:41:56,560 --> 00:41:58,780
There's the state variable,


611
00:41:58,780 --> 00:42:05,800
which records whether this process is running or runnable or sleeping or not allocated at all.


612
00:42:06,550 --> 00:42:13,500
And then finally, there's a lock that protects various things as we'll see ,


613
00:42:15,710 --> 00:42:22,670
for now , we can observe that at least protects changes to the state variable.


614
00:42:23,480 --> 00:42:26,360
So that for example to schedule threads,


615
00:42:26,360 --> 00:42:29,720
don't try to grab a runnable process and run it at the same time.


616
00:42:30,300 --> 00:42:32,970
One of the many things this lock does is prevent that from happening.


617
00:42:35,280 --> 00:42:40,680
I'm gonna run a , a simple demo program for you, the spin program.


618
00:42:41,460 --> 00:42:47,220
I'm using it mostly just to drive the sort of create a predictable situation


619
00:42:47,220 --> 00:42:49,710
in which we switch from one thread to another,


620
00:42:49,890 --> 00:42:54,840
but this is this program, spin program creates two processes


621
00:42:54,840 --> 00:42:56,850
and the processes both compute forever.


622
00:42:57,320 --> 00:42:59,570
You know call fork here,


623
00:43:00,320 --> 00:43:02,330
I make a child


624
00:43:02,420 --> 00:43:07,310
and then forever, both children both children just sit in this loop


625
00:43:07,310 --> 00:43:09,170
and everyone's while print a character just,


626
00:43:09,170 --> 00:43:10,760
so we can see they're making progress,


627
00:43:11,000 --> 00:43:13,340
but they don't print characters very often,


628
00:43:13,340 --> 00:43:17,570
and they never sort of intentionally give up the CPU,


629
00:43:17,600 --> 00:43:21,620
so what we have here is two, essentially two compute bound processes


630
00:43:21,740 --> 00:43:23,390
and in order for both of them to run,


631
00:43:23,390 --> 00:43:25,940
I'm gonna run them on a single CPU,


632
00:43:26,780 --> 00:43:29,660
xv6 that is only one core


633
00:43:29,720 --> 00:43:31,490
and so in order for both of them to execute,


634
00:43:31,610 --> 00:43:38,800
you know it's going to be necessary to to switching between the two processes.


635
00:43:40,440 --> 00:43:41,850
Let me fire up,


636
00:43:42,960 --> 00:43:45,600
spin program under gdb.


637
00:43:51,020 --> 00:43:53,180
Around the spin program and you can see it's printing,


638
00:43:53,210 --> 00:43:56,720
one of the two processes prints forward slash


639
00:43:56,720 --> 00:43:58,400
and the other prints backward slash


640
00:43:58,400 --> 00:44:00,500
and you can see that every once in a while,


641
00:44:00,740 --> 00:44:03,170
xv6 is switching between them


642
00:44:03,170 --> 00:44:05,270
and only has one core the way I've configured it,


643
00:44:05,480 --> 00:44:08,960
so we see a bunch of forward slashes printing


644
00:44:08,960 --> 00:44:11,000
and then apparently a timer interrupt,


645
00:44:11,000 --> 00:44:15,410
most go off switch, the one CPU to the other process


646
00:44:15,410 --> 00:44:17,930
and then prints the other kind of slash for a while,


647
00:44:17,990 --> 00:44:20,660
so what I want to observe is the timer going off,


648
00:44:20,810 --> 00:44:24,630
so I'm gonna put a break point in trap,


649
00:44:26,940 --> 00:44:30,360
and in particular at line 207 in trap,


650
00:44:35,060 --> 00:44:45,240
which is a code in trap in devintr,


651
00:44:45,240 --> 00:44:49,320
that recognizes that we're in interrupt


652
00:44:49,320 --> 00:44:52,050
and the interrupt was caused by a timer interrupt.


653
00:44:53,430 --> 00:44:59,880
So I put a breakpoint here, at trap.c 207


654
00:44:59,880 --> 00:45:04,080
and continue boom the trap I triggers right away,


655
00:45:04,080 --> 00:45:05,850
because timer apps are pretty frequent


656
00:45:06,150 --> 00:45:08,700
and we can tell from where that indeed when user trap


657
00:45:08,700 --> 00:45:12,930
and user trap has called devintr to handle this interrupt.


658
00:45:13,480 --> 00:45:18,400
I wanna take finish to get out of devintr back into user trap.


659
00:45:20,420 --> 00:45:21,230
Because, in fact we don't,


660
00:45:22,640 --> 00:45:25,250
the code devintr [into timer] or you know says almost nothing.


661
00:45:27,340 --> 00:45:34,810
However, once we're back at in user trap,


662
00:45:36,200 --> 00:45:39,440
we can see that from this line here,


663
00:45:39,440 --> 00:45:41,630
that we just returned from devintr.


664
00:45:47,860 --> 00:45:52,930
And the interesting thing about this is that ,


665
00:45:54,340 --> 00:45:56,020
what we're about to do,


666
00:45:56,530 --> 00:45:59,020
I'm looking forward we're currently at this line here


667
00:45:59,140 --> 00:46:03,010
and we're looking forward to this called the yield,


668
00:46:03,730 --> 00:46:07,300
when devintr return to you can see from this,


669
00:46:07,450 --> 00:46:08,980
you return is two,


670
00:46:08,980 --> 00:46:11,140
two is basically the device number


671
00:46:11,320 --> 00:46:13,450
and we're going to see that by and by,


672
00:46:13,480 --> 00:46:18,820
because which devices to user traps going to call yield,


673
00:46:18,820 --> 00:46:22,320
which go to CPU and allows pushing the process,


674
00:46:22,770 --> 00:46:23,700
you'll see that in a moment,


675
00:46:23,910 --> 00:46:27,960
meantime let's look at what was currently executing when the interrupt happened.


676
00:46:28,140 --> 00:46:29,820
I'm going to print p,


677
00:46:31,010 --> 00:46:35,960
the variable p holds a pointer to the current processes struct proc.


678
00:46:38,890 --> 00:46:42,790
Okay, the question what makes each processes kernel thread different.


679
00:46:43,260 --> 00:46:47,310
Every process has a separate kernel thread,


680
00:46:47,930 --> 00:46:52,490
there's really two things that differentiate different processes kernel thread,


681
00:46:52,490 --> 00:46:55,670
because more than one could be executing on different cores.


682
00:46:57,650 --> 00:47:01,940
One is indeed that every process has a separate kernel stack


683
00:47:01,940 --> 00:47:05,990
and that's what's pointed to by that kstack element of struct proc


684
00:47:06,470 --> 00:47:08,150
and the other is that,


685
00:47:13,490 --> 00:47:22,720
early in, when user trap which is you know the C code is called, by trampoline, when interrupt occurs.


686
00:47:25,640 --> 00:47:29,630
We can tell by this call them by any, any kernel code,


687
00:47:29,630 --> 00:47:36,050
can tell by calling myproc what the processes is running on the current CPU.


688
00:47:36,620 --> 00:47:39,290
And that's another thing that differentiates ,


689
00:47:40,500 --> 00:47:43,500
that allows each that allows kernel code to tell


690
00:47:43,500 --> 00:47:47,850
what process it's part of that is which processes kernel thread is executing


691
00:47:47,850 --> 00:47:50,640
and what myproc does basically use the tp register


692
00:47:50,640 --> 00:47:58,300
which you may recall, is set up to contain the current cores hartid or core number,


693
00:47:58,360 --> 00:48:01,000
it uses that to index into an array of structures,


694
00:48:01,000 --> 00:48:04,270
that say for each core that the scheduler sets


695
00:48:04,300 --> 00:48:07,420
whenever it switches processes to indicate for each core


696
00:48:07,420 --> 00:48:09,490
which process is running on that core.


697
00:48:10,660 --> 00:48:13,720
And so that's how different kernels are differentiated.


698
00:48:15,730 --> 00:48:17,650
Okay, so I was going to use that p value,


699
00:48:17,710 --> 00:48:21,910
the name and that p value to figure out what process is running,


700
00:48:21,910 --> 00:48:26,680
xv6 remembers the name it's that spin process just exactly as expected,


701
00:48:26,920 --> 00:48:31,570
there were two of them, I think with process ids three and four.


702
00:48:32,150 --> 00:48:32,780
Oops.


703
00:48:34,680 --> 00:48:37,260
We're currently executed again process id three,


704
00:48:37,440 --> 00:48:41,400
so after the switch we'd expect to be in process id four,


705
00:48:41,430 --> 00:48:42,900
the other spin process,


706
00:48:43,170 --> 00:48:48,390
how can we we can look at the saved user registers in the trapframe.


707
00:48:54,600 --> 00:49:01,890
And these are the 32 registers that a trampoline code saves a way to save the user state,


708
00:49:02,130 --> 00:49:08,130
there's the user ra return address register user stack pointer ,


709
00:49:08,520 --> 00:49:11,070
user program counter at hex 62.


710
00:49:12,050 --> 00:49:16,580
These are familiar things from when we looked at traps.


711
00:49:17,290 --> 00:49:20,170
And you may be the most interest is that ,


712
00:49:23,290 --> 00:49:27,040
the trapframe saves the user program counter and that value 62,


713
00:49:28,090 --> 00:49:35,620
if we cared, we can look in the assembly code for spin.c.


714
00:49:36,540 --> 00:49:40,160
Oops, spin.asm look for 62,


715
00:49:41,100 --> 00:49:44,190
now we can see that owes a interrupt timer,


716
00:49:44,190 --> 00:49:50,490
interrupt occurred during this add instruction in that infinite loop in spin,


717
00:49:50,700 --> 00:49:51,900
so it's not too surprising.


718
00:49:55,390 --> 00:50:01,630
Okay, so back to a trap code [seven] just returned,


719
00:50:01,780 --> 00:50:06,700
I'm going to take step a few times to get us to the,


720
00:50:09,130 --> 00:50:11,590
just being about to execute this yield.


721
00:50:12,260 --> 00:50:14,480
And yield is sort of the first step in the process


722
00:50:14,480 --> 00:50:17,090
of giving up the CPU switching to the scheduler,


723
00:50:17,090 --> 00:50:20,840
letting the scheduler choose another kernel thread in process to run.


724
00:50:24,240 --> 00:50:27,000
Alright, so let's actually step into yield,


725
00:50:27,480 --> 00:50:28,710
now we're yields.


726
00:50:30,110 --> 00:50:30,980
If you have a question.


727
00:50:31,580 --> 00:50:31,910
NO.


728
00:50:43,450 --> 00:50:46,840
Okay, we're yield ,


729
00:50:47,500 --> 00:50:49,180
yield does a couple of things,


730
00:50:49,180 --> 00:50:54,370
it a acquires the lock for this process,


731
00:50:54,400 --> 00:50:57,070
because it's about to make a bunch of changes to this process,


732
00:50:57,070 --> 00:50:58,960
and it doesn't want any other


733
00:50:59,140 --> 00:51:04,450
and in fact until it gives up the lock, the state of this process will be sort of inconsistent


734
00:51:04,540 --> 00:51:09,460
like for example, it's about yield is about to change the state of the process to runnable,


735
00:51:09,460 --> 00:51:12,010
which would you know indicates that,


736
00:51:12,420 --> 00:51:14,640
the process is not running, but would like to,


737
00:51:14,940 --> 00:51:17,550
but this process is running right,


738
00:51:17,640 --> 00:51:19,290
I mean we're running the process right now,


739
00:51:19,290 --> 00:51:21,870
that's what's executing is the kernel thread for this process


740
00:51:22,020 --> 00:51:25,530
and so the one of the many things that acquire this lock does is,


741
00:51:25,890 --> 00:51:29,070
makes it so that even though we just changed the state runnable,


742
00:51:29,100 --> 00:51:33,430
no other cores scheduling thread will look at this process


743
00:51:33,580 --> 00:51:35,350
and because of lock


744
00:51:35,560 --> 00:51:38,020
and see that it's runnable and try to run it


745
00:51:38,290 --> 00:51:41,200
while we're still running it on this core would be a disaster right,


746
00:51:41,200 --> 00:51:44,770
running the same process onto different cores,


747
00:51:44,800 --> 00:51:46,780
you know process has only one stack,


748
00:51:47,050 --> 00:51:51,070
so that means like two different cores are calling [subroutines] on the same stack,


749
00:51:51,070 --> 00:51:53,920
which is just a recipe for disaster.


750
00:51:54,680 --> 00:51:56,750
So we take the lock out,


751
00:51:59,380 --> 00:52:03,430
we, yield changes the state to runnable .


752
00:52:04,130 --> 00:52:05,300
And what this means is that,


753
00:52:05,420 --> 00:52:08,290
you know we finally given up the.


754
00:52:11,960 --> 00:52:13,550
When we finally yield the CPU


755
00:52:13,550 --> 00:52:15,470
and get up and switch the scheduler process,


756
00:52:15,470 --> 00:52:17,450
this state will be left in this runnable state,


757
00:52:17,450 --> 00:52:18,710
so that it will run again,


758
00:52:18,980 --> 00:52:20,750
because after all this was a timer interrupt,


759
00:52:20,750 --> 00:52:22,970
that interrupted a running user level process,


760
00:52:22,970 --> 00:52:24,590
that would like to continue computing.


761
00:52:25,790 --> 00:52:27,140
We're gonna leave state runnable,


762
00:52:27,140 --> 00:52:30,440
so that it will run again, as soon as the scheduler decides to.


763
00:52:33,290 --> 00:52:37,670
And then um the only other thing that,


764
00:52:44,090 --> 00:52:46,610
yield does is call this scheduler function.


765
00:52:48,580 --> 00:52:50,380
So I'm going to step into the scheduler function.


766
00:52:51,210 --> 00:52:53,190
I'll show this whole thing here.


767
00:53:01,470 --> 00:53:04,590
This scheduler is something does almost nothing,


768
00:53:04,590 --> 00:53:05,790
it has a bunch of checks,


769
00:53:05,880 --> 00:53:09,360
it does a whole bunch of [sanity] checks and panics


770
00:53:09,360 --> 00:53:11,220
and the reason for that is actually that,


771
00:53:13,550 --> 00:53:20,840
this code in xv6 over its many your lifetime had been among the most bug [prone]


772
00:53:20,960 --> 00:53:24,830
and have most surprises unhappy surprises,


773
00:53:24,830 --> 00:53:28,650
so there's a lot of sanity checks and panics here,


774
00:53:28,650 --> 00:53:33,250
because, because there's often bugs associated with this code.


775
00:53:35,650 --> 00:53:40,540
Alright, I'm gonna skip over these sanity checks


776
00:53:40,540 --> 00:53:43,510
and proceed to the,


777
00:53:46,850 --> 00:53:49,670
called to switch, this called to switch is where the real action happens,


778
00:53:49,670 --> 00:53:57,330
this is called the switch is going to save away the current kernel threads registers in p->content,


779
00:53:57,330 --> 00:54:01,950
which is the current processes saved kernel thread context save set of registers,


780
00:54:02,640 --> 00:54:08,760
c arrow context, c is the pointer to this core's struct cpu,


781
00:54:09,090 --> 00:54:16,490
struct cpu has context to save registers of this core's scheduler threads,


782
00:54:16,490 --> 00:54:18,140
we're going to be switching from this thread


783
00:54:18,140 --> 00:54:24,140
and saving this thread state restoring the thread state of this core's scheduler


784
00:54:24,140 --> 00:54:29,990
and continuing the execution of this core's core's scheduler thread.


785
00:54:32,760 --> 00:54:34,200
Okay, so let's see what,


786
00:54:36,250 --> 00:54:40,990
let's take a quick preview at the context


787
00:54:40,990 --> 00:54:42,550
that we're going to be switching to


788
00:54:43,030 --> 00:54:44,680
and I can get that turns out that,


789
00:54:45,860 --> 00:54:47,810
can't actually print c->context,


790
00:54:47,810 --> 00:54:52,340
but I happen to know that c prints to cpus zero,


791
00:54:52,430 --> 00:54:54,560
just because we're on the zero with core,


792
00:54:54,560 --> 00:54:55,550
there's only one core,


793
00:54:55,820 --> 00:54:57,950
and I can print its context.


794
00:55:00,960 --> 00:55:08,610
And so this is the saved registers from this core's, scheduler thread.


795
00:55:10,560 --> 00:55:12,510
And of particular interest is the ra,


796
00:55:12,510 --> 00:55:16,050
because the ra register I is where,


797
00:55:16,600 --> 00:55:19,150
the current function call is going to return to,


798
00:55:19,150 --> 00:55:20,770
so we're going to switch the scheduler thread,


799
00:55:20,770 --> 00:55:24,070
it's going to do return and return to that ra.


800
00:55:24,930 --> 00:55:32,500
And my, we can find out where that where that return address by looking in kernel.asm.


801
00:55:35,000 --> 00:55:36,050
Actually that's .


802
00:55:43,800 --> 00:55:45,930
And as you can see this x/i,


803
00:55:45,930 --> 00:55:48,750
you know prints instructions that at a certain address,


804
00:55:48,750 --> 00:55:53,460
but it also prints the label of the name of the function


805
00:55:53,460 --> 00:55:54,600
that those instructions are in it,


806
00:55:54,600 --> 00:55:58,140
so we're going to be returning to scheduler by and by,


807
00:55:58,620 --> 00:56:01,350
that's just you know as you might expect.


808
00:56:05,040 --> 00:56:05,460
Okay.


809
00:56:11,120 --> 00:56:14,720
I want to look at what switch actually does about to call switch.


810
00:56:18,910 --> 00:56:20,140
So I put a breakpoint on switch


811
00:56:20,140 --> 00:56:20,830
and putting a breakpoint,


812
00:56:20,830 --> 00:56:22,330
because there's a bunch of setup code


813
00:56:22,540 --> 00:56:26,710
that pulls the values of context out of those structures,


814
00:56:26,710 --> 00:56:27,580
I'll skip over it.


815
00:56:28,820 --> 00:56:30,920
Okay, so now ,


816
00:56:31,520 --> 00:56:32,870
when a breakpoint and switch,


817
00:56:33,230 --> 00:56:36,250
the gdb won't show us the instructions,


818
00:56:36,250 --> 00:56:40,690
but we can look at switch.S to look at the instructions were about to execute.


819
00:56:41,350 --> 00:56:43,210
So as you can see we're on the very first instruction,


820
00:56:43,210 --> 00:56:48,340
the store of ra to the address pointed to by a0,


821
00:56:48,640 --> 00:56:50,890
you may remember in the call to switch,


822
00:56:50,890 --> 00:56:54,430
that the first argument was the current thread's context


823
00:56:54,430 --> 00:56:58,390
and the second argument was the context of the thread we're switching to,


824
00:56:58,420 --> 00:57:00,610
the two arguments going a0 and a1


825
00:57:01,000 --> 00:57:05,230
and so the reason why we see all these stores through register a0


826
00:57:05,230 --> 00:57:08,590
is because we're storing away a bunch of registers in the memory


827
00:57:08,590 --> 00:57:13,000
that a0 points to that is in the context of the thread we're switching from


828
00:57:13,300 --> 00:57:15,850
and the loads load from address a1,


829
00:57:15,850 --> 00:57:19,720
because that's a pointer to the context of the thread, we're switching to.


830
00:57:26,740 --> 00:57:37,000
Okay, thread, you know, switch saves registers, loads registers from the target thread's context and then return


831
00:57:37,510 --> 00:57:39,550
and that's why the ra was interesting,


832
00:57:39,550 --> 00:57:43,180
because it's going to return to the place that ra pointed you namely into scheduler.


833
00:57:44,290 --> 00:57:51,040
Alright, so one question is you may notice here that while switch saves ra sp and a bunch of [s] registers,


834
00:57:51,070 --> 00:57:53,860
one that does not save is the program counter,


835
00:57:54,360 --> 00:57:57,150
there's no mention of the program counter here ,


836
00:57:57,660 --> 00:57:58,710
so why is that.


837
00:58:04,650 --> 00:58:10,380
Is it because the program counter is a updated with like the function calls anyway.


838
00:58:10,980 --> 00:58:15,840
Yeah it's it's the program counter, there's no actual information value in the program counter,


839
00:58:15,840 --> 00:58:20,320
we know that we're executing right now is in switch, right.


840
00:58:20,350 --> 00:58:22,780
So there be no point in saving the program counter,


841
00:58:22,780 --> 00:58:27,040
because it has an extremely predictable value namely this instruction,


842
00:58:27,040 --> 00:58:28,900
the address of this instruction and switch


843
00:58:29,890 --> 00:58:33,070
what we really care about is where we we're called from,


844
00:58:33,370 --> 00:58:35,410
because when we switch back to this thread,


845
00:58:35,980 --> 00:58:39,580
we want to continue executing out whatever points swhich was called from


846
00:58:39,730 --> 00:58:44,740
and it's ra that holds the address of the instruction that switch was called from.


847
00:58:45,320 --> 00:58:48,590
So it's ra, that's being saved away here,


848
00:58:48,980 --> 00:58:55,140
and ra is the point at which will be executing out again.


849
00:58:55,790 --> 00:58:58,040
Let's switch return, so we even print that we can print,


850
00:58:58,930 --> 00:59:01,120
ra, oops.


851
00:59:03,740 --> 00:59:06,680
We can print ra and you know we haven't actually switched threads yet,


852
00:59:06,770 --> 00:59:09,260
you remember we came here from this sched function,


853
00:59:09,710 --> 00:59:13,640
so ra as you might expect the pointer back into this sched function,


854
00:59:14,960 --> 00:59:19,520
another question is how come switch only saves fourteen registers I counted them,


855
00:59:19,550 --> 00:59:25,520
it only saves and restores fourteen registers even though the RISC-V has 32 registers


856
00:59:25,520 --> 00:59:29,300
available for the or use it for code to use.


857
00:59:29,820 --> 00:59:30,750
Why?


858
00:59:31,740 --> 00:59:33,390
Why only half the registers are saved?


859
00:59:33,780 --> 00:59:36,960
Well when switch was called, it was called as a normal function,


860
00:59:36,960 --> 00:59:40,560
so whoever called switch already assumed will switch might modify those,


861
00:59:40,560 --> 00:59:44,490
so that that function already saved that on its stack,


862
00:59:44,520 --> 00:59:49,020
meaning that like when we jump from one to the other ,


863
00:59:49,020 --> 00:59:54,030
that one's gonna self restore its caller save registers.


864
00:59:54,090 --> 00:59:58,110
That's exactly right, this switches are called from C code,


865
00:59:58,170 --> 01:00:04,050
we know that the C compiler saves on the current stack,


866
01:00:04,140 --> 01:00:08,460
any caller saved registers that have values in them,


867
01:00:08,460 --> 01:00:10,020
that the compiler's going to need later.


868
01:00:10,590 --> 01:00:14,730
And those caller saved registers actually include,


869
01:00:15,030 --> 01:00:18,680
I think there's eighteen,


870
01:00:19,010 --> 01:00:20,240
depending on how you count them,


871
01:00:20,240 --> 01:00:24,020
there's somewhere between fifteen and eighteen caller saved registers,


872
01:00:24,980 --> 01:00:29,750
so the registers we see here are all the registers that aren't caller saved


873
01:00:29,750 --> 01:00:32,150
and that the compiler [does] promise to save,


874
01:00:32,180 --> 01:00:37,640
but nevertheless may hold values that are needed by the calling function,


875
01:00:37,670 --> 01:00:42,530
so we all need to save the callee saved registers when we're switching threads.


876
01:00:45,830 --> 01:00:47,150
Okay .


877
01:00:48,340 --> 01:00:50,830
Final thing I want to print is the,


878
01:00:50,890 --> 01:00:52,960
we do save and restore the stack pointer,


879
01:00:53,080 --> 01:00:55,690
the current stack pointer, it's like hard to tell from this value,


880
01:00:55,690 --> 01:00:59,350
what that means but it's the kernel stack of the current process,


881
01:00:59,350 --> 01:01:01,660
which I don't know if you recall,


882
01:01:01,660 --> 01:01:05,380
but is allocated is mapped by the virtual memory system at high memory.


883
01:01:07,340 --> 01:01:20,240
Okay, so, okay, so we're going to save away the current registers and restore registers from scheduler thread's context ,


884
01:01:20,240 --> 01:01:23,660
I don't want to execute every single one of these loads or store,


885
01:01:23,660 --> 01:01:25,190
so I'm gonna step over,


886
01:01:25,580 --> 01:01:29,360
all the fourteen loads, the fourteen stores, and the fourteen loads,


887
01:01:29,360 --> 01:01:32,900
going to proceed directly to the return instructions.


888
01:01:32,900 --> 01:01:35,900
Okay, so we executed everything in switch except the return,


889
01:01:36,170 --> 01:01:41,000
before we do the return, we'll just print the interesting registers again,


890
01:01:41,000 --> 01:01:41,750
to see where we are.


891
01:01:41,780 --> 01:01:46,610
So stack pointer , now it has a different value,


892
01:01:46,820 --> 01:01:51,170
stack pointer now points into this stack zero area in memory,


893
01:01:51,170 --> 01:01:55,580
and this is actually the place it very very early in the boot sequence


894
01:01:55,640 --> 01:02:01,310
where start.S puts the stack so it may call the very first C function,


895
01:02:01,610 --> 01:02:04,520
so actually back on the original boot stack for this CPU


896
01:02:04,520 --> 01:02:07,820
which just happens to be where the scheduler runs.


897
01:02:10,500 --> 01:02:14,460
Okay, the program counter after interesting,


898
01:02:14,460 --> 01:02:16,260
we're in switch because we haven't returned yet


899
01:02:16,350 --> 01:02:21,240
and the ra register now points the scheduler,


900
01:02:21,240 --> 01:02:27,450
because we've loaded, we've restored the register set previously saved by the scheduler thread.


901
01:02:28,990 --> 01:02:31,540
And indeed, we're really now in the scheduler thread,


902
01:02:31,540 --> 01:02:35,680
if I were on where the where now looks totally different from the last time we ran,


903
01:02:35,680 --> 01:02:38,260
it were now indeed a call to switch,


904
01:02:38,260 --> 01:02:43,990
but now we're in a call from switch to switch the scheduler made at some point in the past


905
01:02:44,290 --> 01:02:46,750
and the scheduler was run long ago during boot


906
01:02:46,990 --> 01:02:50,860
was called as the last thing that [] did during the boot process.


907
01:02:53,310 --> 01:02:58,500
So I'm gonna ask you one instruction to return from switch now into scheduler.


908
01:02:59,890 --> 01:03:03,250
So now we're in this core's scheduler, look the full code,


909
01:03:08,320 --> 01:03:10,810
so this is the scheduler code ,


910
01:03:11,480 --> 01:03:12,710
this function called scheduler,


911
01:03:12,710 --> 01:03:14,720
now we're executing in the scheduler thread,


912
01:03:14,780 --> 01:03:23,060
for the CPU and we're just at the point we just returned from a previous call to switch


913
01:03:23,060 --> 01:03:24,920
the scheduler made a while ago,


914
01:03:25,250 --> 01:03:28,370
when it decided it was going to start running that process,


915
01:03:28,370 --> 01:03:32,180
you know pid three which was the spin process that was interrupted,


916
01:03:32,750 --> 01:03:37,070
so now it's this switch process id three,


917
01:03:37,070 --> 01:03:40,550
that's spin called switch but it's not switch that switch this returning


918
01:03:40,550 --> 01:03:41,810
last which hasn't returned yet


919
01:03:42,080 --> 01:03:48,310
is still saved away in process id three stack and context


920
01:03:48,340 --> 01:03:50,770
just return from this earlier called switch.


921
01:03:53,160 --> 01:03:55,530
Alright, so the stuff that happens here in the scheduler,


922
01:03:55,740 --> 01:03:59,820
were stopped running this process


923
01:03:59,820 --> 01:04:05,790
and so you want to forget about the various things we did in the process of


924
01:04:05,820 --> 01:04:09,600
running this process, we want to forget the c->proc equals zero,


925
01:04:09,660 --> 01:04:11,310
basically means that we're forgetting that.


926
01:04:11,970 --> 01:04:14,310
We're no longer running this process in this core's,


927
01:04:14,310 --> 01:04:17,670
so we don't want to have anybody be confused about that,


928
01:04:17,700 --> 01:04:23,480
let me set this per core proc pointer to zero instead this process,


929
01:04:23,900 --> 01:04:27,620
the next thing that happens is that you remember yield,


930
01:04:28,160 --> 01:04:30,260
acquired the lock for this process,


931
01:04:30,320 --> 01:04:32,300
because it didn't want any other core's scheduler


932
01:04:32,300 --> 01:04:34,220
or to look at this process and maybe run it


933
01:04:34,760 --> 01:04:37,970
until the process was completely put to sleep.


934
01:04:39,290 --> 01:04:41,930
We've now completed the switch away from this process,


935
01:04:42,140 --> 01:04:45,710
so we can release the lock on the process that just yielded.


936
01:04:46,880 --> 01:04:52,910
That's the release, at this point, we're still in the scheduler,


937
01:04:52,910 --> 01:04:55,070
if there was another core at this point,


938
01:04:55,070 --> 01:04:58,730
some other core's scheduler could find that process,


939
01:04:58,730 --> 01:05:00,170
because it's runnable and run it.


940
01:05:00,700 --> 01:05:01,360
But that's okay,


941
01:05:01,360 --> 01:05:07,540
because we've completely saved its registers were no longer executing on its that processes stack,


942
01:05:07,540 --> 01:05:12,050
because now executing on the this core's scheduler stack,


943
01:05:12,290 --> 01:05:15,830
so it's actually fine if some other core decides to run that process.


944
01:05:16,840 --> 01:05:21,040
Okay, but there's no other core, so that doesn't actually happen in this demonstration.


945
01:05:26,440 --> 01:05:31,240
Actually, I want to spend a moment talking about the p->lock a little bit more,


946
01:05:31,510 --> 01:05:36,190
p->lock actually does a couple of things.


947
01:05:39,120 --> 01:05:42,470
It does really two things from the point of view of scheduling,


948
01:05:42,500 --> 01:05:45,290
one is that yielding the CPU,


949
01:05:45,650 --> 01:05:48,410
involves multiple steps we have to set the state


950
01:05:48,410 --> 01:05:50,930
to run up change the state from running to runnable,


951
01:05:51,080 --> 01:05:55,460
we save the registers in the yielding processes context,


952
01:05:55,550 --> 01:05:58,730
now we have to stop using the yielding processes stack,


953
01:05:58,760 --> 01:06:01,910
there's at least three steps which take time,


954
01:06:02,510 --> 01:06:07,640
in order to do all the steps required to yield the CPU


955
01:06:07,910 --> 01:06:10,100
and so one of the things that lock does


956
01:06:10,130 --> 01:06:13,940
as I mentioned is prevent any other core scheduler from looking at our process


957
01:06:13,940 --> 01:06:16,070
until all three steps have completed,


958
01:06:16,070 --> 01:06:19,580
so the lock is basically making those steps atomic,


959
01:06:19,860 --> 01:06:22,680
they either all happened from the point of view of other cores,


960
01:06:22,800 --> 01:06:24,900
or none of them happened.


961
01:06:26,160 --> 01:06:30,030
It's going to turn out also when we start running a process


962
01:06:30,270 --> 01:06:35,490
that the p->lock is going to have a similar protective function,


963
01:06:36,750 --> 01:06:39,210
we're going to set the state of a process to running


964
01:06:39,210 --> 01:06:40,800
when we start executing a process


965
01:06:40,800 --> 01:06:46,260
and we're going to move its registers from its process context into the RISC-V registers,


966
01:06:46,320 --> 01:06:51,910
but, if an interrupt should happen in the middle of that process,


967
01:06:51,910 --> 01:06:54,220
the interrupt going to see the process in a weird state


968
01:06:54,220 --> 01:06:56,560
like maybe in the state of mark running,


969
01:06:56,560 --> 01:07:01,960
but hasn't yet finished moving its registers from the context into the RISC-V registers,


970
01:07:02,170 --> 01:07:03,250
and that would be a disaster,


971
01:07:03,250 --> 01:07:04,990
if a timer interrupt happened then,


972
01:07:04,990 --> 01:07:09,880
because we might switch away from that process before it had restored its registers.


973
01:07:11,220 --> 01:07:13,680
And switching away from that process would save,


974
01:07:13,710 --> 01:07:18,690
now an initialized RISC-V registers into the context processes context,


975
01:07:18,780 --> 01:07:20,520
overwriting its real registers.


976
01:07:20,940 --> 01:07:25,620
So indeed we want starting a process to also be effectively atomic,


977
01:07:26,430 --> 01:07:29,670
and in this case holding a lock holding p->lock


978
01:07:29,670 --> 01:07:35,220
across switching to a process as well as preventing other cores from looking at that process


979
01:07:35,220 --> 01:07:41,100
also turns off interrupts for the duration of [firing] up of switching to that thread


980
01:07:41,190 --> 01:07:44,670
which prevents a timer interrupt from ever seeing a process,


981
01:07:44,670 --> 01:07:48,030
that's only midway through being switched to.


982
01:07:51,320 --> 01:07:51,800
Okay.


983
01:07:53,670 --> 01:07:55,470
So we're in the scheduler,


984
01:07:55,770 --> 01:07:58,680
we're executing this loop in the scheduler's loop in the scheduler,


985
01:07:58,680 --> 01:08:01,500
that looks at all the process in turn to find one to run


986
01:08:01,860 --> 01:08:04,590
and in this case, we know there's another process,


987
01:08:04,590 --> 01:08:08,870
because , there's that other spin process that we've forked,


988
01:08:09,170 --> 01:08:13,070
but there's a lot of process lots to examine,


989
01:08:13,460 --> 01:08:18,230
I'm going to skip over the actual process scanning of the process table


990
01:08:18,320 --> 01:08:22,390
and go direct to the point of which, the scheduler finds the next process,


991
01:08:22,390 --> 01:08:28,270
so I'm gonna put a breakpoint at line four seventy four,


992
01:08:28,270 --> 01:08:29,950
where it's actually found a new process to run.


993
01:08:31,920 --> 01:08:37,740
But here we are, the schedulers scan the process table and found another process to run.


994
01:08:38,840 --> 01:08:41,990
And it's going to call that process run,


995
01:08:41,990 --> 01:08:43,670
you can see a line 468,


996
01:08:43,670 --> 01:08:45,800
it acquired that processes lock,


997
01:08:45,830 --> 01:08:48,530
so now it's entitled to do the various steps


998
01:08:48,530 --> 01:08:50,510
were required to switch to that process,


999
01:08:50,930 --> 01:08:54,860
in line 473, it set the process of state to running,


1000
01:08:55,440 --> 01:08:56,550
it's now at 474,


1001
01:08:56,550 --> 01:09:01,680
we're going to record in the CPU structure which process the CPU is executing


1002
01:09:02,730 --> 01:09:06,990
and then call switch to save the scheduler's registers


1003
01:09:06,990 --> 01:09:10,440
and restore the target processor's registers,


1004
01:09:10,500 --> 01:09:12,030
so you can see what process is found


1005
01:09:12,030 --> 01:09:15,060
by looking at a new process name,


1006
01:09:15,150 --> 01:09:16,590
surprisingly its spin,


1007
01:09:17,590 --> 01:09:19,330
it's process id is now four,


1008
01:09:19,540 --> 01:09:21,670
used to be running three now running four.


1009
01:09:23,960 --> 01:09:25,580
We've already set the state to running,


1010
01:09:25,580 --> 01:09:31,790
so just the states running .


1011
01:09:32,550 --> 01:09:35,400
We can see where this thread is going to switch to


1012
01:09:35,430 --> 01:09:37,770
in the call to switch line 475,


1013
01:09:39,020 --> 01:09:42,050
print this context the saved registers,


1014
01:09:42,470 --> 01:09:44,600
so where is the ra,


1015
01:09:45,140 --> 01:09:46,280
we're going to call switch,


1016
01:09:46,280 --> 01:09:49,340
but switch as we know it returns,


1017
01:09:49,520 --> 01:09:52,430
when it returns it returns to the restored ra,


1018
01:09:52,550 --> 01:09:57,080
so we really care about is where is it that a ra points to,


1019
01:09:57,080 --> 01:09:58,820
we can find that out by.


1020
01:10:00,740 --> 01:10:01,310
Oops.


1021
01:10:02,100 --> 01:10:04,290
Using x/i,


1022
01:10:07,440 --> 01:10:10,500
it's going to return ra points to some point instead,


1023
01:10:10,530 --> 01:10:11,640
that's not too surprising,


1024
01:10:11,640 --> 01:10:18,210
since presumably that other spin process was suspended due to a timer interrupt,


1025
01:10:18,210 --> 01:10:21,930
which as we know called sched, what's called switch.


1026
01:10:26,450 --> 01:10:31,730
Alright, so about to call switch, let me just bring up the switch code again.


1027
01:10:38,660 --> 01:10:40,790
Actually enter switch were still,


1028
01:10:41,390 --> 01:10:44,450
where shows that were still in the scheduler's context.


1029
01:10:46,940 --> 01:10:50,540
I want to again execute all of the instructions to switch this time,


1030
01:10:50,540 --> 01:10:53,660
switching from the scheduler to the new process.


1031
01:10:54,400 --> 01:10:57,250
We skip over the 28 stores and loads.


1032
01:11:00,460 --> 01:11:04,300
Just convince ourselves that we are actually about to return to sched,


1033
01:11:04,300 --> 01:11:07,270
so now since we're about to return to sched and not scheduler ,


1034
01:11:07,390 --> 01:11:11,020
we must now be in a process's kernel thread and no longer.


1035
01:11:12,980 --> 01:11:16,070
The scheduler of thread in indeed if we look at the backtrace,


1036
01:11:16,190 --> 01:11:19,790
we had a user trap call that must have been a timer interrupt,


1037
01:11:19,880 --> 01:11:21,860
from long you know sometime in the past,


1038
01:11:22,310 --> 01:11:24,860
that as we've seen called yield and sched,


1039
01:11:24,860 --> 01:11:27,470
but it was the timer interrupted the other process now,


1040
01:11:27,860 --> 01:11:30,260
not in the process that we originally looked at.


1041
01:11:35,600 --> 01:11:39,320
Okay, any questions about,


1042
01:11:39,380 --> 01:11:43,010
I think I'm gonna leave off stepping through the code at this point,


1043
01:11:43,580 --> 01:11:47,330
any questions about any of the material we've seen.


1044
01:11:49,790 --> 01:11:53,540
Oh, sorry if it was a for example this [],


1045
01:11:53,810 --> 01:11:59,300
then we would see that ra would point somewhere to


1046
01:11:59,720 --> 01:12:02,780
like [sleep] or something like that, right.


1047
01:12:03,360 --> 01:12:06,240
Um, yes.


1048
01:12:07,040 --> 01:12:10,470
Well, we see that the where at this point


1049
01:12:10,470 --> 01:12:13,950
would include some system call implementation functions


1050
01:12:13,950 --> 01:12:14,880
and a call to sleep,


1051
01:12:14,910 --> 01:12:19,660
as it happens I think, this is you're basically answering questions.


1052
01:12:19,660 --> 01:12:26,650
Yes, if we had just left off executing this process for some reason other than the timer interrupt,


1053
01:12:26,920 --> 01:12:32,680
switch would be basically returning to some system call code instead of to sched,


1054
01:12:32,680 --> 01:12:35,320
as it happens I think sleep may call sched, so .


1055
01:12:37,750 --> 01:12:40,210
The backtrace would look different, what just happened include sched,


1056
01:12:40,300 --> 01:12:42,940
but yes I've chosen just one way of,


1057
01:12:43,650 --> 01:12:47,980
you know just one way of switching between processes due to timer interrupts.


1058
01:12:49,700 --> 01:12:52,340
But you also get switches to wait for user IO,


1059
01:12:52,370 --> 01:12:55,100
wait for other processes to do things like write to pipe.


1060
01:12:58,380 --> 01:13:00,930
OK, one thing to you probably noticed is


1061
01:13:00,930 --> 01:13:04,410
that scheduler called switch,


1062
01:13:04,940 --> 01:13:07,460
and we're about to return from switch here,


1063
01:13:07,730 --> 01:13:11,360
but we're returning really from a different call to switch


1064
01:13:11,360 --> 01:13:12,830
than them on the scheduler are made,


1065
01:13:12,950 --> 01:13:16,160
were returning from a call to switch that this process made a long time ago.


1066
01:13:18,110 --> 01:13:21,290
So, you know, this is potentially a little bit confusing,


1067
01:13:21,290 --> 01:13:24,660
but, you know, this is how the [guts] of a thread switch work.


1068
01:13:25,730 --> 01:13:29,000
Another thing to notice is that the code we're looking at,


1069
01:13:29,000 --> 01:13:32,990
this switch code, this is really the heart of thread switching.


1070
01:13:33,600 --> 01:13:36,810
And really all you have to do to switch switch threads


1071
01:13:36,810 --> 01:13:40,470
is save registers and restore registers,


1072
01:13:40,560 --> 01:13:42,900
now threads have a lot more state than just registers,


1073
01:13:42,900 --> 01:13:47,490
they have variables and stuff in the heap and who knows what ,


1074
01:13:47,580 --> 01:13:52,380
all that other state is in-memory and isn't going to be disturbed,


1075
01:13:52,380 --> 01:13:56,750
we've done nothing to disturb any of these threads stacks,


1076
01:13:56,750 --> 01:13:59,240
for example or heap values,


1077
01:14:00,320 --> 01:14:04,700
so the registers in the microprocessor are really the only kind of volatile state,


1078
01:14:04,730 --> 01:14:07,730
that actually needs to be saved and restored to do a thread switch,


1079
01:14:07,760 --> 01:14:10,280
all the stuffs in memory stack for example,


1080
01:14:10,280 --> 01:14:12,800
will still be in memory on undisturbed


1081
01:14:12,890 --> 01:14:15,620
and so it doesn't have to be explicitly saved and restored.


1082
01:14:16,380 --> 01:14:19,830
Now we're only saving and restoring this microprocessor, the CPU registers,


1083
01:14:19,980 --> 01:14:24,330
because we want to reuse those very registers in the CPU for the new thread


1084
01:14:24,330 --> 01:14:26,490
and overwrite whatever values they have.


1085
01:14:26,850 --> 01:14:31,800
So, register, that's why we have to save the old thread's registers.


1086
01:14:32,970 --> 01:14:35,820
What about, other processors state,


1087
01:14:35,850 --> 01:14:39,870
so I don't know the RISC-V processor that we're using has other flags,


1088
01:14:39,870 --> 01:14:44,970
but I know like some x86 Intel chips have like like the floating point unit state


1089
01:14:44,970 --> 01:14:46,620
and like things like that,


1090
01:14:46,620 --> 01:14:48,990
do we do we just not have that in RISC-V.


1091
01:14:50,620 --> 01:14:54,550
Your point's very well taken on other microprocessors like x86,


1092
01:14:54,550 --> 01:14:58,960
the details switching are a bit different,


1093
01:14:58,960 --> 01:15:01,690
because they have different registers in different state


1094
01:15:02,830 --> 01:15:06,370
and so the code this is very very RISC-V dependent code


1095
01:15:06,370 --> 01:15:12,050
and the switch routine for some other processor might look quite different,


1096
01:15:12,050 --> 01:15:15,050
like indeed might have to save floating point registers,


1097
01:15:15,260 --> 01:15:19,090
now at RISC-V actually uses the general purpose register.


1098
01:15:20,220 --> 01:15:22,860
Actually I'm not sure what it does for floating point,


1099
01:15:22,950 --> 01:15:26,100
but the kernel doesn't use floating point, so it doesn't have to worry about it,


1100
01:15:27,300 --> 01:15:29,550
but yeah this is totally microprocessor dependent.


1101
01:15:31,710 --> 01:15:33,870
A question about the timer interrupts.


1102
01:15:34,680 --> 01:15:39,620
So it sounds like the the core of all of this scheduling working


1103
01:15:39,620 --> 01:15:41,390
is that there will be a timer interrupt,


1104
01:15:41,750 --> 01:15:45,020
what happens in cases where that [malfunctions].


1105
01:15:45,260 --> 01:15:47,300
There is going to be a timer interrupt.


1106
01:15:49,070 --> 01:15:55,150
So the, I know so ,


1107
01:15:55,180 --> 01:16:00,880
okay, so the reasoning for how come preemptive scheduling of user process's works,


1108
01:16:01,410 --> 01:16:07,510
is that user processes execute with interrupts turned on always,


1109
01:16:07,570 --> 01:16:13,000
xv6 just ensures that interrupts are enabled before returning to user space


1110
01:16:13,000 --> 01:16:16,810
and that means that a timer interrupt can happen if you're executing in user space,


1111
01:16:17,200 --> 01:16:22,240
so there's nothing a user process, if one user space that timer interrupt just will happen,


1112
01:16:22,820 --> 01:16:24,020
when the time comes.


1113
01:16:24,200 --> 01:16:25,520
So little trickier in the kernel,


1114
01:16:25,610 --> 01:16:27,320
the kernel sometimes turns off interrupts,


1115
01:16:27,320 --> 01:16:28,850
like when you acquire a lock,


1116
01:16:28,970 --> 01:16:30,170
the interrupts are going to be turned off,


1117
01:16:30,170 --> 01:16:31,010
until you release it.


1118
01:16:31,370 --> 01:16:31,910
So.


1119
01:16:34,600 --> 01:16:38,110
So if there were some bugs in the kernel,


1120
01:16:39,040 --> 01:16:40,570
if the kernel turned off interrupts


1121
01:16:40,570 --> 01:16:42,340
and never turn them back on


1122
01:16:42,760 --> 01:16:45,910
and the code in the kernel never gave up the CPU


1123
01:16:45,970 --> 01:16:49,030
never called sleep, gave up the CPU for any other reason,


1124
01:16:49,510 --> 01:16:53,050
then indeed a timer interrupt would occur


1125
01:16:53,050 --> 01:17:00,300
and that would mean that, this kernel code, may you know, would never give the CPU,


1126
01:17:00,300 --> 01:17:05,330
but in fact, as far as we know xv6 [is] xv6,


1127
01:17:05,330 --> 01:17:08,150
so that it always turns interrupts back on,


1128
01:17:08,150 --> 01:17:12,510
or you know, if there's code in xv6, it turns off interrupts,


1129
01:17:12,600 --> 01:17:14,250
it either turns them back on


1130
01:17:14,280 --> 01:17:18,070
and so timer interrupt can then occur in the kernel,


1131
01:17:18,070 --> 01:17:20,020
and we can switch away from this kernel thread


1132
01:17:20,170 --> 01:17:23,350
or the code returns back to user space,


1133
01:17:23,380 --> 01:17:25,150
kernel code turns back to user space,


1134
01:17:25,180 --> 01:17:31,900
we believe there's never a situation in which kernel code will simply loop with interrupts turned off forever.


1135
01:17:33,990 --> 01:17:36,090
I got, my question was more about like,


1136
01:17:36,120 --> 01:17:39,090
so I assume the interrupts are actually coming from some piece of hardware,


1137
01:17:39,330 --> 01:17:41,490
what if that piece of hardware [malfunctions].


1138
01:17:41,520 --> 01:17:41,970
No.


1139
01:17:45,330 --> 01:17:47,790
It's all right, your computer is broken, you should buy a new one.


1140
01:17:50,560 --> 01:17:50,920
Okay.


1141
01:17:51,010 --> 01:17:53,380
I mean that's a valid question for,


1142
01:17:53,410 --> 01:17:56,740
there's you know 10 billion transistors in your computer,


1143
01:17:56,740 --> 01:18:04,290
and indeed sometimes the hardware just like has bugs in it, but that's beyond our reach for.


1144
01:18:05,220 --> 01:18:08,340
I mean, if you add one and one and the computer says three,


1145
01:18:08,340 --> 01:18:14,110
then you just have deep problems that xv6 can't help you with.


1146
01:18:16,710 --> 01:18:18,900
So we're assuming that the computer works.


1147
01:18:20,370 --> 01:18:21,780
The only time when that,


1148
01:18:23,030 --> 01:18:24,740
when software mean,


1149
01:18:24,830 --> 01:18:28,100
there are times when software tries to compensate for hardware level errors,


1150
01:18:28,100 --> 01:18:30,740
like if you're sending packets across the network,


1151
01:18:31,100 --> 01:18:32,720
you always send a checksum,


1152
01:18:33,240 --> 01:18:38,220
so that if the network hardware flips a bit malfunctions flips a bit,


1153
01:18:38,220 --> 01:18:39,720
then you can correct that,


1154
01:18:39,720 --> 01:18:41,340
but for stuff inside the computer,


1155
01:18:42,100 --> 01:18:43,180
people tend not to,


1156
01:18:43,360 --> 01:18:50,290
it's just, people basically don't try to make the software compensate for hardware errors.


1157
01:18:54,170 --> 01:18:59,720
Oh, I have a question, why I was like in trampling dot actually swhich,


1158
01:18:59,780 --> 01:19:02,570
we write the code in assembly,


1159
01:19:02,570 --> 01:19:03,350
is that why,


1160
01:19:03,380 --> 01:19:08,450
is that because we want to make sure that exactly this thing happening,


1161
01:19:08,630 --> 01:19:11,140
so we cannot, you cann't write in C,


1162
01:19:11,140 --> 01:19:16,500
because we just need it feels like those exact things to happen basically.


1163
01:19:19,530 --> 01:19:21,540
Yeah yeah, um.


1164
01:19:22,800 --> 01:19:26,460
Yes, certainly we want this exact sequence to happen,


1165
01:19:26,460 --> 01:19:30,960
and C it it's very hard to talk about things like ra


1166
01:19:30,960 --> 01:19:39,670
and see or sp certainly there's no way within the C language to talk about changing the stack pointer,


1167
01:19:40,840 --> 01:19:42,130
with ra register,


1168
01:19:42,660 --> 01:19:48,760
so these are things that, just can't be you can't see it in ordinary C,


1169
01:19:49,270 --> 01:19:51,100
the only way you can see it in C,


1170
01:19:51,100 --> 01:19:56,920
is there, there is possible in C to sort of embed assembly instructions in C code,


1171
01:19:57,340 --> 01:20:00,760
so we could have just embedded these assembly instructions in the C function,


1172
01:20:00,760 --> 01:20:02,750
but, would amount to the same thing.


1173
01:20:03,510 --> 01:20:07,920
We're basically we're operating at a level below below C,


1174
01:20:07,920 --> 01:20:11,080
so we can't really can't really use C here.


1175
01:20:13,950 --> 01:20:14,700
I have a question,


1176
01:20:14,700 --> 01:20:17,280
about when a thread finishes executing


1177
01:20:17,280 --> 01:20:19,290
and assuming that happens in the user space


1178
01:20:19,290 --> 01:20:23,600
when we call the exec , I'm sorry, exec system call


1179
01:20:24,230 --> 01:20:30,110
and that also ends the process, the thread assuming in the kernel space,


1180
01:20:30,200 --> 01:20:36,110
but if the thread ends within before a new timer interrupt happens,


1181
01:20:36,170 --> 01:20:38,540
does it still look like,


1182
01:20:39,570 --> 01:20:42,720
is this like the CPU still acquired by that thread


1183
01:20:42,720 --> 01:20:45,060
or do we and that thread and start a new one


1184
01:20:45,060 --> 01:20:46,380
before the new timer interrupt.


1185
01:20:46,560 --> 01:20:47,460
Oh yeah,


1186
01:20:49,460 --> 01:20:53,930
the thread, the thread yields the CPU,


1187
01:20:54,820 --> 01:20:57,160
there's the exec exec yields the CPU,


1188
01:20:57,190 --> 01:20:58,690
so there's actually many points,


1189
01:20:58,690 --> 01:21:01,960
that even though I've been driving this discussion with a timer interrupt,


1190
01:21:01,960 --> 01:21:06,070
in fact, in almost almost all cases


1191
01:21:06,070 --> 01:21:08,200
where xv6 switches between threads,


1192
01:21:08,200 --> 01:21:09,670
it's not due to timer interrupts,


1193
01:21:09,760 --> 01:21:13,960
it's because some system calls waiting for something


1194
01:21:13,960 --> 01:21:17,740
or decides that it needs to give up the CPU


1195
01:21:17,770 --> 01:21:23,590
and so for example exec does various things and then calls yield to give up the CPU


1196
01:21:23,590 --> 01:21:24,870
and it does that.


1197
01:21:25,490 --> 01:21:28,940
There's really nothing does that independently of whether there's timer interrupt.


1198
01:21:31,730 --> 01:21:32,300
Yes.


1199
01:21:37,740 --> 01:21:40,890
All right, the time is up for this lecture,


1200
01:21:40,890 --> 01:21:44,190
I think I'll continue some of this discussion next week,


1201
01:21:44,190 --> 01:21:48,210
but I'm happy to take more questions right now, if people have them.


1202
01:21:52,150 --> 01:21:55,330
So let's say the operating system actually ,


1203
01:21:56,320 --> 01:21:58,870
I takes on a the thread implementation,


1204
01:21:58,900 --> 01:22:05,560
so so for example you want to run multiple threads of a process on multiple CPUs,


1205
01:22:05,560 --> 01:22:07,480
like that has to be handled by the OS,


1206
01:22:07,480 --> 01:22:09,940
that cannot just be handled in user space right.


1207
01:22:10,360 --> 01:22:11,950
How does that kind of switching work


1208
01:22:11,950 --> 01:22:15,130
is each, each thread now becomes the same as a process,


1209
01:22:15,130 --> 01:22:18,280
like is always going to loop through all existing threads


1210
01:22:18,400 --> 01:22:22,630
or you know cause like each CPU will still switch between,


1211
01:22:22,630 --> 01:22:24,610
even if one process give eight cores,


1212
01:22:24,610 --> 01:22:28,330
like it's still gonna switch switch each of the CPUs between those


1213
01:22:28,330 --> 01:22:30,070
and a couple of other processes


1214
01:22:30,550 --> 01:22:36,880
and also we don't want to really switch between one and the other thread on the same CPU or do we,


1215
01:22:36,910 --> 01:22:37,540
I don't know.


1216
01:22:38,640 --> 01:22:39,660
Wait, can I.


1217
01:22:41,300 --> 01:22:43,280
I'm not sure what the question is.


1218
01:22:43,880 --> 01:22:47,780
Yeah I guess I guess, can you just explain more like how does that happen.


1219
01:22:48,380 --> 01:22:49,610
Sorry, how does what happened.


1220
01:22:50,270 --> 01:22:53,750
Let's say we have multiple threads per process,


1221
01:22:53,750 --> 01:22:56,540
so that they can and they can run on different CPUs,


1222
01:22:56,660 --> 01:22:58,880
like how do we go, how do you go about there.


1223
01:22:59,480 --> 01:23:03,650
So Linux, for example supports multiple threads per process


1224
01:23:03,650 --> 01:23:06,110
and in Linux, the implementation,


1225
01:23:06,960 --> 01:23:09,000
yeah it's a complex implementation,


1226
01:23:09,000 --> 01:23:11,430
but maybe the simplest way to explain it is that,


1227
01:23:11,910 --> 01:23:18,230
each, it's almost as if each thread in Linux is a complete process,


1228
01:23:19,200 --> 01:23:25,050
and the the threads of a given, what we would call the threads of a particular process


1229
01:23:25,080 --> 01:23:29,310
are essentially separate processes that share the same memory,


1230
01:23:30,040 --> 01:23:35,530
so Linux has sort of separated out the notion of thread of execution from address space


1231
01:23:35,530 --> 01:23:39,140
and you know you can have them separately,


1232
01:23:39,140 --> 01:23:41,150
and if you make two threads in one process,


1233
01:23:41,150 --> 01:23:44,360
it basically makes two processes that share one address space,


1234
01:23:44,690 --> 01:23:51,320
and then from then on, the scheduling is not unlike what xv6 does for individual processes.


1235
01:23:51,740 --> 01:23:53,000
I see and then,


1236
01:23:53,120 --> 01:23:59,510
is there anything like does the user have to specify like, okay [pin] each thread to a CPU


1237
01:23:59,810 --> 01:24:05,570
or how does the OS make sure that different threads of the same process don't run on the same core,


1238
01:24:05,570 --> 01:24:08,330
because that's kind of defeating the purpose or not I guess,


1239
01:24:08,330 --> 01:24:08,690
I don't know.


1240
01:24:09,160 --> 01:24:14,050
The the, it's actually just like it's much like xv6, namely the,


1241
01:24:15,820 --> 01:24:21,610
you know there's four cores and Linux will just find four things for one of those four cores,


1242
01:24:22,000 --> 01:24:28,780
they maybe, you know if there's not much going on then maybe they'll be four threads of the same process,


1243
01:24:29,370 --> 01:24:32,430
or if there's a hundred users logged in on Athena machine,


1244
01:24:32,430 --> 01:24:37,230
maybe it's one thread each from multiple different processes, you know.


1245
01:24:39,060 --> 01:24:40,650
There's not any one answer


1246
01:24:40,710 --> 01:24:43,890
or the kernelod basically find something for each core to do


1247
01:24:43,890 --> 01:24:45,330
and then that core does that thing.


1248
01:24:46,500 --> 01:24:47,580
Okay, that makes sense.


1249
01:24:48,520 --> 01:24:52,120
You can, you know if you're if you want to do careful measurements,


1250
01:24:52,120 --> 01:24:53,980
there is a way to pin threads to cores,


1251
01:24:53,980 --> 01:24:57,490
but people only do it when they're up to something strange.


1252
01:25:00,540 --> 01:25:05,280
So you share the virtual table, just [] memory,


1253
01:25:05,490 --> 01:25:10,020
so they say they have the same page table, those threads.


1254
01:25:10,140 --> 01:25:15,510
Yeah yeah, if you're on Linux, if you create two threads in one process,


1255
01:25:15,510 --> 01:25:16,680
then you have these two threads.


1256
01:25:19,080 --> 01:25:23,760
I don't know if they like literally share the exact same page table


1257
01:25:23,760 --> 01:25:27,330
or whether their page tables are identical, one or the other.


1258
01:25:28,620 --> 01:25:31,860
Is there a reason why they would have to be separate, ever,


1259
01:25:32,220 --> 01:25:34,740
if you manually map memory, or.


1260
01:25:36,540 --> 01:25:41,640
I I don't know enough to know whether which which Linux does.


1261
01:25:44,320 --> 01:25:47,860
Okay, I have another question about a small detail ,


1262
01:25:48,130 --> 01:25:52,030
so basically like from my understanding when you call switch,


1263
01:25:52,450 --> 01:25:56,050
you switch from one call to switch to another,


1264
01:25:56,080 --> 01:25:57,640
so the first time you call switch,


1265
01:25:57,670 --> 01:26:03,310
you have to like kind of artificially create other endpoint to come back to right.


1266
01:26:03,430 --> 01:26:04,060
Yes.


1267
01:26:04,510 --> 01:26:07,030
Because you can't just randomly jump in to write any code.


1268
01:26:07,300 --> 01:26:10,980
Yes, you want to know where that,


1269
01:26:11,860 --> 01:26:16,790
where that [fake], where that context was cooked up.


1270
01:26:17,750 --> 01:26:20,690
Probably somewhere where the processes created,


1271
01:26:20,690 --> 01:26:21,560
I guess I don't know.


1272
01:26:21,560 --> 01:26:27,760
Yeah yeah, maybe user in that, or not using [a] lock proc.


1273
01:26:31,030 --> 01:26:32,560
Don't know.


1274
01:26:32,920 --> 01:26:35,680
There's something called fork trap or something.


1275
01:26:35,680 --> 01:26:39,250
Yeah, look at this, yeah we got forkret,


1276
01:26:39,250 --> 01:26:44,590
okay so an alloc proc which is called both for the very first process at boot time


1277
01:26:44,590 --> 01:26:52,720
and by fork alloc proc sets up the critical elements of the context for the new processes,


1278
01:26:53,770 --> 01:26:56,320
it, it sets up the new process's context,


1279
01:26:56,380 --> 01:26:59,230
it actually doesn't matter what most of the registers are ,


1280
01:26:59,260 --> 01:27:00,820
but it does matter what ra is,


1281
01:27:00,820 --> 01:27:05,710
because that's where the switch the very first switch and that process is going to return to ra.


1282
01:27:07,450 --> 01:27:10,120
And that process is going to need to use its own stack,


1283
01:27:10,150 --> 01:27:14,100
so ra and sp setup, faked essentially,


1284
01:27:14,370 --> 01:27:17,430
so the very first switched or process works.


1285
01:27:18,130 --> 01:27:20,410
So, so if I understand this correctly,


1286
01:27:20,410 --> 01:27:27,490
when this switch will happen then it'll basically just start executing the first instruction inside of the forkret,


1287
01:27:27,550 --> 01:27:30,460
as if forkret just called switch and return from.


1288
01:27:30,910 --> 01:27:36,370
Yeah yeah, the return from switch is gonna be a jump to the beginning of forkret.


1289
01:27:37,330 --> 01:27:39,800
Right, interesting,


1290
01:27:40,370 --> 01:27:43,880
do we ever call forkret or is it always happens,


1291
01:27:44,060 --> 01:27:45,650
I think it always happens like this.


1292
01:27:45,860 --> 01:27:48,710
I don't think anything ever calls forkret for real,


1293
01:27:48,860 --> 01:27:57,300
because just, yeah it's only executed in this weird way from first timer process is run.


1294
01:27:58,510 --> 01:28:04,180
It is really its job is to release the lock the scheduler took


1295
01:28:04,330 --> 01:28:08,890
and then return and then this usertrapret, of course, is also fake.


1296
01:28:09,500 --> 01:28:16,800
That it's, it's yeah it's like it's as if returning from a trap except the trapframe is faked,


1297
01:28:16,800 --> 01:28:23,490
also to to have like jump to the first instruction in the user (Right) code.


1298
01:28:24,760 --> 01:28:29,110
Oh, but the trapframe, it's again the same like you don't need to initialize any registers,


1299
01:28:29,110 --> 01:28:31,930
because it's like well we're going to the beginning,


1300
01:28:31,930 --> 01:28:33,970
so you don't need to assume anything.


1301
01:28:34,330 --> 01:28:36,700
Yeah, the program counter I think is a.


1302
01:28:37,400 --> 01:28:37,670
Yeah.


1303
01:28:37,670 --> 01:28:39,920
It needs to be initialized to zero,


1304
01:28:40,340 --> 01:28:41,510
I don't know what else,


1305
01:28:42,320 --> 01:28:43,700
you maybe maybe it.


1306
01:28:46,440 --> 01:28:48,870
They probably if we call them it doesn't right,


1307
01:28:48,990 --> 01:28:52,590
because if we already do the call then that's going to set the program counter.


1308
01:28:52,770 --> 01:29:00,160
Yeah, so here's this only happens because fork copies fork copies, the program counter, the user program counter


1309
01:29:00,580 --> 01:29:02,470
and so the only time when we're not doing it fork,


1310
01:29:02,470 --> 01:29:03,670
is for the very first process


1311
01:29:03,670 --> 01:29:05,830
where it's like explicitly deceptive.


1312
01:29:05,830 --> 01:29:06,490
Oh.


1313
01:29:07,310 --> 01:29:08,030
And stack pointer.


1314
01:29:08,030 --> 01:29:08,480
Oh yeah.


1315
01:29:08,510 --> 01:29:09,560
Also needs to be set up.


1316
01:29:11,130 --> 01:29:14,100
Oh yeah, because it's that EPC that's not PC,


1317
01:29:14,100 --> 01:29:17,370
that's the one that's gonna get swapped by the trap trampoline.


1318
01:29:17,610 --> 01:29:18,330
Yes.


1319
01:29:19,660 --> 01:29:20,620
Oh, I see.


1320
01:29:21,820 --> 01:29:25,540
Because the real piece is actually gonna be in trap like inside traveling,


1321
01:29:26,020 --> 01:29:28,180
but then we're gonna switch it to jump to there.


1322
01:29:29,220 --> 01:29:30,480
Yeah.


1323
01:29:32,870 --> 01:29:36,590
Can I just ask like can you go back to the alloc proc.


1324
01:29:42,940 --> 01:29:45,370
I think there's ,


1325
01:29:45,960 --> 01:29:52,920
oh no sorry, forkret there is something there, that happens I think for the first process only.


1326
01:29:53,960 --> 01:29:58,970
What's this for a first call, I wasn't really sure what happened.


1327
01:29:59,090 --> 01:30:03,620
Let's see, the file system, the file system needs to be initialized


1328
01:30:03,620 --> 01:30:07,070
and in particular some stuff needs to be read off the disk


1329
01:30:07,190 --> 01:30:08,960
in order to get the file system going,


1330
01:30:09,520 --> 01:30:15,280
like there's this thing called the super block which describes how big the file system are,


1331
01:30:15,280 --> 01:30:17,560
and where the various things are in the file system


1332
01:30:17,620 --> 01:30:22,130
and there's also a crash recovery log that needs to be replayed


1333
01:30:22,250 --> 01:30:26,600
in order to recover from a previous crash, if there was one.


1334
01:30:27,770 --> 01:30:31,040
But in order to do anything in the file system,


1335
01:30:31,040 --> 01:30:35,060
you need to be able to wait for disk operations to complete,


1336
01:30:35,060 --> 01:30:36,770
but the way xv6 works,


1337
01:30:37,010 --> 01:30:41,420
you really can only execute the file system code in the context of process,


1338
01:30:42,110 --> 01:30:44,930
in order to like wait for IO


1339
01:30:45,320 --> 01:30:49,340
and so therefore the initialization of the file system has to be deferred


1340
01:30:49,340 --> 01:30:51,650
until the first time we have a process running.


1341
01:30:53,140 --> 01:30:56,800
And that occurs in the very first process in forkret.


1342
01:31:00,350 --> 01:31:01,070
I see.


1343
01:31:01,640 --> 01:31:04,550
And I'm guessing we'll learn more about this later.


1344
01:31:05,320 --> 01:31:10,390
Yeah, not about this horrible mess, but about how file systems work.


1345
01:31:11,050 --> 01:31:13,030
All right okay well, thank you,


1346
01:31:13,030 --> 01:31:15,100
I'm sorry for holding on so long.


1347
01:31:16,920 --> 01:31:18,900
Thanks for all the answers.


1348
01:31:21,840 --> 01:31:26,400
Sorry, it's not even that process, when this thing is executed ....


